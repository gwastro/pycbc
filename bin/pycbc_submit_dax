#!/bin/bash
# Redirect stdout ( > ) into a named pipe ( >() ) running "tee"
set -e

mkdir -p workflow/planning
COMMAND_LINE="${0} ${@}"
echo "${COMMAND_LINE}" > workflow/planning/pycbc-submit-dax-command.txt
echo "Executing on `hostname` in directory `pwd` at `date`" > workflow/planning/pycbc-submit-dax-log.txt

exec > >(tee -a workflow/pycbc-submit-dax-log.txt)

# Without this, only stdout would be captured - i.e. your
# log file would not contain any error messages.
# SEE answer by Adam Spiers, which keeps STDERR a seperate stream -
# I did not want to steal from him by simply adding his answer to mine.
exec 2>&1

DAX_FILE=""
CACHE_FILE=0
LOCAL_PEGASUS_DIR=""
PEGASUS_PROPERTIES=""
SHARED_FILESYSTEM=0
SITE_LIST=""
STAGING_SITES=""
TRANSFORMATION_CATALOG=""
SITE_CATALOG=""
LOCAL_STAGING_SERVER="file://"
REMOTE_STAGING_SERVER="file://"
NO_CREATE_PROXY=0
NO_QUERY_DB=0
NO_GRID=""
SUBMIT_DAX="--submit"
HTML_ENTITIES="{\"\'\": '&#39;', '(': '&#40;', ')': '&#41;', '+': '&#43;', '\"': '&quot;'}"

# These will be changed by the bundle builder
DATA_INLINE=False

function expand_pegasus_files() {
(
base64 -d <<DATA_END
PEGASUS_FILE_DATA
DATA_END
) | tar -zx
}

echo "# Properties set on command line" > extra-properties.conf
rm -f _reuse.cache
touch _reuse.cache
rm -f *-extra-site-properties.xml

GETOPT_CMD=`getopt -o d:c:C:g:r:p:P:es:S:t:nl:Gh --long dax:,cache-file:,local-staging-server:,remote-staging-server:,pegasus-properties:,append-pegasus-property:,enable-shared-filesystem,execution-sites:,staging-sites:,transformation-catalog:,site-catalog:,no-create-proxy,no-query-db,no-submit,local-dir:,no-grid,help -n 'pycbc_submit_dax' -- "$@"`
eval set -- "$GETOPT_CMD"

while true ; do
  case "$1" in
    -d|--dax)
      case "$2" in
        "") shift 2 ;;
        *) DAX_FILE=$2 ; shift 2 ;;
      esac ;;
    -c|--cache-file)
      case "$2" in
        "") shift 2 ;;
        *) CACHE_FILE=1
           OLD_IFS=${IFS}
           IFS=","
           set +e
           for URL in $2; do
              FOUND_URL=1
              # strip of any leading file:// URI and see if the file exists
              FILE_URL=${URL##file://}
              if [ -f ${FILE_URL} ] ; then
                 cat ${FILE_URL} >> _reuse.cache
                 FOUND_URL=0
              else
                 # otherwise try and get the file using resolve_url
                 DOWNLOADED_FILE=`python - <<EOF
from pycbc.workflow import resolve_url
import os
filename = resolve_url("${FILE_URL}")
print(filename)
EOF`
                 FOUND_URL=${?}
                 if [ ${FOUND_URL} -eq 0 ] ; then
                   cat $DOWNLOADED_FILE >> _reuse.cache
                   rm -f $DOWNLOADED_FILE
                 fi
              fi
              if [ ${FOUND_URL} -ne 0 ] ; then
                 echo "$URL not found!"
                 exit
              fi
              IFS=${OLD_IFS}
           done
           set -e
           shift 2 ;;
      esac ;;
    -g|--local-staging-server)
      case "$2" in
        "") shift 2 ;;
        *) LOCAL_STAGING_SERVER=$2 ; shift 2 ;;
      esac ;;
    -r|--remote-staging-server)
      case "$2" in
        "") shift 2 ;;
        *) REMOTE_STAGING_SERVER=$2 ; shift 2 ;;
      esac ;;
    -p|--pegasus-properties)
      case "$2" in
        "") shift 2 ;;
        *) PEGASUS_PROPERTIES=$2 ; shift 2 ;;
      esac ;;
    -P|--append-pegasus-property)
      case "$2" in
        "") shift 2 ;;
        *) echo $2 >> extra-properties.conf ; shift 2 ;;
      esac ;;
    -e|--enable-shared-filesystem) SHARED_FILESYSTEM=1 ; shift ;;
    -s|--execution-sites)
      case "$2" in
        "") shift 2 ;;
        *) SITE_LIST=$2 ; shift 2 ;;
      esac ;;
    -S|--staging-sites)
      case "$2" in
        "") shift 2 ;;
        *) STAGING_SITES="--staging-site $2" ; shift 2 ;;
      esac ;;
    -t|--transformation-catalog)
      case "$2" in
        "") shift 2 ;;
        *) TRANSFORMATION_CATALOG=$2 ; shift 2 ;;
      esac ;;
    -C|--site-catalog)
      case "$2" in
        "") shift 2 ;;
        *) SITE_CATALOG=$2 ; shift 2 ;;
      esac ;;
    -K|--no-create-proxy) NO_CREATE_PROXY=1 ; shift ;;
    -Q|--no-query-db) NO_QUERY_DB=1 ; shift ;;
    -n|--no-submit) SUBMIT_DAX="" ; shift ;;
    -l|--local-dir)
      case "$2" in
        "") shift 2 ;;
        *) LOCAL_PEGASUS_DIR=$2 ; shift 2 ;;
      esac ;;
    -G|--no-grid) NO_GRID="--forward nogrid" ; shift ;;
    -h|--help)
      echo "usage: pycbc_submit_dax [-h] --dax DAX [optional arguments]"
      echo
      echo "required arguments:"
      echo "  -d, --dax DAX           name of the dax file to plan"
      echo
      echo "optional arguments:"
      echo "  -h, --help              show this help message and exit"
      echo "  -c, --cache-file FILE   replica cache file for data reuse"
      echo "  -g, --local-staging-server uri://host provide a uri on host"
      echo "                                 for the local site scratch"
      echo "                                 (default is file://)"
      echo "  -r, --remote-staging-server uri://host provide a uri on host"
      echo "                                 for the remote site scratch."
      echo "  -p, --pegasus-properties FILE use the specified file as"
      echo "                               the pegasus properties file"
      echo "  -P, --append-pegasus-property STRING add the extra property"
      echo "                                          specified by the argument"
      echo "  -s, --execution-sites A,B,C specify a comma separated list"
      echo "                               of execution sites that will be"
      echo "                               used in addition to the local site"
      echo "  -S, --staging-sites A=X,B=Y,C=Z  comma separated list of key=value"
      echo "                                   pairs, where the key is the"
      echo "                                   execution site and value is the"
      echo "                                   staging site for that execution site"
      echo "  -t, --transformation-catalog FILE pass the specified"
      echo "                                   transformation catalog to Pegasus"
      echo "  -t, --site-catalog FILE pass the specified"
      echo "                                   site catalog to Pegasus"
      echo "  -K, --no-create-proxy   Do not run ligo-proxy-init and assume"
      echo "                             that the user has a valid grid proxy"
      echo "  -n, --no-submit         Plan the DAX but do not submit it"
      echo "  -l, --local-dir         Directory to put condor files under"
      echo "  -G, --no-grid           Disable checks for grid proxy and"
      echo "                             GLOBUS_LOCATION in pegasus-plan"
      echo
      echo "If the environment variable TMPDIR is set then this is prepended to the "
      echo "path to the temporary workflow execute directory passed to pegasus-plan."
      echo "If the --local-dir option is not given."
      echo
      echo "If the environment variable PEGASUS_FILE_DIRECTORY is set then the"
      echo "script will look there for configuration, "
      echo "otherwise the script will look for this directory by querying the"
      echo "pycbc.workflow module."
      echo
      exit 0 ;;
    --) shift ; break ;;
    *) echo "Internal error!" ; exit 1 ;;
  esac
done

if [ "x$DAX_FILE" == "x" ]; then
  echo "Error: --dax must be specified. Use --help for options."
   exit 1
fi

if [ $NO_CREATE_PROXY == 0 ]; then
  # Force the user to create a new grid proxy
  LIGO_USER_NAME=""
  while true; do
    read -p "Enter your LIGO.ORG username in (e.g. albert.einstein): " LIGO_USER_NAME
    echo
    if [ ! -z $LIGO_USER_NAME ] ; then
      break
    fi
  done
  unset X509_USER_PROXY
  ligo-proxy-init -p $LIGO_USER_NAME || exit 1
else
  if [ ! -z ${X509_USER_PROXY} ] ; then
    if [ -f ${X509_USER_PROXY} ] ; then
      cp -a ${X509_USER_PROXY} /tmp/x509up_u`id -u`
    fi
  unset X509_USER_PROXY
  fi
fi

if [ -z "${NO_GRID}" ] ; then
  #Check that the proxy is valid
  set +e
  grid-proxy-info -exists
  RESULT=$?
  set -e
  if [ ${RESULT} -eq 0 ] ; then
    PROXY_TYPE=`grid-proxy-info -type | tr -d ' '`
    if [ x${PROXY_TYPE} == 'xRFC3820compliantimpersonationproxy' ] ; then
      grid-proxy-info
    else
      cp /tmp/x509up_u`id -u` /tmp/x509up_u`id -u`.orig
      grid-proxy-init -hours 276 -cert /tmp/x509up_u`id -u`.orig -key /tmp/x509up_u`id -u`.orig
      rm -f /tmp/x509up_u`id -u`.orig
      grid-proxy-info
    fi
  else
    echo "Error: Could not find a valid grid proxy to submit workflow."
    exit 1
  fi
fi

#Make a directory for the submit files
SUBMIT_DIR=`mktemp --tmpdir=${LOCAL_PEGASUS_DIR} -d pycbc-tmp.XXXXXXXXXX`

#Make sure the directory is world readable
chmod 755 $SUBMIT_DIR

# find the site-local template directory
if [ $DATA_INLINE == "True" ] ; then
  expand_pegasus_files
  PEGASUS_FILE_DIRECTORY=${PWD}/pegasus_files
elif [ -z $PEGASUS_FILE_DIRECTORY ] ; then
  PEGASUS_FILE_DIRECTORY=`python -c 'from pycbc.workflow import PEGASUS_FILE_DIRECTORY;print(PEGASUS_FILE_DIRECTORY)'`
fi

# add the local site
# Set remote staging url to local
if [ -z $SITE_LIST ] ; then
  SITE_LIST="condorpool"
else
  SITE_LIST="condorpool,${SITE_LIST}"
fi


# Plan the workflow
echo "Generating concrete workflow"

# cache the pegasus config
if [ -z ${PEGASUS_PROPERTIES} ] ; then
  cp $PEGASUS_FILE_DIRECTORY/pegasus-properties.conf ./pegasus-properties.conf
else
  cp ${PEGASUS_PROPERTIES} ./pegasus-properties.conf
fi

# add a transformation catalog, if not given
if [ -z ${TRANSFORMATION_CATALOG} ] ; then
  TRANSFORMATION_CATALOG="${DAX_FILE%.dax}.tc.yml"
fi

# add a site catalog, if not give
if [ -z ${SITE_CATALOG} ] ; then
  SITE_CATALOG="${DAX_FILE%.dax}.sc.yml"
fi


echo >> pegasus-properties.conf
cat extra-properties.conf >> pegasus-properties.conf

# If user specified a value for pegasus.data.configuration
# then don't add another one
if ! grep -q pegasus.data.configuration= pegasus-properties.conf
then
  # No option specified, append nonshared unless user said
  # not to
  if [ $SHARED_FILESYSTEM == 0 ]
  then
    echo 'pegasus.data.configuration=nonsharedfs' >> pegasus-properties.conf
  fi
fi



if [ $CACHE_FILE == 0 ]; then
  CACHE_OPTION=''
else
  CACHE_OPTION='--cache _reuse.cache'
fi

pegasus-plan -Dpegasus.catalog.site.file=${SITE_CATALOG} -Dpegasus.catalog.transformation.file=${TRANSFORMATION_CATALOG} --conf ./pegasus-properties.conf --sites $SITE_LIST $STAGING_SITES -o local --dir $SUBMIT_DIR --cleanup inplace ${CACHE_OPTION} --relative-dir work --cluster horizontal $SUBMIT_DAX $NO_GRID $DAX_FILE

echo

rm -f submitdir
ln -sf $SUBMIT_DIR submitdir

echo "pegasus-status --verbose --long $SUBMIT_DIR/work \$@" > status
chmod 755 status

echo "pegasus-analyzer -r -v $SUBMIT_DIR/work \$@" > debug
chmod 755 debug

echo "pegasus-remove $SUBMIT_DIR/work \$@" > stop
chmod 755 stop

if [ -z "${NO_GRID}" ] ; then
  cat << EOF > start
#!/bin/bash

if [ -f /tmp/x509up_u\`id -u\` ] ; then
  unset X509_USER_PROXY
else
  if [ ! -z \${X509_USER_PROXY} ] ; then
    if [ -f \${X509_USER_PROXY} ] ; then
      cp -a \${X509_USER_PROXY} /tmp/x509up_u\`id -u\`
    fi
  fi
  unset X509_USER_PROXY
fi

# Check that the proxy is valid
grid-proxy-info -exists
RESULT=\${?}
if [ \${RESULT} -eq 0 ] ; then
  PROXY_TYPE=\`grid-proxy-info -type | tr -d ' '\`
  if [ x\${PROXY_TYPE} == 'xRFC3820compliantimpersonationproxy' ] ; then
    grid-proxy-info
  else
    cp /tmp/x509up_u\`id -u\` /tmp/x509up_u\`id -u\`.orig
    grid-proxy-init -hours 276 -cert /tmp/x509up_u\`id -u\`.orig -key /tmp/x509up_u\`id -u\`.orig
    rm -f /tmp/x509up_u\`id -u\`.orig
    grid-proxy-info
  fi
else
  echo "Error: Could not find a valid grid proxy to submit workflow."
  exit 1
fi

EOF
fi

echo "pegasus-run $SUBMIT_DIR/work \$@" > start

chmod 755 start

# Copy planning information into workflow directory so it can be displayed on the results page
cp pegasus-properties.conf workflow/planning
cp $SUBMIT_DIR/work/braindump.yml workflow/planning

mkdir -p workflow/input_map
cp _reuse.cache workflow/input_map/reuse.cache

mkdir -p workflow/dax
mkdir -p workflow/output_map

# copy the main dax and output.map to the workflow directory
cp $DAX_FILE workflow/dax
if [ -e output.map ] ; then
  cp output.map workflow/output_map
fi

# ugly shell to extract the sub-dax names from the uberdax xml
for dax in `egrep '<[[:blank:]]*dax' ${DAX_FILE} | tr ' ' \\\n | grep file | awk -F= '{print $2}' | tr -d '">'` ; do
  if [ -e ${dax} ] ; then
    cp ${dax} workflow/dax
  fi
  map=`basename ${dax} .dax`.map
  if [ -e ${map} ] ; then
    cp ${map} workflow/output_map
  fi
done

if [ -z ${SUBMIT_DAX} ] ; then
  echo
  echo "WARNING: DAX planned but not submitted. No dashboard entry has been created and"
  echo "         the workflow section of results page will not show a dashboard URL."
  echo "         You must run this script without the --no-submit option if this is a"
  echo "         production run."
  echo
  exit 0
fi

if [ ! -e ${HOME}/.pegasus/workflow.db ] ; then
  echo "WARNING: Could not find Pegasus dashboard database in ${HOME}/.pegasus"
  echo "         Workflow has been submitted but the results page will not contain"
  echo "         a link to the dashboard page. If this is a production workflow,"
  echo "         please remove the workflow, check for the origin of this error,"
  echo "         and re-submit the workflow by re-running this script."
  echo
  exit 1
fi

WORKFLOW_ID_STRING=""
WORKFLOW_DB_CMD="sqlite3 -csv ${HOME}/.pegasus/workflow.db \"select submit_hostname,wf_id,wf_uuid from master_workflow where submit_dir = '${SUBMIT_DIR}/work';\""
DB_TRY=0

# force a condor reschedule to get the workflow running
set +e
condor_reschedule &> /dev/null
set -e

if [ $NO_QUERY_DB == 0 ]; then

    /bin/echo "Querying Pegasus database for workflow stored in ${SUBMIT_DIR}/work"
    /bin/echo -n "This may take up to 120 seconds. Please wait..."
    rm -f pegasus_db.log
    touch pegasus_db.log
    # querying the database sometimes fails, so allow retries
    set +e
    until [ $DB_TRY -ge 15 ]
    do
      /bin/echo -n "."
      WORKFLOW_ID_STRING=`eval $WORKFLOW_DB_CMD 2>> pegasus_db.log`
      if [ $? -eq 0 ] && [ ! -z $WORKFLOW_ID_STRING ] ; then
        /bin/echo " Done."
        DB_QUERY_SUCCESS=0
        break
      else
        DB_QUERY_SUCCESS=1
      fi
      DB_TRY=$(( $DB_TRY + 1 ))
      for s in `seq ${DB_TRY}`
      do
        /bin/echo -n "."
        sleep 1
      done
    done
    set -e

    if [ ${DB_QUERY_SUCCESS} -eq 1 ] ; then
      echo; echo
      /bin/echo "Query failed: ${WORKFLOW_DB_CMD}"
      cat pegasus_db.log
    else
      rm -f pegasus_db.log
    fi

    if [ -z $WORKFLOW_ID_STRING ] ; then
      echo "WARNING: Could not find the workflow in the Pegasus dashboard database."
      echo "         Workflow has been submitted but the results page will not contain"
      echo "         a link to the dashboard page. If this is a production workflow,"
      echo "         please remove the workflow, check for the origin of this error,"
      echo "         and re-submit the workflow by re-running this script."
      echo
      exit 1
    fi

fi

WORKFLOW_ID_ARRAY=(${WORKFLOW_ID_STRING//,/ })
DASHBOARD_URL="https://${WORKFLOW_ID_ARRAY[0]}/pegasus/u/${USER}/r/${WORKFLOW_ID_ARRAY[1]}/w?wf_uuid=${WORKFLOW_ID_ARRAY[2]}"

echo ${DASHBOARD_URL} > workflow/pegasus-dashboard-url.txt

shopt -s nullglob
DASHBOARD_GLOB=(results/*_workflow/*DASHBOARD*.html)
DASHBOARD_PATH=${DASHBOARD_GLOB[0]}
if [ -e ${DASHBOARD_PATH} ] ; then
  DASHBOARD_COPY=workflow/`basename ${DASHBOARD_PATH}`.orig
  if [ -e ${DASHBOARD_COPY} ] ; then
    cp ${DASHBOARD_COPY} ${DASHBOARD_PATH}
  else
    cp ${DASHBOARD_PATH} ${DASHBOARD_COPY}
  fi

  COMMAND_LINE=$(python -c "import sys; from xml.sax.saxutils import (escape, unescape); print(escape(unescape(sys.stdin.read()), entities=${HTML_ENTITIES}))" <<< ${SITE_PROFILE})
  sed -i.bak -e "s+PYCBC_SUBMIT_DAX_ARGV+${COMMAND_LINE}+g" ${DASHBOARD_PATH}
  sed -i.bak -e "s+PEGASUS_DASHBOARD_URL+${DASHBOARD_URL}+g" ${DASHBOARD_PATH}
fi
shopt -u nullglob

echo "Workflow submission completed successfully."
echo
echo "The Pegasus dashboard URL for this workflow is:"
echo "  ${DASHBOARD_URL}"
echo
echo "Note that it make take a while for the dashboard entry to appear while the workflow"
echo "is parsed by the dashboard. The delay can be on the order of one hour for very large"
echo "workflows."
echo

# If the workflow results directory exists, copy the files there
for dir in planning dax input_map output_map
do
  RESULT_DIR=results/*_workflow/*_${dir}
  if [ -d ${RESULT_DIR} ] ; then
    cp workflow/${dir}/* ${RESULT_DIR}
  fi
done

exit 0
