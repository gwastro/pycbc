#!/bin/env python

"""
Create a file containing the time, phase and amplitude correlations between two
or more detectors for signals by doing a simple monte-carlo.

Output is the relative amplitude, time, and phase as compared to a reference
IFO. A separate calculation is done for each IFO as a possible reference.  The
data is stored as two vectors : one vector gives the discrete integer bin
corresponding to a particular location in 3*(Nifo-1)-dimensional
amplitude/time/phase space, the other gives the weight assigned to that bin.
To get the signal rate this weight should be scaled by the local sensitivity
value and by the SNR of the event in the reference detector.
"""

import argparse, h5py, numpy as np, pycbc.detector, logging
from numpy.random import uniform, normal
from scipy.stats import norm
from copy import deepcopy


def get_factors(fp, fc, psi, phi, alpha, theta, beta, num_comps=5):
    aplus = []
    across = []

    if num_comps >= 1:
        aplus += [(1. + np.cos(theta) ** 2.) / 2.]
        across += [np.cos(theta)]

    if num_comps >= 2:
        aplus += [2. * np.sin(theta) * np.cos(theta)]
        across += [2 * np.sin(theta)]

    if num_comps >= 3:
        aplus += [3. * np.sin(theta) ** 2.]
        across += [np.zeros(aplus[-1].shape, dtype=aplus[-1].dtype)]

    if num_comps >= 4:
        aplus += [- aplus[1]]
        across += [across[1]]

    if num_comps == 5:
        aplus += [aplus[0]]
        across += [- across[0]]

    aplus = np.stack(aplus, axis=1)
    across = np.stack(across, axis=1)

    phik = np.stack([2. * phi + (2. - k) * alpha for k in range(num_comps)], axis=1)
    psi2 = 2. * psi[:, None]

    a1 = aplus * np.cos(phik) * np.cos(psi2) - across * np.sin(phik) * np.sin(psi2)
    a2 = aplus * np.cos(phik) * np.sin(psi2) + across * np.sin(phik) * np.cos(psi2)
    a3 = - aplus * np.sin(phik) * np.cos(psi2) - across * np.cos(phik) * np.sin(psi2)
    a4 = - aplus * np.sin(phik) * np.sin(psi2) + across * np.cos(phik) * np.cos(psi2)

    b = np.tan(beta / 2.)
    bk = np.stack([b ** k for k in range(num_comps)], axis=1)
    bt = bk / ((1 + b[:, None] ** 2.) ** 2.)

    wp = fp * np.cos(2. * psi) - fc * np.sin(2. * psi)
    wc = fp * np.sin(2. * psi) + fc * np.cos(2. * psi)

    wp = wp[:, None]
    wc = wc[:, None]

    real = bt * (wp * a1 + wc * a2)
    imag = bt * (wp * a3 + wc * a4)

    return real + 1.j * imag


parser = argparse.ArgumentParser(description=__doc__)
parser.add_argument('--sample-size', type=int, required=True,
                    help="Approximate number of independent samples to draw "
                         "for the distribution")
parser.add_argument('--noise-per-sample', type=int, default=10,
                    help="The number of noise iterations to add to each sample.")
parser.add_argument('--seed', type=int, default=124)
parser.add_argument('--output-file', required=True)
parser.add_argument('--verbose', action='store_true')
parser.add_argument('--bin-density', type=int, default=1,
                    help="Number of bins per 1 sigma uncertainty in a "
                         "parameter. Higher values increase the resolution of "
                         "the histogram at the expense of storage.")
parser.add_argument('--snr-reference', type=float, default=5,
                    help="Reference SNR to scale SNR uncertainty")
parser.add_argument('--snr-uncertainty', type=float, default=1.0,
                    help="SNR uncertainty to set bin size and smoothing")
parser.add_argument('--weight-threshold', type=float, default=1e-10,
                    help="Minimum histogram weight to store as a proportion "
                         "of the histogram maximum: bins with less weight "
                         "will not be stored.")
parser.add_argument('--batch-size', type=int, default=1000000)
parser.add_argument('--param-bin-dtype', default='int32',
                    help="Type to use to store param_bin information. "
                         "Affects the maximum values that can be taken by "
                         "bin parameters. Default int32.",
                    choices=['int8', 'int16', 'int32'])
parser.add_argument('--beta-bins', type=int, default=10,
                    help="The number of bins to split the beta parameter "
                         "into, each will produce one histogram.")
parser.add_argument('--beta-min', type=float, default=0.,
                    help="The minimum value for the beta parameter in radians.")
parser.add_argument('--beta-max', type=float, default=np.pi,
                    help="The maximum value for the beta parameter in radians.")
parser.add_argument('--num-comps', type=int, default=5,
                    help="The number of harmonics to include in the histograms.")
args = parser.parse_args()

# Approximate error for small SNR ratios
sref = args.snr_reference
serr = args.snr_uncertainty / sref
swidth = serr / args.bin_density

# Approximate phase error at lower SNRs
perr = np.arctan(args.snr_uncertainty / sref)
pwidth = perr / args.bin_density

# Number of harmonics to be used
num_comps = min(args.num_comps, 5)

# Space bins geometrically over b = tan(beta/2)
bmin = np.tan(args.beta_min / 2.)
bmax = np.tan(args.beta_max / 2.)

bs = np.geomspace(bmin, bmax, num=args.beta_bins + 1, endpoint=True)
betas = 2. * np.arctan(bs)

# The choice of detector doesn't matter use H1
d = pycbc.detector.Detector('H1')

pycbc.init_logging(args.verbose)

np.random.seed(args.seed)
size = args.batch_size
chunks = int(args.sample_size / size) + 1

bin_dtype = args.param_bin_dtype
f = h5py.File(args.output_file, 'w')

# Generate a set of histograms for each beta bin
for bi in range(args.beta_bins):
    logging.info(f'Generating hists for beta bin {bi}')
    bmin = np.tan(betas[bi] / 2.)
    bmax = np.tan(betas[bi+1] / 2.)

    # Generate a set of samples and split them into the appropriate
    # histogram based on the loudest harmonic
    # Keep track of the total weight in each histogram and overall
    nproposal = 0
    nsamples = np.zeros(num_comps, dtype=int)
    wproposal = 0
    wsamples = np.zeros(num_comps, dtype=float)
    srweights = [{} for i in range(num_comps)]
    dpweights = [{} for i in range(num_comps)]

    # Draw samples until each histogram containes the required number of samples
    # or until the largest weight possible is less than the minimum threshold
    for i in range(chunks):
        logging.info(f'Chunk {i+1}/{chunks}')
        logging.info(f'generating {size} samples')

        # Choose random sky location and polarizations from
        # an isotropic population
        ra = uniform(0., 2. * np.pi, size=size)
        dec = np.arccos(uniform(-1., 1., size=size)) - np.pi / 2.
        psi = uniform(0., 2. * np.pi, size=size)
    
        # Choose random coalescence phase, precession phase and thetaJN
        phi = uniform(0., 2. * np.pi, size=size)
        alpha = uniform(0., 2. * np.pi, size=size)
        theta = np.arccos(uniform(-1., 1., size=size))
    
        # Choose a random thetaJL
        b = np.exp(uniform(np.log(bmin), np.log(bmax), size=size))
        beta = 2. * np.arctan(b)

        # calculate the toa, poa, and amplitude of each sample
        fp, fc = d.antenna_pattern(ra, dec, psi, 0)
        base_factors = get_factors(fp, fc, psi, phi, alpha, theta, beta,
                                   num_comps=num_comps)

        # calculate weights before rescaling to add noise
        amps = (base_factors.real ** 2. + base_factors.imag ** 2.) ** 0.5
        snrs = np.sum(amps ** 2., axis=1) ** 0.5
        w = snrs ** 3.

        # rescale the total SNR to the reference and add gaussian noise
        base_factors *= args.snr_reference / snrs[:, None]

        for ni in range(args.noise_per_sample):
            logging.info(f'Applying noise iteration {ni + 1}/{args.noise_per_sample}')
            fsize = base_factors.shape
            factors = (
                base_factors 
                + normal(scale=args.snr_uncertainty, size=fsize)
                + normal(scale=args.snr_uncertainty, size=fsize) * 1.j
            )
    
            amps = (factors.real ** 2. + factors.imag ** 2.) ** 0.5
            phases = np.arctan2(factors.imag, factors.real)

            nproposal += size
            wproposal += np.sum(w)

            # For each harmonic take the samples where it is the loudest
            for nc in range(num_comps):
                logging.info(f'Binning samples for harmonic {nc}')

                lgc = np.argmax(amps, axis=1) == nc
                namps = amps[lgc, :]
                nphases = phases[lgc, :]
                nw = w[lgc]

                nsamples[nc] += len(nw)
                wsamples[nc] += np.sum(nw)
    
                # Bin the data
                bind = []
                for i in range(num_comps):
                    if i == nc:
                        continue
                    sr = (namps[:, i] / namps[:, nc])
                    dp = (nphases[:, i] - nphases[:, nc]) % (2. * np.pi)
                    bind += [(sr / swidth).astype(int)]
                    bind += [(dp / pwidth).astype(int)]
                
                # Calculate and sum the weights for each bin
                # use first ifo as reference for weights
                for i, key in enumerate(zip(*bind)):
                    srkey = key[0::2]
                    if srkey not in srweights[nc]:
                        srweights[nc][srkey] = 0
                    srweights[nc][srkey] += nw[i]
    
                    dpkey = key[1::2]
                    if dpkey not in dpweights[nc]:
                        dpweights[nc][dpkey] = 0
                    dpweights[nc][dpkey] += nw[i]

                logging.info(f'Collected {nsamples[nc]} / {args.sample_size}')
                logging.info(f'sr bins: {len(srweights[nc])},\t dp bins: {len(dpweights[nc])}')

    for name, weights, width in zip(['dp', 'sr'], [dpweights, srweights], [pwidth, swidth]):
        for nc in range(num_comps):
            logging.info(f"finalising harmonic {nc} {name} hists")
            nweights = weights[nc]

            logging.info('converting to numpy arrays and normalizing')
            keys = np.array(list(nweights.keys()))
            values = np.array(list(nweights.values()))
            values /= wsamples[nc]
    
            logging.info('discarding bins below threshold to limit storage')
            l = values > args.weight_threshold
            if np.sum(l) > 0:
                keys = keys[l].astype(bin_dtype)
                values = values[l].astype(np.float32)
            else:
                keys = np.zeros((1, num_comps), dtype=int)
                values = np.array([args.weight_threshold])
            logging.info(f'Final length {name}: {len(keys)}')

            logging.info('Presorting by keys for downstream use')
            ncol = keys.shape[1]
            pdtype = [('c%s' % i, bin_dtype) for i in range(ncol)]
            keys_bin = np.zeros(len(values), dtype=pdtype)
            for i in range(ncol):
                keys_bin['c%s' % i] = keys[:, i]
            lsort = keys_bin.argsort()
            keys_bin = keys_bin[lsort]
            values = values[lsort]

            # Divide by bin volume to get PDF
            values /= (width ** (num_comps - 1.))
    
            logging.info('Writing results to file')
            f.create_dataset(f'{bi}/{nc}/{name}/param_bin', data=keys_bin, compression='gzip',
                             compression_opts=7)
            f.create_dataset(f'{bi}/{nc}/{name}/weights', data=values, compression='gzip',
                             compression_opts=7)
            f[f'{bi}/{nc}/{name}'].attrs['prob'] = wsamples[nc] / wproposal

f.attrs['pwidth'] = pwidth
f.attrs['swidth'] = swidth
f.attrs['beta_bins'] = betas
f.attrs['num_comps'] = num_comps
f.attrs['stat'] = 'phasetd_newsnr'

logging.info('Done')
