#! /usr/bin/env python

# Copyright (C) 2016 Christopher M. Biwer
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

import argparse
import logging
import numpy
import pycbc.opt
import pycbc.weave
import random
from pycbc import DYN_RANGE_FAC, fft, inference, psd, scheme, strain, waveform
from pycbc.filter import autocorrelation
from pycbc.io.inference_hdf import InferenceFile, read_label_from_config
from pycbc.types import FrequencySeries, MultiDetOptionAction
from pycbc.workflow import WorkflowConfigParser

def select_waveform_generator(approximant):
    """ Returns the generator for the approximant.
    """
    if approximant in waveform.fd_approximants():
        return waveform.FDomainCBCGenerator
    elif approximant in waveform.td_approximants():
        return waveform.TDomainCBCGenerator
    elif approximant in waveform.ringdown_fd_approximants:
        return waveform.FDomainRingdownGenerator
    elif approximant in waveform.ringdown_td_approximants:
        raise ValueError("Time domain ringdowns not supported")
    else:
        raise ValueError("%s is not a valid approximant."%approximant)

def read_distributions_from_config(cp, section):
    """ Returns a list of PyCBC distribution instances for a section in the
    configuration file.
    """
    dists = []
    variable_args = []
    for subsection in cp.get_subsections(section):
        name = cp.get_opt_tag(section, "name", subsection)
        dist = inference.priors[name].from_config(cp, section, subsection)
        if set(dist.params).isdisjoint(variable_args):
            dists.append(dist)
            variable_args += dist.params
        else:
            raise ValueError("Same parameter in more than one distribution.")
    return dists

# command line usage
parser = argparse.ArgumentParser(usage="pycbc_inference [--options]",
                  description="Runs an sampler to find the posterior.")

# add data options
parser.add_argument("--instruments", type=str, nargs="+", required=True,
    help="IFOs, eg. H1 L1.")
parser.add_argument("--frame-type", type=str, nargs="+",
    action=MultiDetOptionAction, metavar="IFO:FRAME_TYPE",
    help="Frame type for each IFO.")
parser.add_argument("--low-frequency-cutoff", type=float, required=True,
    help="Low frequency cutoff for each IFO.")

# add inference options
parser.add_argument("--sampler", required=True,
    choices=inference.samplers.keys(),
    help="Sampler class to use for finding posterior.")
parser.add_argument("--likelihood-evaluator", required=True,
    choices=inference.likelihood_evaluators.keys(),
    help="Evaluator class to use to calculate the likelihood.")
parser.add_argument("--nwalkers", type=int, required=True,
    help="Number of walkers to use in sampler.")
parser.add_argument("--niterations", type=int, required=True,
    help="Number of iterations to perform after burn in.")
parser.add_argument("--nprocesses", type=int, default=None,
    help="Number of processes to use. If not given then use maximum.")
parser.add_argument("--skip-burn-in", action="store_true", default=False,
    help="Do not burn in with sampler.")

# add config options
parser.add_argument("--config-files", type=str, nargs="+", required=True,
    help="A file parsable by pycbc.workflow.WorkflowConfigParser.")

# output options
parser.add_argument("--output-file", type=str, required=True,
    help="Output file path.")
parser.add_argument("--checkpoint-interval", type=int, default=None,
    help="Number of iterations to take before saving new samples to file.")

# verbose option
parser.add_argument("--verbose", action="store_true", default=False,
    help="")

# add module pre-defined options
fft.insert_fft_option_group(parser)
pycbc.opt.insert_optimization_option_group(parser)
psd.insert_psd_option_group_multi_ifo(parser)
scheme.insert_processing_option_group(parser)
strain.insert_strain_option_group_multi_ifo(parser)
pycbc.weave.insert_weave_option_group(parser)

# parse command line
opts = parser.parse_args()

# verify options are sane
fft.verify_fft_options(opts, parser)
pycbc.opt.verify_optimization_options(opts, parser)
#psd.verify_psd_options(opts, parser)
scheme.verify_processing_options(opts, parser)
#strain.verify_strain_options(opts, parser)
pycbc.weave.verify_weave_options(opts, parser)

# setup log
if opts.verbose:
    log_level = logging.DEBUG
else:
    log_level = logging.WARN
logging.basicConfig(format="%(asctime)s : %(message)s", level=log_level)

# change measure level for FFT to 0
fft.fftw.set_measure_level(0)

# get scheme
ctx = scheme.from_cli(opts)
fft.from_cli(opts)

# get strain time series
strain_dict = strain.from_cli_multi_ifos(opts, opts.instruments,
                                         precision="double")

with ctx:

    # FFT strain and save each of the length of the FFT, delta_f, and
    # low frequency cutoff to a dict
    logging.info("FFT strain")
    stilde_dict = {}
    length_dict = {}
    delta_f_dict = {}
    low_frequency_cutoff_dict = {}
    for ifo in opts.instruments:
        stilde_dict[ifo] = strain_dict[ifo].to_frequencyseries()
        length_dict[ifo] = len(stilde_dict[ifo])
        delta_f_dict[ifo] = stilde_dict[ifo].delta_f
        low_frequency_cutoff_dict[ifo] = opts.low_frequency_cutoff

    # get PSD as frequency series
    psd_dict = psd.from_cli_multi_ifos(opts, length_dict, delta_f_dict,
                                   low_frequency_cutoff_dict, opts.instruments,
                                   strain_dict=strain_dict, precision="double")

    # apply dynamic range factor for saving PSDs since plotting code expects it
    logging.info("Saving PSDs")
    psd_dyn_dict = {}
    for key,val in psd_dict.iteritems():
         psd_dyn_dict[key] = FrequencySeries(psd_dict[key]*DYN_RANGE_FAC**2,
                                             delta_f=psd_dict[key].delta_f)

    # save PSD
    with InferenceFile(opts.output_file, "w") as fp:
        fp.write_psds(psds=psd_dyn_dict,
                      low_frequency_cutoff=low_frequency_cutoff_dict)

    # read configuration file
    logging.info("Reading configuration file")
    cp = WorkflowConfigParser(opts.config_files)

    # sanity check that each parameter in [variable_args] has a priors section
    variable_args = cp.options("variable_args")
    subsections = cp.get_subsections("prior")
    tags = numpy.concatenate([tag.split("+") for tag in subsections])
    if not any(param in tags for param in variable_args):
        raise KeyError("You are missing a priors section in the config file.")

    # get parameters that do not change in sampler
    static_args = dict([(key,cp.get_opt_tags("static_args",key,[])) \
                                         for key in cp.options("static_args")])
    for key,val in static_args.iteritems():
        try:
            static_args[key] = float(val)
        except:
            pass

    # get labels for each parameter from configuration file
    labels = [read_label_from_config(cp, param) for param in variable_args]

    # get prior distribution for each variable parameter
    logging.info("Setting up priors for each parameter")
    distributions = read_distributions_from_config(cp, "prior")

    # construct class that will return the prior
    prior = inference.PriorEvaluator(variable_args, *distributions)

    # select generator that will generate waveform
    # for likelihood evaluator
    logging.info("Setting up sampler")
    generator_function = select_waveform_generator(static_args["approximant"])

    # construct class that will generate waveforms
    generator = waveform.FDomainDetFrameGenerator(generator_function,
                        epoch=stilde_dict.values()[0].epoch,
                        variable_args=variable_args,
                        detectors=opts.instruments,
                        delta_f=delta_f_dict.values()[0], **static_args)

    # construct class that will return the natural logarithm of likelihood
    likelihood = inference.likelihood_evaluators[opts.likelihood_evaluator](generator,
                        stilde_dict, opts.low_frequency_cutoff,
                        psds=psd_dict, prior=prior)

    # create sampler that will run
    ndim = len(variable_args)
    sampler = inference.samplers[opts.sampler](likelihood, ndim=ndim,
                                               nwalkers=opts.nwalkers,
                                               processes=opts.nprocesses)

    # get distribution to draw from for each walker initial position
    logging.info("Setting walkers initial conditions for varying parameters")

    # check if user wants to use different distributions than prior for setting
    # walkers initial positions
    if len(cp.get_subsections("initial")):
        initial_distributions = read_distributions_from_config(cp, "initial")
    else:
        initial_distributions = read_distributions_from_config(cp, "prior")

    # set initial values for variable parameters for each walker
    # loop over all walkers and then parameters
    # find the distribution that has that parameter in it and draw a
    # random value from the distribution
    p0 = numpy.ones(shape=(opts.nwalkers, ndim))
    for i,_ in enumerate(p0):
        for j,param in enumerate(variable_args):
            for idist in initial_distributions:
                if param in idist.params:
                    p0[i][j] = idist.rvs(size=1, param=param)[0][0]
                    break

    # setup checkpointing
    if opts.checkpoint_interval:

        # determine intervals to run sampler until we save the new samples 
        intervals = [i for i in range(0, opts.niterations, opts.checkpoint_interval)] \
                    + [opts.niterations]

        # determine if there is a small bit at the end
        remainder = opts.niterations % opts.checkpoint_interval
        if remainder:
            intervals += [intervals[-1]+remainder]

    # if not checkpointing then set intervals to run sampler in one call
    else:
        intervals = [0, opts.niterations]

    intervals = numpy.array(intervals)

    # check if user wants to skip the burn in
    # if burn in then switch p0 to None and sampler should pick up from last
    # position after burn in
    if not opts.skip_burn_in:
        logging.info("Burn in")
        sampler.burn_in(p0)
        p0 = None
        n_burnin = sampler.burn_in_iterations
        logging.info("Used %i burn in samples" % n_burnin)
    else:
        n_burnin = 0

    with InferenceFile(opts.output_file, "a") as fp:
        # create arrays in the file for the burnin + samples
        fp.write_samples(variable_args, nwalkers=opts.nwalkers,
                         niterations=opts.niterations+n_burnin,
                         labels=labels)
        fp.write_acceptance_fraction(niterations=opts.niterations+n_burnin)
        # write the burn in
        if not opts.skip_burn_in:
            fp.write_samples_from_sampler(sampler, start=0, end=n_burnin)
            fp.write_acceptance_fraction(data=sampler.acceptance_fraction,
                                         start=0, end=n_burnin)

    # write sampler attributes to file
    with InferenceFile(opts.output_file, "a") as fp:
        fp.write_sampler_attrs(sampler)

    # increase the intervals to account for the burn in
    intervals += n_burnin

    # loop over number of checkpoints
    for i,start in enumerate(intervals[:-1]):

        end = intervals[i+1]

        # run sampler and set initial values to None so that sampler
        # picks up from where it left off next call
        logging.info("Running sampler for %i to %i out of %i iterations"%(
            start-n_burnin,end-n_burnin,opts.niterations))
        sampler.run(end-start, p0=p0)
        p0 = None

        # write new samples
        with InferenceFile(opts.output_file, "a") as fp:
            fp.write_samples_from_sampler(sampler, start=start, end=end)
            fp.write_acceptance_fraction(data=sampler.acceptance_fraction,
                                         start=start, end=end)

# calculate ACL for each series of samples for each parameter for each walker
logging.info("Calculating maximum ACL")
acls = []
with InferenceFile(opts.output_file, "r") as fp:
    for i in range(opts.nwalkers):
        for param in fp.variable_args:
            acls.append( autocorrelation.calculate_acl(
                fp.read_samples(param, walkers=i)[param], dtype=int) )

# get max ACL to save to file
# if ACL is infinity then set it to the length of the chain of samples
max_acl = numpy.array(acls).max()
if max_acl == numpy.inf:
    max_acl = opts.niterations

# write ACL as an int rounding up
logging.info("Writing ACL of %f samples to file"%max_acl)
with InferenceFile(opts.output_file, "a") as fp:
    fp.write_acl(max_acl)

# exit
logging.info("Done")
