#!/bin/env python
# Copyright (C) 2015-2023 Alexander Harvey Nitz, Gareth Cabourn Davies
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
""" Prepare files for upload to GraceDB for foreground events
"""
import os, sys, argparse, logging, re, h5py, pycbc.workflow as wf
from pycbc.results import layout
from pycbc.types import MultiDetOptionAction
from pycbc.events import select_segments_by_definer, coinc
from pycbc.io import get_all_subkeys
import pycbc.workflow.minifollowups as mini
from pycbc.workflow.core import resolve_url_to_file
import numpy as np
import pycbc.version

def get_params(curr_idx, times, tids, bank_id, bank_data, fsdt):
    params = {}
    for ifo in times:
        params['%s_end_time' % ifo] = times[ifo][curr_idx]
        try:
            # Only present for precessing case
            params['u_vals_%s'%ifo] = \
                                 fsdt[ifo][ifo]['u_vals'][tids[ifo][curr_idx]]
        except:
            pass
    params['mean_time'] = coinc.mean_if_greater_than_zero([times[ifo][curr_idx]
                                                    for ifo in times])[0]

    params['mass1'] = bank_data['mass1'][bank_id]
    params['mass2'] = bank_data['mass2'][bank_id]
    params['spin1z'] = bank_data['spin1z'][bank_id]
    params['spin2z'] = bank_data['spin2z'][bank_id]
    params['f_lower'] = bank_data['f_lower'][bank_id]
    params['approximant'] = bank_data['approximant'][bank_id]
    # don't require precessing template info if not present
    try:
        params['spin1x'] = bank_data['spin1x'][bank_id]
        params['spin1y'] = bank_data['spin1y'][bank_id]
        params['spin2x'] = bank_data['spin2x'][bank_id]
        params['spin2y'] = bank_data['spin2y'][bank_id]
        params['inclination'] = bank_data['inclination'][bank_id]
    except KeyError:
        pass
    return params

parser = argparse.ArgumentParser(description=__doc__[1:])
parser.add_argument('--verbose', action='count',
                    help='Add progressively more verbose output, '
                         'default=info')
parser.add_argument('--version', action='version', version=pycbc.version.git_verbose_msg)
parser.add_argument('--bank-file',
                    help="HDF format template bank file")
parser.add_argument('--statmap-file',
                    help="HDF format clustered coincident trigger result file")
parser.add_argument('--xml-all-file',
                    help="XML format result file containing all events")
parser.add_argument('--single-detector-triggers', nargs='+', action=MultiDetOptionAction,
                    help="HDF format merged single detector trigger files")
parser.add_argument('--inspiral-segments',
                    help="xml segment files containing the inspiral analysis times")
parser.add_argument('--inspiral-data-read-name',
                    help="Name of inspiral segmentlist containing data read in "
                         "by each analysis job.")
parser.add_argument('--inspiral-data-analyzed-name',
                    help="Name of inspiral segmentlist containing data "
                         "analyzed by each analysis job.")
parser.add_argument('--psd-files', nargs='+', action=MultiDetOptionAction,
                    help="HDF format merged single detector PSD files")
parser.add_argument('--ifar-thresh', type=float,
                    help="IFAR threshold for preparing SNR timeseries "
                         "files for upload. Default=No upload prep")

wf.add_workflow_command_line_group(parser)
wf.add_workflow_settings_cli(parser, include_subdax_opts=True)
args = parser.parse_args()

if args.verbose:
    args.verbose += 1
else:
    args.verbose = 1
pycbc.init_logging(args.verbose)

workflow = wf.Workflow(args)

wf.makedir(args.output_dir)

# create a FileList that will contain all output files
layouts = []

tmpltbank_file = resolve_url_to_file(os.path.abspath(args.bank_file))
insp_segs = resolve_url_to_file(os.path.abspath(args.inspiral_segments))
xml_all = resolve_url_to_file(os.path.abspath(args.xml_all_file))

single_triggers = []
psd_files = []
fsdt = {}
insp_data_seglists = {}
insp_analysed_seglists = {}
for ifo in args.single_detector_triggers:
    strig_fname = args.single_detector_triggers[ifo]
    strig_file = resolve_url_to_file(os.path.abspath(strig_fname),
                                     attrs={'ifos': ifo})
    single_triggers.append(strig_file)

    psd_fname = args.psd_files[ifo]
    psd_file = resolve_url_to_file(os.path.abspath(psd_fname),
                                     attrs={'ifos': ifo})
    psd_files.append(psd_file)

    fsdt[ifo] = h5py.File(args.single_detector_triggers[ifo], 'r')
    insp_data_seglists[ifo] = select_segments_by_definer(
        args.inspiral_segments,
        segment_name=args.inspiral_data_read_name,
        ifo=ifo)
    insp_analysed_seglists[ifo] = select_segments_by_definer(
        args.inspiral_segments,
        segment_name=args.inspiral_data_analyzed_name,
        ifo=ifo)
    # NOTE: make_singles_timefreq needs a coalesced set of segments. If this is
    #       being used to determine command-line options for other codes,
    #       please think if that code requires coalesced, or not, segments.
    insp_data_seglists[ifo].coalesce()
    insp_analysed_seglists[ifo].coalesce()

f = h5py.File(args.statmap_file, 'r')
stat = f['foreground/stat'][:]

bank_data = h5py.File(args.bank_file, 'r')

ifar_limit = args.ifar_thresh
# Get indices of all events which pass the IFAR threshold
event_ifars = f['foreground/ifar'][:]
events_to_read = np.count_nonzero(event_ifars > ifar_limit)
# Sort by IFAR, descending
event_idx = event_ifars.argsort()[::-1][:events_to_read]
# Times and tids need to be reset for this set of events:
times = {}
tids = {}
bank_ids = {}

ifo_list = f.attrs['ifos'].split(' ')
for ifo in ifo_list:
    times[ifo] = f[f'foreground/{ifo}/time'][:][event_idx]
    tids[ifo] = f[f'foreground/{ifo}/trigger_id'][:][event_idx]
bank_ids = f['foreground/template_id'][:][event_idx]

for curr_idx in range(event_idx.size):
    params = get_params(curr_idx, times, tids, bank_ids[curr_idx], bank_data, fsdt)
    # Extract approximant
    appx = params['approximant']
    del params['approximant']

    single_temp_files = []
    for ifo in ifo_list:
        if params['mean_time'] not in insp_analysed_seglists[ifo]:
            logging.info("Mean time %.3f not in segment list",
                         params['mean_time'])
            continue
        # Make single-template files to put into the XML file for upload
        single_temp_files += mini.make_single_template_files(
            workflow,
            insp_segs,
            ifo,
            args.inspiral_data_read_name,
            args.inspiral_data_analyzed_name,
            params,
            args.output_dir,
            store_file=True,
            tags=args.tags+['upload', str(curr_idx)],
        )

    mini.make_upload_files(
        workflow,
        psd_files,
        single_temp_files,
        xml_all,
        curr_idx,
        appx,
        args.output_dir,
        tags=args.tags+['upload', str(curr_idx)]
    )

workflow.save()
