#!/usr/bin/env python

# Copyright (C) 2016 Christopher M. Biwer, Alexander Harvey Nitz
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
""" Creates a DAX for a parameter estimation workflow.
"""

import argparse
import h5py
import logging
import os
import Pegasus.DAX3 as dax
import pycbc
import pycbc.workflow.minifollowups as mini
import pycbc.workflow.pegasus_workflow as wdax
import socket
import sys
import numpy
from ligo import segments
from pycbc import results
from pycbc.results import layout
from pycbc.types import MultiDetOptionAction
from pycbc.types import MultiDetOptionAppendAction
from pycbc.workflow import configuration
from pycbc.workflow import core
from pycbc.workflow import datafind
from pycbc.workflow import plotting

from pycbc import __version__

# FIXME see https://github.com/gwastro/pycbc/issues/2349
import pycbc.workflow.inference_followups as inffu

def load_inference_configs(config_cache):
    """Loads configuration files from the given config file.
    
    Each line in the cache file should be a comma-separated list of label,
    config file(s), overrides, deletions. If multiple config files are to be
    used, they should be space separted. Likewise, if multiple overrides or
    deletions are provided, they should be space separated. Overrides should be
    formatted as ``SECTION:OPTION:VALUE``, deletions as ``SECTION:OPTION``.
    For example::

        Event 1,config1.ini config2.ini,foo:bar:2 life:meaning:42,knights:ni

    Labels must be unique across all events. Otherwise, a ``ValueError`` is
    raised.

    Parameters
    ----------
    config_cache : str
        The name of the cache file.

    Returns
    -------
    labels : list
        List of strings giving a label for each event.
    config_files : list
        List of the config files. Each element is itself a list providing
        all of the config files to combine.
    overrides : list
        List of the overrides. Each element is itself a list of tuples giving
        (section, option, value). If no overrides were provided for an
        entry, that element will be ``None``.
    deletions : list
        List of the deletions. Each element is itself a list of tuples giving
        (section, option). If no deletions were provided for an entry, that
        element will be ``None``.
        List of config files, overrides, and deletions. Config
    """
    labels = []
    config_files = []
    overrides = []
    deletions = []
    with open(config_cache, 'r') as fp:
        for line in fp:
            if line.startswith('#'):
                continue
            line = line.replace('\n', '')
            lbl, cfs, ovs, dels = line.split(',')
            cfs = map(os.path.abspath, cfs.strip().split())
            ovs = [tuple(o.split(':')) for o in ovs.strip().split()]
            if ovs == []:
                ovs = None
            dels = [tuple(d.split(':')) for d in dels.strip().split()]
            if dels == []:
                dels = None
            labels.append(lbl.strip())
            config_files.append(cfs)
            overrides.append(ovs)
            deletions.append(dels)
    # check that the labels are unique
    if not len(set(labels)) == len(labels):
        raise ValueError("all labels in config cache must be unique")
    return labels, config_files, overrides, deletions


def to_file(path, ifo=None):
    """ Takes a str and returns a pycbc.workflow.pegasus_workflow.File
    instance.
    """
    fil = wdax.File(os.path.basename(path))
    fil.ifo = ifo
    path = os.path.abspath(path)
    fil.PFN(path, "local")
    return fil

def symlink_path(f, path):
    """ Symlinks a path.
    """
    if f is None:
        return
    try:
        os.symlink(f.storage_path, os.path.join(path, f.name))
    except OSError:
        pass

# command line parser
parser = argparse.ArgumentParser(description=__doc__[1:])

# version option
parser.add_argument("--version", action="version", version=__version__,
                    help="Prints version information.")

# workflow options
parser.add_argument("--workflow-name", default="my_unamed_inference_run",
                    help="Name of the workflow to append in various places.")
parser.add_argument("--tags", nargs="+", default=[],
                    help="Tags to apend in various places.")
parser.add_argument("--output-dir", default=None,
                    help="Path to output directory.")
parser.add_argument("--output-map", default=None,
                    help="Path to output map file.")
parser.add_argument("--transformation-catalog", default=None,
                    help="Path to transformation catalog file.")
parser.add_argument("--output-file", default=None,
                    help="Path to DAX file.")
# input configuration file options
cgroup = parser.add_mutually_exclusive_group(required=True)
cgroup.add_argument("--inference-config-file", type=str, nargs='+',
                    help="Configuration file(s) for the inference job. Must "
                         "provide either this, or an inference-config-cache.")
cgroup.add_argument("--inference-config-cache", type=str,
                    help="A cache file listing configuration files for "
                         "multiple events. Each line in the cache represents "
                         "a single event, and should provide a space-"
                         "separated list of config files to use for that "
                         "event. If provided, will a separate inference "
                         "analysis will be created for each event. Must "
                         "provide either this or inference-config-file.")

# add option groups
configuration.add_workflow_command_line_group(parser)

# parser command line
opts = parser.parse_args()

# get inference configuration file(s)
if opts.inference_config_file is not None:
    # means we're only anlyzing a single group
    infconfig_files = [opts.inference_config_file]
    infconfig_overrides = [None]
    infconfig_deletions = [None]
    elabels = [None]
else:
    # load the inference config cache
    elabels, infconfig_files, infconfig_overrides, infconfig_deletions = \
        load_inference_configs(opts.inference_config_cache)

config_file_tmplt = 'config_files/inference-{}.ini'

# make data output directory
core.makedir(opts.output_dir)
core.makedir('{}/{}'.format(opts.output_dir, 'config_files'))

# the file we'll store all output files in
filedir = 'output'

# log to terminal until we know where the path to log output file
log_format = "%(asctime)s:%(levelname)s : %(message)s"
logging.basicConfig(format=log_format, level=logging.INFO)

# create workflow and sub-workflows
container = core.Workflow(opts, opts.workflow_name)
workflow = core.Workflow(opts, opts.workflow_name + "-main")
finalize_workflow = core.Workflow(opts, opts.workflow_name + "-finalization")

# change working directory to the output
os.chdir(opts.output_dir)

# sections for output HTML pages
rdir = layout.SectionNumber("results",
                            ["detector_sensitivity", "priors", "posteriors",
                             "diagnostics", "config_files", "workflow"])

# make results directories
core.makedir(rdir.base)
core.makedir(rdir["workflow"])
core.makedir(rdir["config_files"])

# create files for workflow log
log_file_txt = core.File(workflow.ifos, "workflow-log", workflow.analysis_time,
                      extension=".txt", directory=rdir["workflow"])
log_file_html = core.File(workflow.ifos, "WORKFLOW-LOG", workflow.analysis_time,
                        extension=".html", directory=rdir["workflow"])

# switch saving log to file
logging.basicConfig(format=log_format, level=logging.INFO,
                    filename=log_file_txt.storage_path, filemode="w")
log_file = logging.FileHandler(filename=log_file_txt.storage_path, mode="w")
log_file.setLevel(logging.INFO)
formatter = logging.Formatter(log_format)
log_file.setFormatter(formatter)
logging.getLogger("").addHandler(log_file)
logging.info("Created log file %s" % log_file_txt.storage_path)


# construct Executable for running sampler
inference_exe = core.Executable(workflow.cp, "inference",
                                ifos=workflow.ifos, out_dir=filedir)
 
# figure out what parameters user wants to plot from workflow configuration
plot_parameters = {}
for option in workflow.cp.options("workflow-inference"):
    if option.startswith("plot-1d-"):
        group = option.replace("plot-1d-", "").replace("-", "_")
        plot_parameters[group] = workflow.cp.get_opt_tag(
                                "workflow", option, "inference").split(" ")
all_parameters = [param for group in plot_parameters.values()
                  for param in group]

# loop over number of loudest events to be analyzed
for num_event, config_fnames in enumerate(infconfig_files):
    label = elabels[num_event]
    if label is None:
        label = 'Event {}'.format(num_event)
    overrides = infconfig_overrides[num_event]
    deletions = infconfig_deletions[num_event]
    # write the configuration file to the config files directory
    cp = configuration.WorkflowConfigParser(config_fnames,
                                            overrideTuples=overrides,
                                            deleteTuples=deletions)
    with open(config_file_tmplt.format(num_event), 'w') as ff:
        cp.write(ff)
    config_file = to_file(config_file_tmplt.format(num_event))

    # make node for running sampler
    node = inference_exe.create_node()
    node.add_input_opt("--config-file", config_file)
    analysis_index = segments.segment(num_event, num_event+1)
    inference_file = node.new_output_file_opt(analysis_index, ".hdf",
                                              "--output-file")

    # add node to workflow
    workflow += node

    # files for posteriors summary subsection
    base = "posteriors/{}".format(label)
    post_table_files = inffu.make_inference_summary_table(
        workflow, inference_file, rdir[base],
        variable_params=all_parameters,
        analysis_seg=analysis_index,
        tags=opts.tags)
    post_files = inffu.make_inference_posterior_plot(
        workflow, inference_file, rdir[base],
        parameters=all_parameters,
        analysis_seg=analysis_index,
        tags=opts.tags)
    layout.single_layout(rdir[base], post_table_files + post_files)

    # files for posteriors 1d_posteriors subsection
    groups = plot_parameters.keys()
    groups.sort()
    for group in groups:
        parameters = plot_parameters[group]
        base = "posteriors/1d_{}_posteriors".format(group)
        post_1d_files = inffu.make_inference_1d_posterior_plots(
                          workflow, inference_file, rdir[base],
                          parameters=parameters,
                          analysis_seg=analysis_index,
                          tags=opts.tags + [str(num_event), group])
        layout.group_layout(rdir[base], post_1d_files)

    if "inference_samples" in workflow.cp.options("executables"):
        # files for samples summary subsection
        base = "diagnostics/samples"
        samples_files = []
        for parameter in all_parameters:
            samples_files += inffu.make_inference_samples_plot(
                              workflow, inference_file, rdir[base],
                              parameters=[parameter], analysis_seg=analysis_index,
                              tags=opts.tags + [str(num_event), parameter])
        layout.group_layout(rdir[base], samples_files)

    if "inference_rate" in workflow.cp.options("executables"):
        # files for samples acceptance_rate subsection
        base = "diagnostics/acceptance_rate"
        acceptance_files = inffu.make_inference_acceptance_rate_plot(
                              workflow, inference_file, rdir[base],
                              analysis_seg=analysis_index,
                              tags=opts.tags + [str(num_event)])
        layout.single_layout(rdir[base], acceptance_files)

    # files for detector_sensitivity summary subsection
    base = "detector_sensitivity/{}".format(label)
    psd_file = plotting.make_spectrum_plot(
        workflow, [inference_file], rdir[base],
        tags=opts.tags + [str(num_event)],
        hdf_group="data")
    layout.single_layout(rdir[base], [psd_file])

    # files for priors summary section
    base = "priors/{}".format(label)
    prior_files = []
    for subsection in cp.get_subsections('prior'):
        prior_files += inffu.make_inference_prior_plot(
            workflow, config_file, rdir[base],
            analysis_seg=workflow.analysis_time,
            parameters=[subsection],
            tags=opts.tags+[str(num_event), subsection])
    layout.group_layout(rdir[base], prior_files)

# files for overall summary section
summ_files = post_table_files + post_files
for summ_file in summ_files:
    symlink_path(summ_file, rdir.base)
layout.single_layout(rdir.base, summ_files)

# create versioning HTML pages
results.create_versioning_page(rdir["workflow/version"], container.cp)

# create node for making HTML pages
plotting.make_results_web_page(finalize_workflow,
    os.path.join(os.getcwd(), rdir.base))

# add sub-workflows to workflow
container += workflow
container += finalize_workflow

# make finalize sub-workflow depend on main sub-workflow
dep = dax.Dependency(parent=workflow.as_job, child=finalize_workflow.as_job)
container._adag.addDependency(dep)

# write dax
outputfile = "{}.dax".format(opts.workflow_name) if opts.output_file == None \
    else opts.output_file
container.save(filename=outputfile, output_map_path=opts.output_map,
               transformation_catalog_path=opts.transformation_catalog)

# save workflow configuration file
base = rdir["workflow/configuration"]
core.makedir(base)
wf_ini = workflow.save_config("workflow.ini", base, container.cp)
layout.single_layout(base, wf_ini)

# close the log and flush to the html file
logging.shutdown()
with open (log_file_txt.storage_path, "r") as log_file:
    log_data = log_file.read()
log_str = """
<p>Workflow generation script created workflow in output directory: %s</p>
<p>Workflow name is: %s</p>
<p>Workflow generation script run on host: %s</p>
<pre>%s</pre>
""" % (os.getcwd(), opts.workflow_name, socket.gethostname(), log_data)
kwds = {"title" : "Workflow Generation Log",
        "caption" : "Log of the workflow script %s" % sys.argv[0],
        "cmd" : " ".join(sys.argv)}
results.save_fig_with_metadata(log_str, log_file_html.storage_path, **kwds)
layout.single_layout(rdir["workflow"], ([log_file_html]))
