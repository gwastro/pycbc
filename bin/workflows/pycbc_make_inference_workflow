#!/bin/env python

# Copyright (C) 2016 Christopher M. Biwer, Alexander Harvey Nitz
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
""" Creates a DAX for a parameter estimation workflow.
"""

import argparse
import h5py
import logging
import os
import pycbc.workflow as wf
import pycbc.workflow.inference_followups as inffu
import pycbc.workflow.minifollowups as mini
import pycbc.workflow.pegasus_workflow as wdax
import pycbc.version
from pycbc_glue import segments
from pycbc.results import layout
from pycbc.types import MultiDetOptionAction
from pycbc.types import MultiDetOptionAppendAction
from pycbc.workflow import WorkflowConfigParser

def to_file(path, ifo=None):
    """ Takes a str and returns a pycbc.workflow.pegasus_workflow.File
    instance.
    """
    fil = wdax.File(os.path.basename(path))
    fil.ifo = ifo
    path = os.path.abspath(path)
    fil.PFN(path, "local")
    return fil

# command line parser
parser = argparse.ArgumentParser(description=__doc__[1:])

# version option
parser.add_argument("--version", action="version",
                    version=pycbc.version.git_verbose_msg, 
                    help="Prints version information.")

# workflow options
parser.add_argument("--workflow-name", default="my_unamed_inference_run",
                    help="Name of the workflow to append in various places.")
parser.add_argument("--tags", nargs="+", default=[],
                    help="Tags to apend in various places.")
parser.add_argument("--output-dir", default=None,
                    help="Path to output directory.")
parser.add_argument("--output-map", required=True,
                    help="Path to output map file.")
parser.add_argument("--output-file", required=True,
                    help="Path to DAX file.")

# input workflow file options
parser.add_argument("--bank-file", default=None,
                    help="HDF format template bank file.")
parser.add_argument("--statmap-file", default=None,
                    help="HDF format clustered coincident trigger "
                         "result file.")
parser.add_argument("--single-detector-triggers", nargs="+", default=None,
                    action=MultiDetOptionAction,
                    help="HDF format merged single detector trigger files.")

# analysis time option
# only required if not using input workflow file options
parser.add_argument("--gps-end-time", type=float, nargs="+", default=None,
                    help="Times to analyze. If given workflow files then "
                         "this option is ignored.")

# input configuration file options
parser.add_argument("--inference-config-file", type=str, required=True,
                    help="workflow.WorkflowConfigParser parsable file with "
                         "proir information.")
parser.add_argument("--prior-section", type=str, default="prior",
                    help="Name of the section in inference configuration file "
                         "that contains priors.")

# input frame files
parser.add_argument("--frame-files", nargs="+", default=None,
                    action=MultiDetOptionAppendAction,
                    help="GWF frame files to use.")

# add option groups
wf.add_workflow_command_line_group(parser)

# parser command line
opts = parser.parse_args()

# setup log
logging.basicConfig(format="%(asctime)s:%(levelname)s : %(message)s", 
                    level=logging.INFO)

# sanity check that user is either using workflow or command line
workflow_options = opts.bank_file != None \
                   and opts.statmap_file != None \
                   and opts.single_detector_triggers != None
if not workflow_options and not (opts.gps_end_time != None):
    raise ValueError("Must use either workflow options or --gps-end-time")

# create workflow
workflow = wf.Workflow(opts, opts.workflow_name)

# create output directory
wf.makedir(opts.output_dir)

# typecast str from command line to File instances
config_file = to_file(opts.inference_config_file)

# if using workflow files to find analysis times
if workflow_options:

    # typecast str from command line to File instances
    tmpltbank_file = to_file(opts.bank_file)
    coinc_file = to_file(opts.statmap_file)
    single_triggers = []
    for ifo in opts.single_detector_triggers:
        fname = opts.single_detector_triggers[ifo]
        single_triggers.append( to_file(fname, ifo=ifo) )

    # get number of events to analyze
    num_events = int(workflow.cp.get_opt_tags("workflow-inference",
                                              "num-events", ""))

    # get detection statistic from statmap file
    # if less events that requested to analyze in config file
    # then analyze all events
    f = h5py.File(opts.statmap_file, "r")
    stat = f["foreground/stat"][:]
    if len(stat) < num_events:
        num_events = len(stat)

    # get index for sorted detection statistic
    sorting = stat.argsort()[::-1]

    # get times for loudest events
    times = {
        f.attrs["detector_1"]: f["foreground/time1"][:][sorting][0:num_events],
        f.attrs["detector_2"]: f["foreground/time2"][:][sorting][0:num_events],
    }

# else get analysis times from command line
else:
    times = dict([(ifo,opts.gps_end_time) for ifo in workflow.ifos])
    num_events = len(opts.gps_end_time)

# construct Executable for running sampler
inference_exe = wf.Executable(workflow.cp, "inference", ifos=workflow.ifos,
                         out_dir=opts.output_dir)

# get plotting parameters
cp = WorkflowConfigParser([opts.inference_config_file])
variable_args = cp.options("variable_args")

# add derived mass parameters if mass1 and mass2 in [variable_args]
if "mass1" in variable_args and "mass2" in variable_args:
    variable_args += ["mchirp", "eta"]
variable_args.sort()

# create a list that will contain all output files
layouts = []

# get channel names
channel_names = {}
for ifo in workflow.ifos:
    channel_names[ifo] = workflow.cp.get_opt_tags(
                               "workflow", "%s-channel-name" % ifo.lower(), "")
channel_names_str = " ".join([key + ":" + val for key, val in \
                              channel_names.iteritems()])

# loop over number of loudest events to be analyzed
for num_event in range(num_events):

    # set GPS times for reading in data around the event
    avg_end_time = sum([end_times[num_event] \
                          for end_times in times.values()]) / len(times.keys())
    seconds_before_time = int(workflow.cp.get_opt_tags("workflow-inference",
                                            "data-seconds-before-trigger", ""))
    seconds_after_time = int(workflow.cp.get_opt_tags("workflow-inference",
                                            "data-seconds-after-trigger", ""))
    gps_start_time = int(avg_end_time) - seconds_before_time
    gps_end_time = int(avg_end_time) + seconds_after_time

    # get dict of segments for each IFO
    seg_dict = {ifo : segments.segmentlist([segments.segment(
                      gps_start_time, gps_end_time)]) for ifo in workflow.ifos}

    # get frame files from command line or datafind server
    frame_files = wf.FileList([])
    if opts.frame_files:
        for ifo in workflow.ifos:
            for path in opts.frame_files[ifo]:
                frame_file = wf.File(
                                ifo, "FRAME",
                                segments.segment(gps_start_time, gps_end_time),
                                file_url="file://" + path)
                frame_file.PFN(frame_file.storage_path, site="local")
                frame_files.append(frame_file)
    else:
        frame_files, _, _, _ = wf.setup_datafind_workflow(workflow, seg_dict,
                                                          "datafind")

    # make node for running sampler
    node = inference_exe.create_node()
    node.add_opt("--instruments", " ".join(workflow.ifos))
    node.add_opt("--gps-start-time", gps_start_time)
    node.add_opt("--gps-end-time", gps_end_time)
    node.add_multiifo_input_list_opt("--frame-files", frame_files)
    node.add_opt("--channel-name", channel_names_str)
    node.add_input_opt("--config-file", config_file)
    analysis_time = segments.segment(gps_start_time, gps_end_time)
    inference_file = node.new_output_file_opt(analysis_time, ".hdf",
                                              "--output-file",
                                              tags=[str(num_event)])

    # add node to workflow
    workflow += node

    # add file that prints information about the search event
    if workflow_options:
        layouts += [mini.make_coinc_info(workflow, single_triggers,
                                         tmpltbank_file, coinc_file,
                                         num_event, opts.output_dir,
                                         tags=opts.tags + [str(num_event)])]

    # add files from posterior summary table for this event
    layouts += [inffu.make_inference_summary_table(
                          workflow, inference_file, opts.output_dir,
                          variable_args=variable_args,
                          analysis_seg=analysis_time,
                          tags=opts.tags + [str(num_event)])]

    # add files from posterior corner plot for this event
    layouts += [inffu.make_inference_posterior_plot(
                          workflow, inference_file, opts.output_dir,
                          variable_args=variable_args,
                          analysis_seg=analysis_time,
                          tags=opts.tags + [str(num_event)])]

    # get files from plotting acceptance rate plot for this event
    rate_files = inffu.make_inference_acceptance_rate_plot(
                          workflow, inference_file,  opts.output_dir,
                          analysis_seg=analysis_time,
                          tags=opts.tags + [str(num_event)])

    # get files from plotting PSD for this event
    psd_file = wf.make_spectrum_plot(workflow, [inference_file],
                                     opts.output_dir,
                                     tags=opts.tags + [str(num_event)])

    # add a row to the HTML page with accpetance rate on the left
    # and the PSD plot on the right
    layouts += list(layout.grouper(rate_files+[psd_file], 2))

    # add files from plotting parameter samples and autocorrelation
    # function for this event
    files = inffu.make_inference_single_parameter_plots(
                          workflow, inference_file, opts.output_dir,
                          variable_args=variable_args,
                          analysis_seg=analysis_time,
                          tags=opts.tags + [str(num_event)]) 
    layouts += list(layout.grouper(files, 2))

# add files from prior plots for this event
# do not add them to layouts so that they appear in the
# additional files section
for subsection in cp.get_subsections(opts.prior_section):
    inffu.make_inference_prior_plot(
                          workflow, config_file, opts.output_dir,
                          analysis_seg=workflow.analysis_time,
                          sections=[opts.prior_section + "-" + subsection],
                          tags=opts.tags + [subsection])

# write dax
workflow.save(filename=opts.output_file, output_map_path=opts.output_map)

# write html well
layout.two_column_layout(opts.output_dir, layouts)
