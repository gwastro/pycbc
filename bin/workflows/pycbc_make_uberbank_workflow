#!/usr/bin/env python

# Copyright (C) 2016 Ian W. Harry
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
Workflow generator for the 'uberbank' template bank construction. The script
as written runs a configurable example of pycbc_geom_aligned_bank and, unless
instructed otherwise, another idependent one of sbank. Typically, a standard
uberbank uses this round of sbank to cover BBH sources, while a PyGRB uberbank
does not require performing this step: this step can be avoided with the
option skip-coarse-bank = in the section [workflow-bank_structure]. The script
then combines these two banks together, if the BBH sbank was indeed generated.
Finally, it runs a second example of sbank (configuration file tag:
[sbank-final]) on the resulting output. It is foreseen that this script would
be altered to generate workflows in slightly different configurations to the
one described, so hopefully the script is clear enough to allow someone to do
that.
"""

#imports
import os
import argparse
import logging
import pycbc
import pycbc.version
import pycbc.workflow as wf
from pycbc.workflow.pegasus_workflow import SubWorkflow

# Boiler-plate stuff
__author__  = "Ian Harry <ian.harry@ligo.org>"
__version__ = pycbc.version.git_verbose_msg
__date__    = pycbc.version.date
__program__ = "pycbc_make_uberbank_workflow"

# Define classes for executables

class GeomBankExecutable(wf.Executable):
    """ Class for running pycbc_geom_aligned_bank.
    """
    # This outputs a dax file.
    current_retention_level = wf.Executable.FINAL_RESULT

    def create_node(self, analysis_time, geom_out_file, config_file,
                    out_storage_path, pegasus_base_dir,
                    workflow_name='geom_bank'):
        node = wf.Executable.create_node(self)

        # Output file: This one is done weirdly because this job doesn't
        # actually produce this file, the resulting dax will do that.
        node.add_opt('--output-file', geom_out_file.storage_path)

        node.add_input_opt('--supplement-config-file', config_file)

        node.add_opt('--workflow-name', workflow_name)
        node.add_opt('--storage-path-base', out_storage_path)

        # Intermediate outputs and dax files
        node.new_output_file_opt(analysis_time, '.hdf',
                                 '--intermediate-data-file',
                                 tags=self.tags + ['INTERMEDIATE'])
        node.new_output_file_opt(analysis_time, '.xml',
                                 '--metadata-file',
                                 tags=self.tags + ['METADATA'])
        node.new_output_file_opt(analysis_time, '.dax',
                                 '--dax-file',
                                 tags=self.tags + ['DAX'])
        node.new_output_file_opt(analysis_time, '.map',
                                 '--output-map',
                                 tags=self.tags + ['MAP'])
        return node

class SbankDaxGenerator(wf.Executable):
    """ Class for running pycbc_make_sbank_workflow.
    """
    # This outputs a dax file.
    current_retention_level = wf.Executable.FINAL_RESULT

    def create_node(self, analysis_time, config_file, out_storage_path,
                    output_file, pegasus_base_dir, seed_files=None,
                    workflow_name='sbank_workflow', sub_wf_tags=None):
        """ Create a sbank dax generator node.
        """
        node = wf.Executable.create_node(self)

        # Output file: This one is done weirdly because this job doesn't
        # actually produce this file, the resulting dax will do that.
        node.add_opt('--output-file', output_file.storage_path)


        node.add_opt('--workflow-name', workflow_name)
        node.add_opt('--output-dir', out_storage_path)
        node.add_opt('--dax-file-directory', '.')

        if sub_wf_tags is not None:
            node.add_opt('--tags', ' '.join(sub_wf_tags))

        node.add_input_opt('--config-files', config_file)

        config_overrides = []

        if seed_files is not None:
            sfs = []
            for seed_file in seed_files:
                sfs.append(seed_file.name)
            config_overrides.append('workflow:seed-bank:"%s"' % ' '.join(sfs))

        if len(config_overrides):
            node.add_opt('--config-overrides', ' '.join(config_overrides))


        node.new_output_file_opt(analysis_time, '.dax',
                                 '--dax-file',
                                 tags=self.tags + sub_wf_tags + ['DAX'])
        node.new_output_file_opt(analysis_time, '.map',
                                 '--output-map',
                                 tags=self.tags + sub_wf_tags + ['MAP'])
        return node

##############################################################################
# Argument parsing and setup of workflow                                     #
##############################################################################


# Use the standard workflow command-line parsing routines. Things like a
# configuration file are specified within the "workflow command line group"
# so run this with --help to see what options are added.
_desc = __doc__[1:]
parser = argparse.ArgumentParser(description=_desc)
parser.add_argument('--version', action='version', version=__version__)
wf.add_workflow_command_line_group(parser)
wf.add_workflow_settings_cli(parser)
args = parser.parse_args()

# Create the workflow object
workflow = wf.Workflow(args)

wf.makedir(args.output_dir)
os.chdir(args.output_dir)
wf.makedir('daxes')

##############################################
# Run the geom_bank workflow the first time  #
##############################################
logging.info("Setting up the round of geometric bank jobs.")

config_path = os.path.abspath('daxes' + '/' + 'geom_workflow.ini')
workflow.cp.write(open(config_path, 'w'))
config_file = wf.resolve_url_to_file(config_path)

geom_exe = GeomBankExecutable(workflow.cp, 'geom_aligned_bank',
                              ifos=workflow.ifos, out_dir='daxes')
# This one's a bit weird as the dag generator names the final output file,
# but that job doesn't *generate* it, so we create this file first.
geom_out_dir = os.path.abspath('initial_geom_bank')
geom_out_file = wf.File(geom_exe.ifo_list, geom_exe.name,
                        workflow.analysis_time, extension='.h5',
                        store_file=True, directory=geom_out_dir,
                        tags=[], use_tmp_subdirs=False)
geom_node = geom_exe.create_node(workflow.analysis_time, geom_out_file,
                                 config_file,
                                 geom_out_dir, workflow.out_dir,
                                 workflow_name='initial_geom_bank')
workflow += geom_node

# Verify files are the correct ones, they have unique extensions
intermediate_file = geom_node.output_files[0]
assert(intermediate_file.name.endswith('hdf'))
metadata_file = geom_node.output_files[1]
assert(metadata_file.name.endswith('xml'))
dax_file = geom_node.output_files[2]
assert(dax_file.name.endswith('.dax'))
map_file = geom_node.output_files[3]
assert(map_file.name.endswith('.map'))

# NOTE: One can use workflow += dax_job, but this seems to miss a bunch of
#       the stuff done below, and does not consider file management
dax_job = SubWorkflow(dax_file, is_planned=False, _id='geom')
dax_job.set_subworkflow_properties(map_file,
                                   staging_site=workflow.staging_site,
                                   cache_file=workflow.cache_file)

# Tell the dax job that it needs some input files
dax_job.add_inputs(metadata_file, intermediate_file)

# And this output is used again so declare it.
dax_job.add_outputs(geom_out_file, stage_out=True)
workflow._outputs.append(geom_out_file)

dax_job.add_into_workflow(workflow)
geom_dax_job = dax_job

###############################################
# Run the sbank workflow for the first time   #
###############################################

sbank_exe = SbankDaxGenerator(workflow.cp, 'sbank_workflow',
                              ifos=workflow.ifos, out_dir='daxes')

config_path = os.path.abspath('daxes' + '/' + 'sbank_workflow.ini')
workflow.cp.write(open(config_path, 'w'))
config_file = wf.resolve_url_to_file(config_path)

# This first round of sbank is used for BBHs for the standard uberbank;
# a PyGRB uberbank covers only potentially EM-bright systems and therfore
# skips this step.
if not workflow.cp.has_option("workflow-bank_structure", "skip-coarse-bank"):
    logging.info("Setting up the (optional) first round of sbank jobs.")
    sbank_out_file_bbh = wf.File(sbank_exe.ifo_list, sbank_exe.name,
                                 workflow.analysis_time, extension='.h5',
                                 store_file=True,
                                 directory=os.path.abspath('sbank_bbh'),
                                 tags=['sbank_bbh'], use_tmp_subdirs=False)
    sbank_node = sbank_exe.create_node(workflow.analysis_time, config_file,
                                       os.path.abspath('sbank_bbh'),
                                       sbank_out_file_bbh,
                                       workflow.out_dir,
                                       workflow_name='sbank_bbh',
                                       sub_wf_tags=['bbh'])

    workflow += sbank_node

    dax_file = sbank_node.output_files[0]
    map_file = sbank_node.output_files[1]
    dax_job = SubWorkflow(dax_file, is_planned=False, _id='sbank_bbh')
    dax_job.set_subworkflow_properties(map_file,
                                       staging_site=workflow.staging_site,
                                       cache_file=workflow.cache_file)

    dax_job.add_outputs(sbank_out_file_bbh, stage_out=True)
    workflow._outputs.append(sbank_out_file_bbh)
    dax_job.add_into_workflow(workflow)
    sbank_bbh_dax_job = dax_job

###############################################
# Run the sbank workflow for the second time  #
###############################################

logging.info("Setting up the final round of sbank jobs.")
sbank_out_file_final = wf.File(sbank_exe.ifo_list, sbank_exe.name,
                         workflow.analysis_time, extension='.h5',
                         store_file=True,
                         directory=os.path.abspath('sbank_final'),
                         tags=['sbank_final'], use_tmp_subdirs=False)

seed_files = [geom_out_file]
if not workflow.cp.has_option("workflow-bank_structure", "skip-coarse-bank"):
    seed_files = [geom_out_file, sbank_out_file_bbh]

# NOTE: The call to generate this workflow actually doesn't depend on the input
#       seed banks being generated. It will only pass the name of these files
#       along, and not do anything with them.
sbank_node = sbank_exe.create_node(workflow.analysis_time, config_file,
                                   os.path.abspath('sbank_final'),
                                   sbank_out_file_final,
                                   workflow.out_dir,
                                   seed_files=seed_files,
                                   workflow_name='sbank_final',
                                   sub_wf_tags=['final'])

workflow += sbank_node

dax_file = sbank_node.output_files[0]
map_file = sbank_node.output_files[1]
dax_job = SubWorkflow(dax_file, is_planned=False, _id='sbank_final')
dax_job.set_subworkflow_properties(map_file,
                                   staging_site=workflow.staging_site,
                                   cache_file=workflow.cache_file)
dax_job.add_inputs(*seed_files)
dax_job.add_into_workflow(workflow)

workflow.save()
