#!/usr/bin/env python

"""Supervise the periodic re-fitting of PyCBC Live single-detector triggers,
and the associated plots.
"""

import re
import logging
import argparse
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import os
import shutil
import subprocess
import numpy as np

from lal import gpstime

import pycbc
from pycbc.io.hdf import HFile
from pycbc.live import supervision as sv
from pycbc.types.config import InterpolatingConfigParser as icp


def read_options(args):
    """
    read the options into a dictionary
    """

    cp = icp(configFiles=[args.config_file])
    option_values = {
        section: {k: v for k, v in cp[section].items()}
        for section in cp.sections()
    }
    del option_values['environment']
    return option_values

def trigger_collation(day_dt, day_str, collation_control_options, collation_options, output_dir, ifos):
    """
    Perform the trigger collation as specified
    """
    collate_args = [
        'pycbc_live_collate_triggers',
    ]
    collate_args += sv.dict_to_args(collation_options)
    gps_start = gpstime.utc_to_gps(day_dt).gpsSeconds
    gps_end = gpstime.utc_to_gps(day_dt + timedelta(days=1)).gpsSeconds
    ifos_str = ''.join(ifos)

    trig_merge_file = os.path.join(
        output_dir,
        collation_control_options['collated-triggers-format'].format(
            ifos=ifos_str,
            start=gps_start,
            duration=(gps_end - gps_start)
        )
    )
    collate_args += [
        '--gps-start-time', f'{gps_start:d}',
        '--gps-end-time', f'{gps_end:d}',
        '--output-file', trig_merge_file,
    ]

    # FIXME remove this logging and comment
    logging.info(collate_args)
    # Run once then comment out for testing, as this can be expensive
    #sv.run_and_error(
    #    collate_args,
    #    option_values['control']
    #)

    return trig_merge_file


def fit_by_template(
        trigger_merge_file,
        day_str,
        fbt_control_options,
        fbt_options,
        output_dir,
        ifo,
        controls
    ):
    fbt_out_fname = fbt_control_options['fit-by-template-format'].format(
        date=day_str,
        ifo=ifo,
    )
    fbt_out_full = os.path.join(output_dir, fbt_out_fname)
    fit_by_args = ['pycbc_fit_sngls_by_template']
    fit_by_args += ['--trigger-file', trigger_merge_file]
    fit_by_args += sv.dict_to_args(fbt_options)
    fit_by_args += ['--output', fbt_out_full, '--ifo', ifo]
    # FIXME remove this logging and comment
    logging.info(fit_by_args)
    #sv.run_and_error(fit_by_args, controls)

    return fbt_out_full, day_str


def find_daily_fit_files(combined_control_options, daily_fname_format, outputs_dir, ifo):
    combined_days = int(combined_control_options['combined-days'])
    if 'replay-start-time' in combined_control_options:
        replay_start_time = int(combined_control_options['replay-start-time'])
        true_start_time = int(combined_control_options['true-start-time'])
        replay_duration = int(combined_control_options['replay-duration'])
        rep_start_utc = lal.GPSToUTC(replay_start_time)[0:6]

        dt_replay_start = datetime(
            year=rep_start_utc[0],
            month=rep_start_utc[1],
            day=rep_start_utc[2],
            hour=rep_start_utc[3],
            minute=rep_start_utc[4],
            second=rep_start_utc[5]
        )

        td = (day_dt - dt_replay_start).total_seconds()

        # Time since the start of this replay
        time_since_replay = np.remainder(td, replay_duration)

        # Add this on to the original start time to get the current time of
        # the replay data
        true_utc = lal.GPSToUTC(true_start_time)[0:6]
        dt_true_start = datetime(
            year=true_utc[0],
            month=true_utc[1],
            day=true_utc[2],
            hour=true_utc[3],
            minute=true_utc[4],
            second=true_utc[5]
        )
        # Original time of the data being replayed right now
        current_date = dt_true_start + timedelta(seconds=time_since_replay)
    else:
        current_date = day_dt

    date_test = current_date + timedelta(days=1)

    logging.info("Finding daily files for combination")

    daily_files = []
    missed_files = 0
    found_files = 0
    while found_files < combined_days and missed_files < 10:
        # Loop through the possible file locations and see if the file exists
        date_test -= timedelta(days=1)
        date_out = date_test.strftime("%Y_%m_%d")
        daily_fname = daily_fname_format.format(
            date=date_out,
            ifo=ifo,
        )

        output_dir = os.path.join(
            outputs_dir,
            date_out
        )
        daily_full = os.path.join(
            output_dir,
            daily_fname
        )
        # Check that the file exists:
        if not os.path.exists(daily_full):
            missed_files += 1
            logging.info("File %s does not exist - skipping", daily_full)
            continue
        if not len(daily_files):
            end_date = date_out
        # This is now the oldest file
        first_date = date_out
        # reset the "missed files" counter, and add to the "found files"
        missed_files = 0
        found_files += 1
        daily_files.append(daily_full)

    if missed_files == 10:
        # If more than 10 days between files, something wrong with analysis.
        # warn and use fewer files - 10 here is chosen to be an unusual amount
        # of time for the analysis to be down in standard operation
        logging.warning('More than 10 days between files, only using '
                        f'{found_files} files for combination!')

    return daily_files, first_date, end_date


def fit_over_multiparam(fit_over_control_options, fit_over_options, ifo, day_dt, day_str, controls):
    daily_files, first_date, end_date = find_daily_fit_files(
        fit_over_control_options,
        fit_over_control_options['fit-by-format'],
        controls['output-directory'],
        ifo
    )
    file_id_str = f'{first_date}-{end_date}'
    out_fname = fit_over_control_options['fit-over-format'].format(
        dates=file_id_str,
        ifo=ifo,
    )

    output_dir = os.path.join(
        controls['output-directory'],
        day_str
    )
    fit_over_args = ['pycbc_fit_sngls_over_multiparam', '--template-fit-file']
    fit_over_args += daily_files
    fit_over_args += sv.dict_to_args(fit_over_options)
    fit_over_full = os.path.join(output_dir, out_fname)
    fit_over_args += ['--output', fit_over_full]
    logging.info(fit_over_args)
    sv.run_and_error(fit_over_args, controls)
    if 'variable-fit-over-param' in fit_over_control_options:
        variable_fits = fit_over_control_options['variable-fit-over-param'].format(ifo=ifo)
        sv.symlink(fit_over_full, variable_fits)

    return fit_over_full, file_id_str

def plot_fits(fits_file, ifo, day_title_str, plot_fit_options, controls, smoothed=False):
    """Plotting for fit_by files, and linking to the public directory"""
    fits_plot_output = fits_file[:-3] + 'png'
    logging.info("Plotting daily fits %s to %s", fits_file, fits_plot_output)
    fits_plot_arguments = [
        'pycbc_plot_bank_corner',
        '--fits-file',
        fits_file,
        '--output-plot-file',
        fits_plot_output,
    ]
    fits_plot_arguments += sv.dict_to_args(plot_fit_options)
    title = "Fit parameters for pycbc-live, triggers from " + day_title_str
    if smoothed == True:
        title += ', smoothed'
    fits_plot_arguments += ['--title', title]
    # FIXME remove this logging and comment
    logging.info(fits_plot_arguments)
    sv.run_and_error(fits_plot_arguments, controls)
    if 'public-dir' in controls:
        public_dir = os.path.abspath(os.path.join(
            controls['public-dir'],
            *day_str.split('_')
        ))
        sv.symlink(fits_plot_output, public_dir)


def single_significance_fits(
        daily_controls,
        daily_options,
        output_dir,
        day_str,
        controls,
        test_options
    ):
    file_id_str = f'{day_str}'
    if 'output-id-str' in daily_controls:
        file_id_str += f"-{daily_controls['output-id-str']}"
    out_fname = f'{file_id_str}-TRIGGER-FITS.hdf'
    daily_options['output'] = os.path.join(output_dir, out_fname)
    daily_args = ['pycbc_live_single_significance_fits']

    gps_start_time = gpstime.utc_to_gps(day_dt).gpsSeconds
    gps_end_time = gpstime.utc_to_gps(day_dt + timedelta(days=1)).gpsSeconds
    daily_options['gps-start-time'] = f'{gps_start_time:d}'
    daily_options['gps-end-time'] = f'{gps_end_time:d}'
    daily_args += sv.dict_to_args(daily_options)

    sv.run_and_error(daily_args, controls)

    if 'check-output' in daily_controls:
        logging.info(
            "Checking that fit coefficients above %s for bins above %ss",
            test_options['lower-limit-coefficient'],
            test_options['duration-bin-lower-limit']
        )
        sv.check_trigger_files(
            [daily_options['output']],
            test_options,
            controls
        )
    return daily_options['output']

def plot_single_significance_fits(output_dir, daily_output, daily_plot_options, controls):
    # Add plotting for daily fits, and linking to the public directory
    logging.info("Plotting daily fits")
    daily_plot_output = os.path.join(output_dir,
                                     '{ifo}-' + f'{out_fname[:-3]}png')
    daily_plot_arguments = [
        'pycbc_live_plot_single_significance_fits',
        '--trigger-fits-file',
        daily_output,
        '--output-plot-name-format',
        daily_plot_output,
        '--log-colormap'
    ]
    daily_plot_arguments += sv.dict_to_args(daily_plot_options)
    sv.run_and_error(daily_plot_arguments)

    # Link the plots to the public-dir if wanted
    if 'public-dir' in controls:
        daily_plot_outputs = [daily_plot_output.format(ifo=ifo) for ifo in
                              daily_options['ifos'].split()]
        logging.info("Linking daily fits plots")
        for dpo in daily_plot_outputs:
            sv.symlink(dpo, public_dir)


#def combine_significance_fits():

#def plot_combined_significance_fits():

def supervise_collation_fits_dq(args, day_dt, day_str):
    """
    Perform the fits as specified
    """
    # Read in the config file and pack into appropriate dictionaries
    option_values = read_options(args)
    controls = option_values['control']
    collation_options = option_values['collation']
    collation_control_options = option_values['collation_control']
    fit_by_template_options = option_values['fit_by_template']
    fit_by_template_control_options = option_values['fit_by_template_control']
    fit_over_options = option_values['fit_over_multiparam']
    fit_over_control_options = option_values['fit_over_multiparam_control']
    plot_fit_options = option_values['plot_fit']
    daily_options = option_values['significance_daily_fits']
    daily_control_options = option_values['significance_daily_fits_control']
    combined_options = option_values['significance_combined_fits']
    combined_control_options = option_values['significance_combined_fits_control']
    test_options = option_values['test']

    # The main output directory will have a date subdirectory which we
    # put the output into
    sv.ensure_directories(controls, day_str)

    ifos = option_values['control']['ifos'].split()
    output_dir = os.path.join(
        option_values['control']['output-directory'],
        day_str
    )
    merged_triggers = trigger_collation(
        day_dt,
        day_str,
        collation_control_options,
        collation_options,
        output_dir,
        ifos,
    )
    for ifo in option_values['control']['ifos'].split():
        if args.fit_by_template:
            fbt_file, date_str = fit_by_template(
                merged_triggers,
                day_str,
                fit_by_template_control_options,
                fit_by_template_options,
                output_dir,
                ifo,
                controls,
            )
            plot_fits(
                fbt_file,
                ifo,
                date_str,
                plot_fit_options,
                controls
            )
        if args.fit_over_multiparam:
            fom_file, date_str = fit_over_multiparam(
                fit_over_control_options,
                fit_over_options,
                ifo,
                day_dt,
                day_str,
                controls
            )
            plot_fits(
                fom_file,
                ifo,
                date_str,
                plot_fit_options,
                controls,
                smoothed=True,
            )
#
#    if args.single_significance_fits:
#        file_id_str = f'{day_str}'
#        if 'output-id-str' in controls:
#            file_id_str += f"-{controls['output-id-str']}"
#        out_fname = f'{file_id_str}-TRIGGER-FITS.hdf'
#        daily_options['output'] = os.path.join(output_dir, out_fname)
#        daily_args = ['pycbc_live_single_significance_fits']
#
#        gps_start_time = gpstime.utc_to_gps(day_dt).gpsSeconds
#        gps_end_time = gpstime.utc_to_gps(day_dt + timedelta(days=1)).gpsSeconds
#        daily_options['gps-start-time'] = f'{gps_start_time:d}'
#        daily_options['gps-end-time'] = f'{gps_end_time:d}'
#        daily_args += sv.dict_to_args(daily_options)
#
#        sv.run_and_error(daily_args, controls)
#
#        # Add plotting for daily fits, and linking to the public directory
#        logging.info("Plotting daily fits")
#        daily_plot_output = os.path.join(output_dir,
#                                         '{ifo}-' + f'{out_fname[:-3]}png')
#        daily_plot_arguments = [
#            'pycbc_live_plot_single_significance_fits',
#            '--trigger-fits-file',
#            daily_options['output'],
#            '--output-plot-name-format',
#            daily_plot_output,
#            '--log-colormap'
#        ]
#        sv.run_and_error(daily_plot_arguments)
#
#        # Link the plots to the public-dir if wanted
#        if 'public-dir' in controls:
#            daily_plot_outputs = [daily_plot_output.format(ifo=ifo) for ifo in
#                                  daily_options['ifos'].split()]
#            logging.info("Linking daily fits plots")
#            for dpo in daily_plot_outputs:
#                sv.symlink(dpo, public_dir)
#
#        if 'check-daily-output' in controls:
#            logging.info(
#                "Checking that fit coefficients above %s for bins above %ss",
#                test_options['lower-limit-coefficient'],
#                test_options['duration-bin-lower-limit']
#            )
#            sv.check_trigger_files(
#                [daily_options['output']],
#                test_options,
#                controls
#            )
#
#    if args.combine_significance_fits:
#        combined_days = int(controls['combined-days'])
#        if 'replay-start-time' in controls:
#            replay_start_time = int(controls['replay-start-time'])
#            true_start_time = int(controls['true-start-time'])
#            replay_duration = int(controls['replay-duration'])
#            dt_replay_start = gpstime.gps_to_utc(replay_start_time)
#
#            td = (day_dt - dt_replay_start).total_seconds()
#
#            # Time since the start of this replay
#            time_since_replay = np.remainder(td, replay_duration)
#
#            # Add this on to the original start time to get the current time of
#            # the replay data
#            dt_true_start = gpstime.gps_to_utc(true_start_time)
#
#            # Original time of the data being replayed right now
#            current_date = dt_true_start + timedelta(seconds=time_since_replay)
#        else:
#            current_date = day_dt
#
#        date_test = current_date + timedelta(days=1)
#
#        logging.info("Finding trigger fit files for combination")
#        if 'check-daily-output' in controls:
#            logging.info(
#                "Checking all files that fit coefficients above %s for bins "
#                "above %ss",
#                test_options['lower-limit-coefficient'],
#                test_options['duration-bin-lower-limit']
#            )
#
#        daily_files = []
#        missed_files = 0
#        found_files = 0
#        while found_files < combined_days and missed_files < 10:
#            # Loop through the possible file locations and see if the file exists
#            date_test -= timedelta(days=1)
#            date_out = date_test.strftime("%Y_%m_%d")
#            trfits_filename = controls['trfits-format'].format(date=date_out)
#            # Check that the file exists:
#            if not os.path.exists(trfits_filename):
#                missed_files += 1
#                logging.info(f"File {trfits_filename} does not exist - skipping")
#                continue
#            if not len(daily_files):
#                end_date = date_out
#            # This is now the oldest file
#            first_date = date_out
#            # reset the "missed files" counter, and add to the "found files"
#            missed_files = 0
#            found_files += 1
#            daily_files.append(trfits_filename)
#
#        if 'check-daily-output' in controls:
#            sv.check_trigger_files(daily_files, test_options, controls)
#
#        if missed_files == 10:
#            # If more than 10 days between files, something wrong with analysis.
#            # warn and use fewer files - 10 here is chosen to be an unusual amount
#            # of time for the analysis to be down in standard operation
#            logging.warning('More than 10 days between files, only using '
#                            f'{found_files} files for combination!')
#
#        file_id_str = f'{first_date}-{end_date}'
#        if 'output-id-str' in controls:
#            file_id_str += f"-{controls['output-id-str']}"
#        out_fname = f'{file_id_str}-TRIGGER_FITS_COMBINED'
#        combined_options['output'] = os.path.join(output_dir, out_fname + '.hdf')
#
#        if not daily_files:
#            raise ValueError("No files meet the criteria")
#
#        combined_options['trfits-files'] = ' '.join(daily_files)
#
#        combined_args = ['pycbc_live_combine_single_significance_fits']
#        combined_args += sv.dict_to_args(combined_options)
#
#        sv.run_and_error(combined_args)
#
#        if 'variable-trigger-fits' in controls:
#            logging.info('Copying combined fits file to local filesystem')
#            try:
#                shutil.copyfile(
#                    combined_options['output'],
#                    controls['variable-trigger-fits']
#                )
#            except Exception as e:
#                sv.mail_volunteers_error(
#                    controls,
#                    [str(e)],
#                    "PyCBC live could not copy to variable trigger fits file"
#                )
#                raise e
#            logging.info(
#                "%s updated to link to %s",
#                controls['variable-trigger-fits'],
#                combined_options['output']
#            )
#
#        logging.info("Plotting combined fits")
#        # Add plotting for combined fits, and linking to the public directory
#        combined_plot_output = os.path.join(output_dir,
#                                            f"{{ifo}}-{out_fname}-{{type}}.png")
#        combined_plot_arguments = [
#            'pycbc_live_plot_combined_single_significance_fits',
#            '--combined-fits-file',
#            combined_options['output'],
#            '--output-plot-name-format',
#            combined_plot_output,
#            '--log-colormap'
#        ]
#
#        sv.run_and_error(combined_plot_arguments)
#
#        combined_plot_outputs = [
#            combined_plot_output.format(ifo=ifo, type='fit_coeffs') for ifo in
#            combined_options['ifos'].split()
#        ]
#        combined_plot_outputs += [
#            combined_plot_output.format(ifo=ifo, type='counts') for ifo in
#            combined_options['ifos'].split()
#        ]
#
#        # Link the plots to the public-dir if wanted
#        if 'public-dir' in controls:
#            logging.info("Linking combined fits")
#            for cpo in combined_plot_outputs:
#                sv.symlink(cpo, public_dir)
#
#        logging.info('Done')
#

parser = argparse.ArgumentParser(description=__doc__)
pycbc.add_common_pycbc_options(parser)
parser.add_argument(
    '--config-file',
    required=True
)
parser.add_argument(
    '--date',
    help='Date to analyse, if not given, will analyse yesterday (UTC). '
         'Format YYYY_MM_DD. Do not use if using --run-daily-at.'
)
parser.add_argument(
    '--fit-by-template',
    action='store_true',
    help="Perform template fits calculation."
)
parser.add_argument(
    '--fit-over-multiparam',
    action='store_true',
    help="Perform template fits smoothing."
)
parser.add_argument(
    '--single-significance-fits',
    action='store_true',
    help="Perform daily singles significance fits."
)
parser.add_argument(
    '--combine-significance-fits',
    action='store_true',
    help="Do combination of singles significance fits."
)
parser.add_argument(
    '--run-daily-at',
    metavar='HH:MM:SS',
    help='Stay running and repeat the fitting daily at the given UTC hour.'
)
args = parser.parse_args()

pycbc.init_logging(args.verbose, default_level=1)

if args.run_daily_at is not None and args.date is not None:
    parser.error('Cannot take --run-daily-at and --date at the same time')

if args.run_daily_at is not None:
    # keep running and repeat the fitting every day at the given hour
    if not re.match('[0-9][0-9]:[0-9][0-9]:[0-9][0-9]', args.run_daily_at):
        parser.error('--run-daily-at takes a UTC time in the format HH:MM:SS')
    logging.info('Starting in daily run mode')
    while True:
        sv.wait_for_utc_time(args.run_daily_at)
        logging.info('==== Time to update the single fits, waking up ====')
        # Get the date string for yesterday's triggers
        day_dt = datetime.utcnow() - timedelta(days=1)
        day_dt = datetime.combine(day_dt, datetime.min.time())
        day_str = day_dt.strftime('%Y_%m_%d')
        supervise_collation_fits_dq(args, day_dt, day_str)
else:
    # run just once
    if args.date:
        day_str = args.date
        day_dt = datetime.strptime(args.date, '%Y_%m_%d')
    else:
        # Get the date string for yesterday's triggers
        day_dt = datetime.utcnow() - timedelta(days=1)
        day_dt = datetime.combine(day_dt, datetime.min.time())
        day_str = day_dt.strftime('%Y_%m_%d')
    supervise_collation_fits_dq(args, day_dt, day_str)
