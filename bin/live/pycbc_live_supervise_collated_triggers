#!/usr/bin/env python
"""
Supervise the periodic collection of triggers from pycbc_live,
using them to get template fits, smoothing those fits,
generating data quality files and the associated plots.
"""

import re
import logging
import argparse
from datetime import datetime, timedelta
import os
import numpy as np

import lal
from lal import gpstime

import pycbc
from pycbc.live import supervision as sv
from pycbc.types.config import InterpolatingConfigParser as icp


def read_options(args):
    """
    read the options into a dictionary
    """

    cp = icp(configFiles=[args.config_file])
    option_values = {
        section: {k: v for k, v in cp[section].items()}
        for section in cp.sections()
    }
    del option_values['environment']
    return option_values


def ensure_directories(control_values, day_str):
    output_dir = os.path.join(
        control_values['output-directory'],
        day_str
    )
    sv.run_and_error(['mkdir', '-p', output_dir], option_values)
    if 'public-dir' in control_values:
        # The public directory wil be in subdirectories for the year, month,
        # day, e.g. 2024_04_12 will be in 2024/04/12.
        public_dir = os.path.join(
            control_values['public-dir'],
            *day_str.split('_')
        )
        sv.run_and_error(
            ['mkdir', '-p', public_dir],
            control_values
        )


def collate_triggers(args, day_dt, day_str, option_values):
    """
    Perform the fits as specified
    """

    # The main output directory will have a date subdirectory which we
    # put the output into
    output_dir = os.path.join(
        option_values['control']['output-directory'],
        day_str
    )
    # Collate the triggers into a single hdf file for input into
    # fit_by_template and bin_dq
    collate_args = [
        'pycbc_live_collate_triggers',
    ]
    collate_args += sv.dict_to_args(option_values['collation'])
    gps_start = gpstime.utc_to_gps(day_dt).gpsSeconds
    gps_end = gpstime.utc_to_gps(day_dt + timedelta(days=1)).gpsSeconds
    ifos_str = ''.join(option_values['control']['ifos'].split())

    trig_merge_file = os.path.join(
        output_dir,
        option_values['collation_control']['collated-triggers-format'].format(
            ifos=ifos_str,
            start=gps_start,
            duration=(gps_end - gps_start)
        )
    )
    collate_args += [
        '--gps-start-time', f'{gps_start:d}',
        '--gps-end-time', f'{gps_end:d}',
        '--output-file', trig_merge_file,
    ]

    logging.info(collate_args)
    # Run once then comment out for testing, as this can be expensive
    sv.run_and_error(
        collate_args,
        option_values['control']
    )

    return trig_merge_file


def fits_plot(fits_file, option_values, ifo, day_title_str):
    # Add plotting for daily fits, and linking to the public directory
    fits_plot_output = fits_file[:-3] + 'png'
    logging.info("Plotting daily fits %s to %s", fits_file, fits_plot_output)
    fits_plot_arguments = [
        'pycbc_plot_bank_corner',
        '--fits-file',
        fits_file,
        '--output-plot-file',
        fits_plot_output,
    ]
    fits_plot_arguments += sv.dict_to_args(option_values['plot_fit'])
    fits_plot_arguments += [
        '--title',
        "Fit parameters for pycbc-live, triggers from " + day_title_str
    ]
    logging.info(fits_plot_arguments)
    sv.run_and_error(fits_plot_arguments, option_values)
    if 'public-dir' in option_values['control']:
        public_dir = os.path.abspath(os.path.join(
            option_values['control']['public-dir'],
            *day_str.split('_')
        ))
        sv.symlink(fits_plot_output, public_dir)


def fit_by_template(trigger_merge_file, day_str, option_values, ifo):
    fbt_out_fname = option_values['fit_by_template_control']['fit-by-template-format'].format(
        date=day_str,
        ifo=ifo,
    )
    fbt_out_fname = os.path.join(
        day_str, fbt_out_fname
    )
    output_dir = os.path.abspath(
        option_values['control']['output-directory']
    )
    fbt_out_full = os.path.join(output_dir, fbt_out_fname)
    option_values['output'] = fbt_out_full
    fit_by_args = ['pycbc_fit_sngls_by_template']
    fit_by_args += ['--trigger-file', trigger_merge_file]
    fit_by_args += sv.dict_to_args(option_values['fit_by_template'])
    fit_by_args += ['--output', fbt_out_full, '--ifo', ifo]
    logging.info(fit_by_args)
    sv.run_and_error(fit_by_args, option_values)

    return fbt_out_full, day_str


def fit_over_multiparam(option_values, ifo, day_dt, day_str):
    combined_days = int(option_values['fit_over_control']['fit-over-combined-days'])
    if 'replay-start-time' in option_values['fit_over_control']:
        replay_start_time = int(option_values['fit_over_control']['replay-start-time'])
        true_start_time = int(option_values['fit_over_control']['true-start-time'])
        replay_duration = int(option_values['fit_over_control']['replay-duration'])
        rep_start_utc = lal.GPSToUTC(replay_start_time)[0:6]

        dt_replay_start = datetime(
            year=rep_start_utc[0],
            month=rep_start_utc[1],
            day=rep_start_utc[2],
            hour=rep_start_utc[3],
            minute=rep_start_utc[4],
            second=rep_start_utc[5]
        )

        td = (day_dt - dt_replay_start).total_seconds()

        # Time since the start of this replay
        time_since_replay = np.remainder(td, replay_duration)

        # Add this on to the original start time to get the current time of
        # the replay data
        true_utc = lal.GPSToUTC(true_start_time)[0:6]
        dt_true_start = datetime(
            year=true_utc[0],
            month=true_utc[1],
            day=true_utc[2],
            hour=true_utc[3],
            minute=true_utc[4],
            second=true_utc[5]
        )
        # Original time of the data being replayed right now
        current_date = dt_true_start + timedelta(seconds=time_since_replay)
    else:
        current_date = day_dt

    date_test = current_date + timedelta(days=1)

    logging.info("Finding fit_by_template files for combination")

    trfits_files = []
    missed_files = 0
    found_files = 0
    while found_files < combined_days and missed_files < 10:
        # Loop through the possible file locations and see if the file exists
        date_test -= timedelta(days=1)
        date_out = date_test.strftime("%Y_%m_%d")
        fbt_out_fname = option_values['fit_by_template_control']['fit-by-template-format'].format(
            date=date_out,
            ifo=ifo,
        )
        output_dir = os.path.abspath(option_values['control']['output-directory'])
        fbt_out_full = os.path.join(
            os.path.join(
                output_dir,
                date_out
            ),
            fbt_out_fname
        )
        # Check that the file exists:
        if not os.path.exists(fbt_out_full):
            missed_files += 1
            logging.info("File %s does not exist - skipping", fbt_out_full)
            continue
        if not len(trfits_files):
            end_date = date_out
        # This is now the oldest file
        first_date = date_out
        # reset the "missed files" counter, and add to the "found files"
        missed_files = 0
        found_files += 1
        trfits_files.append(fbt_out_full)

    if missed_files == 10:
        # If more than 10 days between files, something wrong with analysis.
        # warn and use fewer files - 10 here is chosen to be an unusual amount
        # of time for the analysis to be down in standard operation
        logging.warning('More than 10 days between files, only using '
                        f'{found_files} files for combination!')

    file_id_str = f'{first_date}-{end_date}'
    out_fname = option_values['fit_over_control']['fit-over-format'].format(
        dates=file_id_str,
        ifo=ifo,
    )

    fit_over_args = ['pycbc_fit_sngls_over_multiparam', '--template-fit-file']
    fit_over_args += trfits_files
    fit_over_args += sv.dict_to_args(option_values['fit_over_multiparam'])
    output_dir = os.path.abspath(os.path.join(
            option_values['control']['output-directory'],
            day_str,
        )
    )
    fit_over_full = os.path.join(output_dir, out_fname)
    fit_over_args += ['--output', fit_over_full]
    logging.info(fit_over_args)
    sv.run_and_error(fit_over_args, option_values)
    opt_foc = option_values['fit_over_control']
    if 'variable-fit-over-param' in opt_foc:
        variable_fits = opt_foc['variable-fit-over-param'].format(ifo=ifo)
        sv.symlink(fit_over_full, variable_fits)

    return fit_over_full, file_id_str


def do_collation_fits_dq(args, option_values, day_dt, day_str):
    ensure_directories(option_values['control'], day_str)
    merged_triggers = collate_triggers(
        args,
        day_dt,
        day_str,
        option_values
    )
    for ifo in option_values['control']['ifos'].split():
        if args.fit_by_template:
            fbt_file, date_str = fit_by_template(
                merged_triggers,
                day_str,
                option_values,
                ifo
            )
            fits_plot(fbt_file, option_values, ifo, date_str)
        if args.fit_over_multiparam:
            fom_file, date_str = fit_over_multiparam(
                option_values,
                ifo,
                day_dt,
                day_str
            )
            fits_plot(fom_file, option_values, ifo, date_str)
    logging.info('Done')


parser = argparse.ArgumentParser(description=__doc__)
parser.add_argument('--config-file', required=True)
parser.add_argument(
    '--date',
    help='Date to analyse, if not given, will analyse yesterday (UTC). '
         'Format YYYY_MM_DD. Do not use if using --run-daily-at.'
)
parser.add_argument(
    '--fit-by-template',
    action='store_true',
    help="Perform template fits calculation."
)
parser.add_argument(
    '--fit-over-multiparam',
    action='store_true',
    help="Perform template fits smoothing."
)
parser.add_argument(
    '--run-daily-at',
    metavar='HH:MM:SS',
    help='Stay running and repeat the fitting daily at the given UTC hour.'
)
args = parser.parse_args()

pycbc.init_logging(True)

if args.run_daily_at is not None and args.date is not None:
    parser.error('Cannot take --run-daily-at and --date at the same time')

option_values = read_options(args)

if args.run_daily_at is not None:
    # keep running and repeat the fitting every day at the given hour
    if not re.match('[0-9][0-9]:[0-9][0-9]:[0-9][0-9]', args.run_daily_at):
        parser.error('--run-daily-at takes a UTC time in the format HH:MM:SS')
    logging.info('Starting in daily run mode')
    while True:
        sv.wait_for_utc_time(args.run_daily_at)
        logging.info('==== Time to update the single fits, waking up ====')
        # Get the date string for yesterday's triggers
        day_dt = datetime.utcnow() - timedelta(days=1)
        day_dt = datetime.combine(day_dt, datetime.min.time())
        day_str = day_dt.strftime('%Y_%m_%d')
        do_collation_fits_dq(args, option_values, day_dt, day_str)
else:
    # run just once
    if args.date:
        day_str = args.date
        day_dt = datetime.strptime(args.date, '%Y_%m_%d')
    else:
        # Get the date string for yesterday's triggers
        day_dt = datetime.utcnow() - timedelta(days=1)
        day_dt = datetime.combine(day_dt, datetime.min.time())
        day_str = day_dt.strftime('%Y_%m_%d')
    do_collation_fits_dq(args, option_values, day_dt, day_str)
