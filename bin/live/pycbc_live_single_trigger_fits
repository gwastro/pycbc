#!/usr/bin/python

# Copyright 2020 Gareth S. Davies
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.

import numpy as np
import pycbc
from pycbc import bin_utils
from pycbc.events import cuts, stat, triggers, trigger_fits as trstats
from pycbc.io import DictArray
from pycbc.events import ranking
import argparse, logging, os, sys, h5py

parser = argparse.ArgumentParser(usage="",
    description="Plot histograms of triggers split over various parameters")
parser.add_argument("--verbose", action="store_true",
                    help="Print extra debugging information", default=False)
parser.add_argument("--ifos", nargs="+", required=True,
                    help="Which ifo are we fitting the triggers for? "
                         "Required")
parser.add_argument("--top-directory", metavar='PATH', required=True,
                    help="Directory containing trigger files, top directory, "
                         "will contain subdirectories for each day of data. "
                         "Required.")
parser.add_argument("--analysis-date", required=True,
                    help="Date of the analysis, format YYYY_MM_DD. Required")
parser.add_argument("--file-identifier", default="H1L1V1-Live",
                    help="String required in filename to be considered for "
                         "analysis. Default: 'H1L1V1-Live'.")
parser.add_argument("--fit-function", default="exponential",
                    choices=["exponential", "rayleigh", "power"],
                    help="Functional form for the maximum likelihood fit. "
                         "Choose from exponential, rayleigh or power. "
                         "Default: exponential")
parser.add_argument("--duration-bin-edges", nargs='+', type=float,
                    help="Durations to use for bin edges. "
                         "Use if specifying exact bin edges, "
                         "Not compatible with --duration-bin-start, "
                         "--duration-bin-end and --num-duration-bins")
parser.add_argument("--duration-bin-start", type=float,
                    help="Shortest duration to use for duration bins."
                         "Not compatible with --duration-bins, requires "
                         "--duration-bin-end and --num-duration-bins.")
parser.add_argument("--duration-bin-end", type=float,
                    help="Longest duration to use for duration bins.")
parser.add_argument("--num-duration-bins", type=int,
                    help="How many template duration bins to split the bank "
                         "into before fitting.")
parser.add_argument("--duration-bin-spacing",
                    choices=['linear','log'], default='log',
                    help="How to set spacing for bank split "
                         "if using --duration-bin-start, --duration-bin-end "
                         "and --num-duration-bins.")
parser.add_argument("--fit-threshold", type=float, default=5,
                    help="Lower threshold used in fitting the triggers."
                         "Default 5.")
parser.add_argument("--cluster", action='store_true',
                    help="Only use maximum of the --sngl-ranking value "
                         "from each file.")
parser.add_argument("--output-directory", required=True,
                    help="Directory in which to save the output file.")
parser.add_argument("--output-fit-plot", action='store_true',
                    help="Flag to save a plot of the fits.")
#parser.add_argument("--", default="", help="")

stat.insert_statistic_option_group(parser,
     default_ranking_statistic='single_ranking_only')
cuts.insert_cuts_option_group(parser)

#Add some input sanitisation
args = parser.parse_args()

pycbc.init_logging(args.verbose)

# Check the bin inputs to see if they are sensible
if args.duration_bin_edges and (args.duration_bin_start or
                                args.duration_bin_end or
                                args.num_duration_bins):
    # duration bin edges specified as well as the linear/logarithmi
    raise parser.error("Cannot use --duration-bin-edges with "
                       "--duration-bin-start, --duration-bin-end or "
                       "--num-duration-bins.")

if not args.duration_bin_edges and not (args.duration_bin_start and
                                        args.duration_bin_end and
                                        args.num_duration_bins):
    raise parser.error("--duration-bin-start, --duration-bin-end and "
                       "--num-duration-bins must be set if not using "
                       "--duration-bin-edges.")

if args.duration_bin_end and \
        args.duration_bin_end <= args.duration_bin_start:
    raise parser.error("--duration-bin-end must be greater than "
                       "--duration-bin-start, got %.2f and %.2f",
                       args.duration_bin_end, args.duration_bin_start)

# Create the duration bins:
if args.duration_bin_edges:
    duration_bin_edges = np.array(args.duration_bin_edges)
elif args.duration_bin_spacing == 'log':
    duration_bin_edges = np.logspace(np.log10(args.duration_bin_start),
                                     np.log10(args.duration_bin_end),
                                     args.num_duration_bins + 1)
elif args.duration_bin_spacing == 'linear':
    duration_bin_edges = np.linspace(args.duration_bin_start,
                                     args.duration_bin_end,
                                     args.num_duration_bins + 1)

logging.info("Finding files")

files = [f for f in os.listdir(os.path.join(args.top_directory, args.analysis_date))
         if args.file_identifier in f]

logging.info("{} files found".format(len(files)))

# Add template duration cuts according to the bin inputs
args.template_cuts = args.template_cuts or []
args.template_cuts.append(f"template_duration:{min(duration_bin_edges)}:lower")
args.template_cuts.append(f"template_duration:{max(duration_bin_edges)}:upper_inc")

# Efficiency saving: add SNR cut before any others as sngl_ranking can
# only be less than SNR.
args.trigger_cuts = args.trigger_cuts or []
args.trigger_cuts.insert(0, f"snr:{args.fit_threshold}:lower_inc")

# We can cut out all of the stuff with sngl-ranking
# below threshold now as well
args.trigger_cuts.append(f"{args.sngl_ranking}:{args.fit_threshold}:lower_inc")

logging.info("Setting up the cut dictionaries")
trigger_cut_dict, template_cut_dict = cuts.ingest_cuts_option_group(args)

logging.info("Setting up duration bins")
tbins = bin_utils.IrregularBins(duration_bin_edges)

# Also calculate live time so that this fitting can be used in rate estimation
# Live time is not immediately obvious - get an approximation with 8 second
# granularity by adding 8 seconds per 'valid' file

live_time = {ifo: 0 for ifo in args.ifos}

logging.info("Getting events which meet criteria")

# Loop through files - add events which meet the immediately gettable
# criteria
date_directory = os.path.join(args.top_directory, args.analysis_date)

files = [f for f in os.listdir(date_directory)
         if args.file_identifier in f]

events = {}
counter = 0
no_triggers_yet = {ifo: True for ifo in args.ifos}

for filename in files:
    counter += 1
    if counter % 1000 == 0:
        logging.info("Processed %d files" % counter)
        for ifo in args.ifos:
            logging.info("{}: {} triggers in {:.0f}s".format(ifo,
                events[ifo].data['snr'].size, live_time[ifo]))

    f = os.path.join(date_directory, filename)
    skipping_file = False
    #If there is an IOerror with the file, don't fail, just carry on
    try:
        h5py.File(f, 'r')
    except IOError:
        logging.info('IOError with file ' + f)
        continue

    # Triggers for this file only:
    triggers = {}
    with h5py.File(f, 'r') as fin:
        # Open the file: does it have the ifo group and snr dataset?
        for ifo in args.ifos:
            if not (ifo in fin and 'snr' in fin[ifo]):
                continue
            live_time[ifo] += 8

            n_triggers = fin[ifo]['snr'].size
            # Skip if there are no triggers
            if not n_triggers:
                continue

            # Read datasets from file, only where 
            triggers[ifo] = {k: fin[ifo][k][:] for k in fin[ifo].keys()
                             if k not in ('loudest', 'stat')}

            # The stored chisq is actually reduced chisq, so we need to
            # alter the chisq_dof dataset so we can use the standard conversions.
            # chisq_dof of 1.5 gives the right number (2 * 1.5 - 2 = 1)
            triggers[ifo]['chisq_dof'] = \
                1.5 * np.ones_like(triggers[ifo]['snr'])


    for ifo in args.ifos:
        # Continue if not triggers found in file
        if ifo not in triggers:
            continue

        # Apply the cuts to triggers
        keep_idx = cuts.apply_trigger_cuts(triggers[ifo], trigger_cut_dict)

        # triggers contains the datasets that we want to use for
        # the template cuts, so it can be used as the template bank
        # in this use case
        keep_idx = cuts.apply_template_cuts(triggers[ifo], template_cut_dict,
                                            template_ids=keep_idx)

        # Skip if no triggers survive the cuts
        if not keep_idx.size: continue

        # Apply the cuts
        triggers_cut = {k: triggers[ifo][k][keep_idx]
                        for k in triggers[ifo].keys()}

        # Calculate the sngl_ranking value to be used in the fits
        sngls_value = ranking.get_sngls_ranking_from_trigs(
                          triggers_cut, args.sngl_ranking)

        triggers_cut[args.sngl_ranking] = sngls_value

        triggers_da = DictArray(data=triggers_cut)

        # If we are clustering, take the max sngl_ranking value
        if args.cluster:
            # Use sngls_ranking parameter for clustering
            max_idx = sngls_value.argmax()
            triggers_cut = triggers_da.select(max_idx)

        if no_triggers_yet[ifo]:
            events[ifo] = triggers_da
            no_triggers_yet[ifo] = False
        else:
            events[ifo] += triggers_da

logging.info("All events processed")

logging.info("Number of events which meet all criteria:")
for ifo in args.ifos:
    logging.info("%s: %d in %.2fs",
                 ifo, len(events[ifo]), live_time[ifo])

# split the events into bins by template duration
logging.info('Sorting events into template duration bins')

oput_file = os.path.join(args.output_directory,
                         args.analysis_date + "-TRIGGER-FITS.hdf")

# Fit the triggers within each bin
n_bins = duration_bin_edges.size - 1
alphas = {i: np.zeros(n_bins, dtype=np.float32) for i in args.ifos}
counts = {i: np.zeros(n_bins, dtype=np.float32) for i in args.ifos}
binned_sngl_stats = {ifo: {} for ifo in args.ifos}

for ifo in args.ifos:
    # Sort the events into their bins
    event_bin = np.array([tbins[d]
                          for d in events[ifo].data['template_duration']])

    # For each bin, do the fit
    for bin_num in range(n_bins):
        inbin = event_bin == bin_num
        if not np.count_nonzero(inbin):
            # No triggers, alpha and count are -1
            counts[ifo][bin_num] = -1
            alphas[ifo][bin_num] = -1
            continue

        # For ease, keep the sngl_ranking values as 
        binned_sngl_stats[ifo][bin_num] = \
            events[ifo].data[args.sngl_ranking][inbin]

        counts[ifo][bin_num] = np.count_nonzero(inbin)
        alphas[ifo][bin_num], _ = trstats.fit_above_thresh(args.fit_function,
                                      binned_sngl_stats[ifo][bin_num],
                                      args.fit_threshold)

with h5py.File(oput_file, 'w') as fout:

    for ifo in args.ifos:
        fout.create_group(ifo)
        # Save the triggers we have used for the fits
        fout[ifo].create_group('triggers')
        for key in events[ifo].data:
            fout[ifo]['triggers'][key] = events[ifo].data[key]

        fout[ifo]['fit_coeff'] = alphas[ifo]
        fout[ifo]['counts'] = counts[ifo]
        fout[ifo].attrs['live_time'] = live_time[ifo]

    fout['bins_upper'] = tbins.upper()
    fout['bins_lower'] = tbins.lower()

    fout.attrs['analysis_date'] = args.analysis_date
    fout.attrs['input'] = sys.argv
    fout.attrs['cuts'] = args.template_cuts + args.trigger_cuts
    fout.attrs['fit_function'] = args.fit_function
    fout.attrs['fit_threshold'] = args.fit_threshold

if not args.output_fit_plot:
    logging.info("Done")
    exit(0)

import matplotlib
matplotlib.use('agg')
from matplotlib import pyplot as plt

histcolors = ['r',(1.0,0.6,0),'y','g','c','b','m',(0.8,0.25,0),(0.25,0.8,0)]

logging.info("Plotting fits")

for ifo in args.ifos:
    sngl_stat = events[ifo].data[args.sngl_ranking]

    # Skip if no triggers in this IFO
    if not sngl_stat.size: continue

    # Keep track of some maxima for use in setting the plot limits
    maxstat = sngl_stat.max()
    max_rate = 0

    fig, ax = plt.subplots(1)

    plotbins = np.linspace(args.fit_threshold, 1.05 * maxstat)

    for bin_num, lower_upper in enumerate(zip(duration_bin_edges[:-1],
                                              duration_bin_edges[1:])):
        lower, upper = lower_upper
        binlabel = f"{lower:.3g} - {upper:.3g}"

        # Skip if there are no triggers in this bin in this IFO
        if not bin_num in binned_sngl_stats[ifo]: continue

        # Histogram the triggers
        histcounts, edges = np.histogram(binned_sngl_stats[ifo][bin_num],
                                         bins=plotbins)
        cum_rate = histcounts[::-1].cumsum()[::-1] / live_time[ifo]

        max_rate = max(max_rate, cum_rate[0])

        cum_fit = counts[ifo][bin_num] / live_time[ifo] * \
                      trstats.cum_fit(args.fit_function, plotbins,
                                      alphas[ifo][bin_num], args.fit_threshold)
        ax.plot(edges[:-1], cum_rate, linewidth=2,
                color=histcolors[bin_num], label=binlabel, alpha=0.6)
        ax.plot(plotbins, cum_fit, "--", color=histcolors[bin_num],
                label=r"$\alpha = $%.2f" % alphas[ifo][bin_num])
    oput_plot = os.path.join(args.output_directory,
                             ifo + '-' + args.analysis_date + "-TRIGGER-FITS.png")
    ax.semilogy()
    ax.grid()
    ax.set_xlim(args.fit_threshold, 1.05 * maxstat)
    ax.set_ylim(0.5 / live_time[ifo], 1.5 * max_rate)
    ax.set_xlabel(args.sngl_ranking)
    ax.set_ylabel("Number of louder triggers per live time")
    ax.legend()
    logging.getLogger().setLevel(logging.WARNING)
    fig.savefig(oput_plot)
