#!/bin/env python
""" Create a file containing the phase and amplitude, 
correlations between two detectors by
doing a simple monte-carlo
"""
import argparse, h5py, numpy.random, pycbc.detector, logging, multiprocessing
from numpy.random import normal, uniform, power
from scipy.stats import norm

parser = argparse.ArgumentParser()
parser.add_argument('--ifos', nargs='+',
                    help="The ifos to generate a histogram for")
parser.add_argument('--sample-size', type=int, 
                    help="Approximant number of independent samples to draw for the distribution")
parser.add_argument('--snr-ratio', type=float, 
                    help="SNR ratio permitted between reference ifo and all others")
parser.add_argument('--relative-sensitivities', nargs='+', type=float)
parser.add_argument('--seed', type=int, default=124)
parser.add_argument('--output-file')
parser.add_argument('--verbose', action='store_true')
parser.add_argument('--smoothing-sigma', type=int, default=2)
parser.add_argument('--timing-uncertainty', type=float, default=.001,
                    help="Timing uncertainty to set bin size and smoothing interval")
parser.add_argument('--phase-uncertainty', type=float, default=0.25,
                    help="Phase uncertainty used to set bin size and smoothing")
parser.add_argument('--snr-reference', type=float, default=5, 
                    help="Reference SNR to scale SNR uncertainty")
parser.add_argument('--snr-uncertainty', type=float, default=1.4, 
                    help="SNR uncertainty to set bin size and smoothing")
args = parser.parse_args()

twidth = args.timing_uncertainty  # approximate timing error at lower SNRs
pwidth = args.phase_uncertainty   # approximate phase error at lower SNRs
serr = args.snr_uncertainty   #
sref = args.snr_reference     # Reference SNR for bin smoothing
swidth = serr / sref

srbmax = int((args.snr_ratio / swidth))
srbmin = int((1.0 / args.snr_ratio) / swidth)

# Apply a simple smoothing to help account for
# measurement errors for weak signals
def smooth_param(data, sigma, index):
    bins = numpy.arange(-args.smoothing_sigma, args.smoothing_sigma+1)
    kernel = norm.pdf(bins)
    
    nweights = {}
    
    # This is massively redundant as many points may be 
    # recalculated, but it's straightforward.
    for key in weights:
        for a in bins:
            nkey = list(key)
            nkey[index] += a
            
            weight = 0
            for b, w in zip(bins, kernel):
                wkey = list(nkey)
                wkey[index] += b
                wkey = tuple(wkey)
                
                if wkey in data:
                    weight += data[wkey] * w
                
            nweights[tuple(nkey)] = weight
            
    return nweights
    
d = {ifo: pycbc.detector.Detector(ifo) for ifo in args.ifos}

pycbc.init_logging(args.verbose)

numpy.random.seed(args.seed)
size = 1000000
chunks = int(args.sample_size / size) + 1

l = 0
nsamples = 0
weights = {}
for k in range(chunks):
    nsamples += size
    logging.info('generating %s samples' % size)

    # Choose random sky location and polarizations from
    # an isotropic population
    ra = uniform(0, 2 * numpy.pi, size=size)
    dec = numpy.arccos(uniform(-1., 1., size=size)) - numpy.pi/2
    inc = numpy.arccos(uniform(-1., 1., size=size))
    pol = uniform(0, 2 * numpy.pi, size=size)
    ip = numpy.cos(inc)
    ic = 0.5 * (1.0 + ip * ip)

    # calculate the toa, poa, and amplitude of each sample
    data = {}
    for r, ifo in enumerate(args.ifos):
        data[ifo] = {}
        fp, fc = d[ifo].antenna_pattern(ra, dec, pol, 0)
        sp, sc = fp * ip, fc * ic
        data[ifo]['s'] = ((sp)**2.0 + (sc)**2.0) ** 0.5 * args.relative_sensitivities[r]
        data[ifo]['t'] = d[ifo].time_delay_from_earth_center(ra, dec, 0)
        data[ifo]['p'] = numpy.arctan2(sc, sp)

    # Bin the data
    bind = []
    keep = None
    for i in range(len(args.ifos) -1):
        ifo0 = args.ifos[0 + i]
        ifo1 = args.ifos[1 + i]
        dt = (data[ifo0]['t'] - data[ifo1]['t'])
        dp = (data[ifo0]['p'] - data[ifo1]['p']) % (numpy.pi * 2.0)
        sr = (data[ifo1]['s'] / data[args.ifos[0]]['s'])
        dtbin = (dt / twidth).astype(numpy.int)
        dpbin = (dp / pwidth).astype(numpy.int)
        srbin = (sr / swidth).astype(numpy.int)
        
        # We'll only store a limited range of ratios
        if keep is None:
            keep = (srbin < srbmax) & srbin > (srbmin)
        else:
            keep = keep & (srbin < srbmax) & srbin > (srbmin)
        bind += [dtbin, dpbin, srbin]
 
    # Calculate and sum the weights for each bin
    # use first ifo as reference for weights
    bind = [a[keep] for a in bind]
    
    w = data[args.ifos[0]]['s'][keep] ** 3            
    for i, key in enumerate(zip(*bind)):
        if key not in weights:
            weights[key] = 0
        weights[key] += w[i]
        
    ol = l
    l = len(weights.values())
    logging.info('%s, %s, %s, %s', l, l - ol, (l - ol) / float(size), l / float(nsamples))

logging.info('applying smoothing')
# apply smoothing iteratively
for i, ifo in enumerate(range(len(args.ifos)-1)):
    logging.info('%s-time', len(weights))
    weights = smooth_param(weights, twidth, i*3 + 0)
    logging.info('%s-phase', len(weights))
    weights = smooth_param(weights, pwidth, i*3 + 1)
    logging.info('%s-amp', len(weights))
    weights = smooth_param(weights, swidth, i*3 + 2)    
logging.info('smoothing done: %s', len(weights))

# save dict to hdf5 file as key + value array
f = h5py.File(args.output_file, 'w')
keys = numpy.array(weights.keys())
values = numpy.array(weights.values())
f['param_bin'] = keys
f['weights'] = values / values.max()

f.attrs['sensitivity_ratios'] = args.relative_sensitivities
f.attrs['srbmin'] = srbmin
f.attrs['srbmax'] = srbmax
f.attrs['twidth'] = twidth
f.attrs['pwidth'] = pwidth
f.attrs['swidth'] = swidth
f.attrs['ifos'] = args.ifos
