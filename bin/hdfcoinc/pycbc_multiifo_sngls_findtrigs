#!/usr/bin/env python
import h5py, argparse, logging, numpy, numpy.random
from ligo.segments import infinity
from pycbc.events import veto, coinc, stat
import pycbc.conversions as conv
import pycbc.version
from numpy.random import seed, shuffle

parser = argparse.ArgumentParser()
parser.add_argument("--verbose", action="count")
parser.add_argument("--version", action="version", version=pycbc.version.git_verbose_msg)
parser.add_argument("--veto-files", nargs='*', action='append', default=[],
                    help="Optional veto file. Triggers within veto segments "
                         "contained in the file are ignored")
parser.add_argument("--segment-name", nargs='*', action='append', default=[],
                    help="Optional, name of veto segment in veto file")
parser.add_argument("--trigger-file",type=str,
                    help="File containing single-detector triggers")
parser.add_argument("--template-bank", required=True,
                    help="Template bank file in HDF format")
# produces a list of lists to allow multiple invocations and multiple args
parser.add_argument("--statistic-files", nargs='*', action='append', default=[],
                    help="Files containing ranking statistic info")
parser.add_argument("--ranking-statistic", choices=stat.statistic_dict.keys(),
                    default='newsnr',
                    help="The single-detector ranking statistic to calculate")
parser.add_argument("--statistic-keywords", nargs='*',
                    default=[],
                    help="Provide additional key-word arguments to be sent to "
                         "the statistic class when it is initialized. Should "
                         "be given in format --statistic-keywords "
                         "KWARG1:VALUE1 KWARG2:VALUE2 KWARG3:VALUE3 ...")
parser.add_argument("--loudest-keep-values", type=str, nargs='*',
                    default=['6:1'],
                    help="Apply successive multiplicative levels of decimation"
                         " to triggers with stat value below the given thresholds"
                         " Ex. 9:10 8.5:30 8:30 7.5:30. Default: no decimation")
parser.add_argument("--template-fraction-range", default="0/1",
                    help="Optional, analyze only part of template bank. Format"
                         " PART/NUM_PARTS")
parser.add_argument("--randomize-template-order", action="store_true",
                    help="Random shuffle templates with fixed seed "
                         "before selecting range to analyze")
parser.add_argument("--cluster-window", type=float,
                    help="Optional, window size in seconds to cluster "
                         "over the bank")
parser.add_argument("--output-file",
                    help="File to store the candidate triggers")
parser.add_argument("--batch-singles", default=5000, type=int,
                    help="Number of single triggers to process at once")
args = parser.parse_args()

args.statistic_files = sum(args.statistic_files, [])
args.segment_name = sum(args.segment_name, [])
args.veto_files = sum(args.veto_files, [])

if args.verbose:
    logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)


def parse_template_range(num_templates, rangestr):
    part = int(rangestr.split('/')[0])
    pieces = int(rangestr.split('/')[1])
    tmin = int(num_templates / float(pieces) * part)
    tmax = int(num_templates / float(pieces) * (part+1))
    return tmin, tmax

class ReadByTemplate(object):
    def __init__(self, filename, bank=None, segment_name=[], veto_files=[]):
        self.filename = filename
        self.file = h5py.File(filename, 'r')
        self.ifo = tuple(self.file.keys())[0]
        self.valid = None
        self.bank = h5py.File(bank, 'r') if bank else None

        # Determine the segments which define the boundaries of valid times
        # to use triggers
        from ligo.segments import segmentlist, segment
        key = '%s/search/' % self.ifo
        s, e = self.file[key + 'start_time'][:], self.file[key + 'end_time'][:]
        self.segs = veto.start_end_to_segments(s, e).coalesce()
        for vfile, name in zip(veto_files, segment_name):
            veto_segs = veto.select_segments_by_definer(vfile, ifo=self.ifo,
                                                        segment_name=name)
            self.segs = (self.segs - veto_segs).coalesce()
        self.valid = veto.segments_to_start_end(self.segs)

    def get_data(self, col, num):
        """ Get a column of data for template with id 'num'

        Parameters
        ----------
        col: str
            Name of column to read
        num: int
            The template id to read triggers for

        Returns
        -------
        data: numpy.ndarray
            The requested column of data
        """
        ref = self.file['%s/%s_template' % (self.ifo, col)][num]
        return self.file['%s/%s' % (self.ifo, col)][ref]

    def set_template(self, num):
        """ Set the active template to read from

        Parameters
        ----------
        num: int
            The template id to read triggers for

        Returns
        -------
        trigger_id: numpy.ndarray
            The indices of this templates triggers
        """
        self.template_num = num
        times = self.get_data('end_time', num)

        # Determine which of these template's triggers are kept after
        # applying vetoes
        if self.valid:
            self.keep = veto.indices_within_times(times, self.valid[0],
                                                  self.valid[1])
            logging.info('applying vetoes')
        else:
            self.keep = numpy.arange(0, len(times))

        if self.bank is not None:
            self.param = {}
            if 'parameters' in self.bank.attrs :
                for col in self.bank.attrs['parameters']:
                    self.param[col] = self.bank[col][self.template_num]
            else :
                for col in self.bank:
                    self.param[col] = self.bank[col][self.template_num]

        # Calculate the trigger id by adding the relative offset in self.keep
        # to the absolute beginning index of this templates triggers stored
        # in 'template_boundaries'
        trigger_id = self.keep + self.file['%s/template_boundaries' % self.ifo][num]
        return trigger_id

    def __getitem__(self, col):
        """ Return the column of data for current active template after
        applying vetoes

        Parameters
        ----------
        col: str
            Name of column to read

        Returns
        -------
        data: numpy.ndarray
            The requested column of data
        """
        if self.template_num == None:
            raise ValueError('You must call set_template to first pick the '
                             'template to read data from')
        data = self.get_data(col, self.template_num)
        data = data[self.keep] if self.valid else data
        return data

logging.info('Starting...')

num_templates = len(h5py.File(args.template_bank, "r")['template_hash'])
tmin, tmax = parse_template_range(num_templates, args.template_fraction_range)
logging.info('Analyzing template %s - %s' % (tmin, tmax-1))

class Trigs(object):
    """store trigger info in parallel with ifo name and shift vector"""
    def __init__(self):
        self.singles = []

trigs = Trigs()
logging.info('Opening trigger file : %s' % args.trigger_file)
reader = ReadByTemplate(args.trigger_file,
                        args.template_bank,
                        args.segment_name,
                        args.veto_files)
ifo = reader.ifo
trigs.singles.append(reader)

fg_segs = reader.segs
valid = veto.segments_to_start_end(reader.segs)

# Stat class instance to calculate the ranking statistic
extra_kwargs = {}
for inputstr in args.statistic_keywords:
    try:
        key, value = inputstr.split(':')
        extra_kwargs[key] = value
    except ValueError:
        err_txt = "--statistic-keywords must take input in the " \
                  "form KWARG1:VALUE1 KWARG2:VALUE2 KWARG3:VALUE3 ... " \
                  "Received {}".format(args.statistic_keywords)
        raise ValueError(err_txt)

rank_method = stat.get_statistic(args.ranking_statistic)(args.statistic_files,
                                                         ifos=[ifo],
                                                         **extra_kwargs)

if args.randomize_template_order:
    seed(0)
    template_ids = numpy.arange(0, num_templates)
    shuffle(template_ids)
    template_ids = template_ids[tmin:tmax]
else:
    template_ids = range(tmin, tmax)

# 'data' will store candidate triggers
# in addition to these lists of info, will also store trigger times and
# ids in ifo-specific datasets
data = {'stat': [], 'decimation_factor': [], 'timeslide_id': [],
        'template_id': [], '%s/time' % ifo : [], '%s/trigger_id' % ifo: []}

for tnum in template_ids:
    logging.info('Obtaining trigs for template %i ..' % (tnum))
    sngl = reader
    tids_full = sngl.set_template(tnum)
    times_full = sngl['end_time']
    sds_full = rank_method.single(sngl)
    logging.info('%s' % len(tids_full))

    if len(tids_full) == 0:
        logging.info('No triggers in template %i, skipping' % tnum)
        continue

    logging.info('Calculating multi-detector singles statistic')
    # list in ifo order of remaining trigger data
    single_info = (ifo, sds_full)
    data['stat'] += [rank_method.single_multiifo(single_info)]
    # All triggers are foreground and therefore not decimated
    data['decimation_factor'] += [numpy.repeat(1, len(sds_full))]
    # No timeslides performed - all zeros
    data['timeslide_id'] += [numpy.repeat(0, len(sds_full))]
    data['template_id'] += [numpy.repeat(tnum, len(sds_full))]
    data['%s/time' % ifo] += [times_full]
    data['%s/trigger_id' % ifo] += [tids_full]

if len(data['stat']) > 0:
    for key in data:
        data[key] = numpy.concatenate(data[key])
else:
    raise RuntimeError("No triggers in any of the templates - expand template "
                       "range or run for longer")
    

if args.cluster_window and len(data['stat']) > 0:
    cid = coinc.cluster_over_time(data['stat'], data['%s/time' % ifo],
                                  args.cluster_window)

logging.info('saving clustered triggers')
f = h5py.File(args.output_file, 'w')
if len(data['stat']) > 0:
    for key in data:
        var = data[key][cid] if args.cluster_window else data[key]
        f.create_dataset('foreground/' + key, data=var,
                         compression='gzip',
                         compression_opts=9,
                         shuffle=True)

# Store segments
f['segments/%s/start' % ifo], f['segments/%s/end' % ifo] = reader.valid

f.attrs['foreground_time'] = abs(fg_segs)
f.attrs['num_of_ifos'] = 1
f.attrs['pivot'] = ifo
f.attrs['fixed'] = ifo
f.attrs['ifos'] = ifo

fore_stat = data['stat'][cid]

logging.info('Assigning IFARs according to X model')
# Currently just using a "Count N louder" model as it's easy to code - 
# which is a bit crap and doesnt give far below 1 per live time
fnlouder = numpy.argsort(fore_stat[::-1])
fg_time = abs(fg_segs)
ifar = fg_time / (fnlouder + 1)
fap = 1 - numpy.exp(-fg_time / ifar)

f['foreground/ifar'] = conv.sec_to_year(ifar)
f['foreground/fap'] = fap

#Do hierarchical removal
#h_iterations = 0
#if args.max_hierarchical_removal != 0:

logging.info('Done')
