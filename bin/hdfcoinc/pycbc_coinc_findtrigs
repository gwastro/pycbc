#!/usr/bin/env python
import h5py, argparse, logging, numpy
from pycbc import events, detector, io
from pycbc import pnutils
from pycbc.events import veto, coinc
import pycbc.version


def parse_template_range(num_templates, rangestr):
    part = int(rangestr.split('/')[0])
    pieces = int(rangestr.split('/')[1])
    tmin =  int(num_templates / float(pieces) * part)
    tmax =  int(num_templates / float(pieces) * (part+1))
    return tmin, tmax

class Stat(object):
    def __init__(self, files):
        """ Files are accessible as the values of a dictionary self.statfiles
        keyed on the 'stat' attribute of each file.
        """
        self.statfiles = {}
        for fname in files:
            f = h5py.File(fname, 'r')
            s = f.attrs['stat']
            if s in self.statfiles.keys():
                raise RuntimeError("Can't read two files with the same 'stat'"
                                   " attribute value!")
            self.statfiles[s] = f

class NewSNRStatistic(Stat):
    def single(self, trigs):
        """ Read in the single detector information and make a single detector
        statistic. Results can either be a single number or a record array.
        """

        dof = 2 * trigs['chisq_dof'] - 2
        newsnr = events.newsnr(trigs['snr'], trigs['chisq'] / dof)
        return numpy.array(newsnr, ndmin=1, dtype=numpy.float32)

    def coinc(self, s0, s1, slide, step):
        """ Calculate the coincident statistic.
        """
        return (s0 ** 2.0 + s1 ** 2.0) ** 0.5

class NewSNRCutStatistic(Stat):
    def single(self, trigs):
        dof = 2 * trigs['chisq_dof'] - 2
        rchisq = trigs['chisq'] / dof
        newsnr = events.newsnr(trigs['snr'], rchisq)
        newsnr[numpy.logical_and(newsnr < 10, rchisq > 2)] = -1
        return newsnr

    def coinc(self, s0, s1, slide, step):
        cstat = (s0 ** 2.0 + s1 ** 2.0) ** 0.5
        cstat[s0 == -1] = 0
        cstat[s1 == -1] = 0
        return cstat

class PhaseTDStatistic(NewSNRStatistic):
    def __init__(self, files):
        NewSNRStatistic.__init__(self, files)
        self.hist = self.statfiles['phasetd_newsnr']['map'][:]
        top = float(self.hist.max())

        #normalize so that peak has no effect on newsnr
        self.hist = self.hist / top
        self.hist = numpy.log(self.hist)

    def single(self, trigs):
        newsnr = NewSNRStatistic.single(self, trigs)
        return numpy.array((newsnr, trigs['coa_phase'], trigs['end_time'], trigs['sigmasq']**0.5, trigs['snr'])).transpose()

    def coinc(self, s0, s1, slide, step):
        """ Calculate the coincident statistic.
        """
        td = s0[:,2] - s1[:,2] - slide * step
        pd = (s0[:,1] - s1[:,1]) % (numpy.pi * 2)
        rd = s0[:, 3] / s1[:, 3]
        rd[rd > 1] = 1.0 / rd[rd > 1]

        # These are the bin boundaries stored in the hdf file
        tbins = self.statfiles['phasetd_newsnr']['tbins'][:]
        pbins = self.statfiles['phasetd_newsnr']['pbins'][:]
        rbins = self.statfiles['phasetd_newsnr']['rbins'][:]
        mbins = self.statfiles['phasetd_newsnr']['mbins'][:]

        # Find which bin each coinc falls into
        tv = numpy.searchsorted(tbins, td) - 1
        pv = numpy.searchsorted(pbins, pd) - 1
        s0v = numpy.searchsorted(sbins, s0[:,4]) - 1
        s1v = numpy.searchsorted(sbins, s1[:,4]) - 1
        rv = numpy.searchsorted(rbins, rd) - 1

        # The following just enforces that the point fits into the bin boundaries
        tv[tv < 0] = 0
        tv[tv >= len(tbins) - 1] = len(tbins) - 2
        pv[pv < 0] = 0
        pv[pv >= len(pbins) - 1] = len(pbins) - 2
        s0v[s0v < 0] = 0
        s0v[s0v >= len(sbins) - 1] = len(sbins) - 2
        s1v[s1v < 0] = 0
        s1v[s1v >= len(sbins) - 1] = len(sbins) - 2
        rv[rv < 0] = 0
        rv[rv >= len(rbins) - 1] = len(rbins) - 2

        m = self.hist[tv, pv, s0v, s1v, rv]
        rstat = s0[:,0]**2.0 + s1[:,0]**2.0
        cstat = rstat + 2.0 * m
        cstat[cstat < 0] = 0
        return cstat ** 0.5

class MaxContTradNewSNRStatistic(NewSNRStatistic):
    def single(self, trigs):
        """Combined chisq calculation for each trigger."""
        chisq_dof = 2 * trigs['chisq_dof'] - 2
        chisq_newsnr = events.newsnr(trigs['snr'], trigs['chisq'] / chisq_dof)
        autochisq_dof = trigs['cont_chisq_dof']
        autochisq_newsnr = events.newsnr(trigs['snr'],
                                         trigs['cont_chisq'] / autochisq_dof)
        return numpy.array(numpy.minimum(chisq_newsnr, autochisq_newsnr,
                             dtype=numpy.float32), ndmin=1, copy=False)

class ExpFitStatistic(NewSNRStatistic):
    def __init__(self, files):
        NewSNRStatistic.__init__(self, files)
        if not len(files):
            raise RuntimeError("Can't find any statistic files !")
        # the stat file attributes are hard-coded as '%{ifo}-fit_coeffs'
        self.ifos = [f.split('-')[0] for f in self.statfiles.keys() if \
                     f.split('-')[1] == 'fit_coeffs']
        if (not len(files)) or (not len(self.ifos)):
            raise RuntimeError("None of the statistic files has the required "
                               "attribute called {ifo}-fit_coeffs !")
        self.fits_by_tid = {}
        for i in self.ifos:
           self.fits_by_tid[i] = self.assign_fits(i)
        # HACK: hard-code the point at which normalization of trigger rate was done
        self.count_thresh = 6.

    def assign_fits(self, ifo):
        # the template_ids and fit coeffs are stored in an arbitrary order
        # create new arrays in template_id order for easier recall
        coeff_file = self.statfiles[ifo+'-fit_coeffs']
        template_id = coeff_file['template_id'][:]
        alphas = coeff_file['fit_coeff'][:]
        lambdas = coeff_file['count_above_thresh'][:]
        tid_sort = numpy.argsort(template_id)
        return {'alpha':alphas[tid_sort], 'lambda':lambdas[tid_sort]}

    def find_fits(self, trigs):
        # get the coeffs for a specific ifo and template id
        ifo = trigs.ifo
        tnum = trigs.template_num
        # fits_by_template is a dictionary of dictionaries of arrays
        # indexed by ifo / coefficient name / template_id
        alphai = self.fits_by_tid[ifo]['alpha'][tnum][0]
        lambdai = self.fits_by_tid[ifo]['lambda'][tnum][0]
        return alphai, lambdai

    def single(self, trigs):
        """ Read in the single detector information and make the newsnr
        statistic and rescale it by the fitted coefficients
        """
        alphai, lambdai = self.find_fits(trigs)
        dof = 2 * trigs['chisq_dof'] - 2
        newsnr = events.newsnr(trigs['snr'], trigs['chisq'] / dof)
        # alphai is constant of proportionality between single-ifo newsnr and
        # negative log noise likelihood in given template
        # lambdai is rate of trigs in given template compared to average
        lognoisel = - alphai * (newsnr - self.count_thresh) + numpy.log(alphai) + numpy.log(lambdai)
        # correction for detectability - NOT YET
        #chirpfac = pnutils.mass1_mass2_to_mchirp_eta(trigs.param['mass1'], trigs.param['mass2'])[0] / 1.22
        #volterm = (5./2.) * numpy.log(chirpfac)
        # make noise likelihood more negative for high chirp mass
        # do half of the correction for each ifo (hack)
        #lognoisel -= 0.5 * volterm
        return numpy.array(lognoisel, ndmin=1, dtype=numpy.float32)

    def coinc(self, s0, s1, slide, step):
        # approximate log likelihood ratio by sum of single-ifo negative
        # log noise likelihoods
        loglr = - s0 - s1
        # convert back to a familiar statistic-like value
        # notionally, log likelihood ratio \propto rho_c^2 / 2
        return (2 * self.count_thresh**2 + 2 * loglr) ** 0.5

statistic_dict = {'newsnr': NewSNRStatistic,
                  'newsnr_cut': NewSNRCutStatistic,
                  'phasetd_newsnr': PhaseTDStatistic,
                  'max_cont_trad_newsnr': MaxContTradNewSNRStatistic,
                  'exp_fit_stat': ExpFitStatistic
                 }


parser = argparse.ArgumentParser()
parser.add_argument("--verbose", action="count")
parser.add_argument("--version", action="version", version=pycbc.version.git_verbose_msg)
parser.add_argument("--veto-files", nargs='*',
                    help="Optional veto file. Triggers within these times are ignored")
parser.add_argument("--strict-coinc-time", action='store_true',
                    help="Optional, only allow coincidences between triggers that are "
                         "in coincident time after applying vetoes ")
parser.add_argument("--segment-name", default=None, type=str,
                    help="Optional, name of segment list to use for vetoes")
parser.add_argument("--trigger-files", nargs=2,
                    help="Files containing the single-detector triggers. "
                         "Two files must be provided")
parser.add_argument("--template-bank", required=True,
                    help="Template bank file in HDF format. Required")
parser.add_argument("--ranking-statistic", choices=statistic_dict.keys(),
                    help="The ranking statistic to use", default='newsnr')
parser.add_argument("--coinc-threshold", type=float, default=0.0,
                    help="Seconds to add to time-of-flight coincidence window")
parser.add_argument("--timeslide-interval", type=float, default=0,
                    help="Interval between timeslides in seconds.")
parser.add_argument("--decimation-factor", type=int,
                    help="The factor to reduce the background trigger rate.")
parser.add_argument("--loudest-keep", type=int,
                    help="Number of the loudest triggers to keep from each template.")
parser.add_argument("--loudest-keep-value", type=float,
                    help="Keep all coincident triggers above this value.")
parser.add_argument("--template-fraction-range", help="Optional, format string to"
                    "analyze part of template bank. Format is PART/NUM_PARTS",
                    default="0/1")
parser.add_argument("--cluster-window", help="Optional, window size in seconds to "
                    "cluster coincidences over the bank", type=float)
parser.add_argument("--output-file", help="File to store the coincident triggers")
parser.add_argument("--statistic-files", help="Statistic mapping files", nargs='*', default=[])
args = parser.parse_args()

if args.verbose:
    logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)

######## MAIN ########
logging.info('Starting...')

num_templates = len(h5py.File(args.template_bank, "r")['template_hash'])
tmin, tmax = parse_template_range(num_templates, args.template_fraction_range)
logging.info('Analyzing template %s - %s' % (tmin, tmax-1))

logging.info('Opening first trigger file: %s' % args.trigger_files[0])
trigs0 = io.ReadSnglsByTemplate(args.trigger_files[0],
                       args.template_bank, args.segment_name, args.veto_files)
logging.info('Opening second trigger file: %s' % args.trigger_files[1])
trigs1 = io.ReadSnglsByTemplate(args.trigger_files[1],
                        args.template_bank, args.segment_name, args.veto_files)
coinc_segs = (trigs0.segs & trigs1.segs).coalesce()

if args.strict_coinc_time:
    trigs0.segs = coinc_segs
    trigs1.segs = coinc_segs
    trigs0.valid = veto.segments_to_start_end(trigs0.segs)
    trigs1.valid = veto.segments_to_start_end(trigs1.segs)

rank_method = statistic_dict[args.ranking_statistic](args.statistic_files)
det0, det1 = detector.Detector(trigs0.ifo), detector.Detector(trigs1.ifo)
time_window = det0.light_travel_time_to_detector(det1) + args.coinc_threshold
logging.info('The coincidence window is %3.1f ms' % (time_window * 1000))

data = {'stat':[], 'decimation_factor':[], 'time1':[], 'time2':[],
        'trigger_id1':[], 'trigger_id2':[], 'timeslide_id':[], 'template_id':[]}

for tnum in range(tmin, tmax):
    tid0 = trigs0.set_template(tnum)
    tid1 = trigs1.set_template(tnum)

    if (len(tid0) == 0) or (len(tid1) == 0):
        continue

    t0 = trigs0['end_time']
    t1 = trigs1['end_time']
    logging.info('Trigs for template %s, %s:%s %s:%s' % \
                (tnum, trigs0.ifo, len(t0), trigs1.ifo, len(t1)))

    i0, i1, slide = coinc.time_coincidence(t0, t1, time_window, args.timeslide_interval)

    logging.info('Coincident Trigs: %s' % (len(i1)))

    logging.info('Calculating Single Detector Statistic')
    s0, s1 = rank_method.single(trigs0), rank_method.single(trigs1)

    logging.info('Calculating Multi-Detector Combined Statistic')
    c = rank_method.coinc(s0[i0], s1[i1], slide, args.timeslide_interval)

    #index values of the zerolag triggers
    fi = numpy.where(slide == 0)[0]

    #index values of the background triggers
    bi = numpy.where(slide != 0)[0]
    logging.info('%s foreground triggers' % len(fi))
    logging.info('%s background triggers' % len(bi))

    # We split the background triggers into two types which we keep track of
    # in "bh" (triggers which are *not* decimated, stored in full) and
    # "bl" (triggers which may not be stored in full, but we keep track of
    # how many are removed)
    # "bl_int" keeps track of which triggers are *not* in the bh set. Depending
    # on the decimation factor option we may not store any of these or we may
    # keep a fraction of them corresponding to a subset of the timeslides
    if args.loudest_keep:
        sep = len(bi) - args.loudest_keep
        sep = 0 if sep < 0 else sep

        bsort = numpy.argpartition(c[bi], sep)
        bl_int = bi[bsort[0:sep]]
        bh = bi[bsort[sep:]]
        del bsort
        del bi
    elif args.loudest_keep_value:
        bh = bi[c[bi] > args.loudest_keep_value]
        bl_int = bi[c[bi] <= args.loudest_keep_value]
    else:
        bh = bi

    if args.decimation_factor:
        bl = bl_int[slide[bl_int] % args.decimation_factor == 0]
    else:
        bl = []

    ti = numpy.concatenate([bl, bh, fi]).astype(numpy.uint32)
    logging.info('%s after decimation' % len(ti))

    g0 = i0[ti]
    g1 = i1[ti]
    del i0
    del i1

    data['stat'] += [c[ti]]
    dec_fac = numpy.repeat([args.decimation_factor, 1, 1],
                           [len(bl), len(bh), len(fi)]).astype(numpy.uint32)
    data['decimation_factor'] += [dec_fac]
    data['time1'] += [t0[g0]]
    data['time2'] += [t1[g1]]
    data['trigger_id1'] += [tid0[g0]]
    data['trigger_id2'] += [tid1[g1]]
    data['timeslide_id'] += [slide[ti]]
    data['template_id'] += [numpy.zeros(len(ti), dtype=numpy.uint32) + tnum]

for key in data:
    data[key] = numpy.concatenate(data[key])

if args.cluster_window and len(data['stat']) > 0:
    cid = coinc.cluster_coincs(data['stat'], data['time1'], data['time2'],
                               data['timeslide_id'], args.timeslide_interval,
                               args.cluster_window)

logging.info('saving coincident triggers')
f = h5py.File(args.output_file, 'w')
if len(data['stat']) > 0:
    for key in data:
        var = data[key][cid] if args.cluster_window else data[key]
        f.create_dataset(key, data=var,
                              compression='gzip',
                              compression_opts=9,
                              shuffle=True)

f['segments/coinc/start'], f['segments/coinc/end'] = veto.segments_to_start_end(coinc_segs)

for t in [trigs0, trigs1]:
    f['segments/%s/start' % t.ifo], f['segments/%s/end' % t.ifo] = t.valid

f.attrs['timeslide_interval'] = args.timeslide_interval
f.attrs['detector_1'] = det0.name
f.attrs['detector_2'] = det1.name
f.attrs['foreground_time1'] = abs(trigs0.segs)
f.attrs['foreground_time2'] = abs(trigs1.segs)
f.attrs['coinc_time'] = abs(coinc_segs)

if args.timeslide_interval:
    nslides = int(max(abs(trigs0.segs), abs(trigs1.segs)) / args.timeslide_interval)
else:
    nslides = 0

f.attrs['num_slides'] = nslides
logging.info('Done')
