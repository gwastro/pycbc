#!/usr/bin/env python
import h5py, argparse, logging, numpy, numpy.random
from pycbc import events, detector
from pycbc.events import veto, coinc, stat
import pycbc.version

parser = argparse.ArgumentParser()
parser.add_argument("--verbose", action="count")
parser.add_argument("--version", action="version", version=pycbc.version.git_verbose_msg)
parser.add_argument("--veto-files", nargs='*', action='append', default=[],
                    help="Optional veto file. Triggers within veto segments "
                    "contained in the file are ignored")
parser.add_argument("--segment-name", nargs='*', action='append', default=[],
                    help="Optional, name of veto segment in veto file")
parser.add_argument("--strict-coinc-time", action='store_true',
                    help="Optional, only allow coincidences between triggers "
                    "that lie in coincident time after applying vetoes")
parser.add_argument("--trigger-files", nargs='*', action='append', default=[],
                    help="File containing the single-detector triggers")
parser.add_argument("--template-bank", required=True,
                    help="Template bank file in HDF format")
# produces a list of lists to allow multiple invocations and multiple args
parser.add_argument("--statistic-files", nargs='*', action='append', default=[],
                    help="Files containing ranking statistic info")
parser.add_argument("--ranking-statistic", choices=stat.statistic_dict.keys(),
                    help="The ranking statistic to use", default='newsnr')
parser.add_argument("--use-maxalpha", action="store_true")
parser.add_argument("--coinc-threshold", type=float, default=0.0,
                    help="Seconds to add to time-of-flight coincidence window")
parser.add_argument("--timeslide-interval", type=float,
                    help="Interval between timeslides in seconds. Timeslides are "
                         "disabled if the option is omitted.")
parser.add_argument("--decimation-factor", type=int,
                    help="The factor to reduce the background trigger rate.")
loudest = parser.add_mutually_exclusive_group()
loudest.add_argument("--loudest-keep", type=int,
                     help="Keep this number of loudest triggers from each template.")
loudest.add_argument("--loudest-keep-value", type=float,
                     help="Keep all coincident triggers above this value.")
parser.add_argument("--template-fraction-range", default="0/1",
                    help="Optional, analyze only part of template bank. Format"
                    " PART/NUM_PARTS")
parser.add_argument("--cluster-window", type=float,
                    help="Optional, window size in seconds to cluster "
                    "coincidences over the bank")
parser.add_argument("--output-file",
                    help="File to store the coincident triggers")
args = parser.parse_args()
# disable detector pair calculations for 3 detector coinc testing
detpaircoinc = False
# flatten the list of lists of filenames to a single list (may be empty)
args.statistic_files = sum(args.statistic_files, [])
args.segment_name = sum(args.segment_name, [])
args.veto_files = sum(args.veto_files, [])
args.trigger_files = sum(args.trigger_files, [])

if args.verbose:
    logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)


def parse_template_range(num_templates, rangestr):
    part = int(rangestr.split('/')[0])
    pieces = int(rangestr.split('/')[1])
    tmin =  int(num_templates / float(pieces) * part)
    tmax =  int(num_templates / float(pieces) * (part+1))
    return tmin, tmax

class ReadByTemplate(object):
    def __init__(self, filename, bank=None, segment_name=[], veto_files=[]):
        self.filename = filename
        self.file = h5py.File(filename, 'r')
        self.ifo = self.file.keys()[0]
        self.valid = None
        self.bank = h5py.File(bank, 'r') if bank else None

        # Determine the segments which define the boundaries of valid times
        # to use triggers
        from glue.segments import segmentlist, segment
        key = '%s/search/' % self.ifo
        s, e = self.file[key + 'start_time'][:], self.file[key + 'end_time'][:]
        self.segs = veto.start_end_to_segments(s, e).coalesce()
        for vfile, name in zip(veto_files, segment_name):
            veto_segs = veto.select_segments_by_definer(vfile, ifo=self.ifo,
                                                     segment_name=name)
            self.segs = (self.segs - veto_segs).coalesce()
        self.valid = veto.segments_to_start_end(self.segs)

    def get_data(self, col, num):
        """ Get a column of data for template with id 'num'

        Parameters
        ----------
        col: str
            Name of column to read
        num: int
            The template id to read triggers for

        Returns
        -------
        data: numpy.ndarray
            The requested column of data
        """
        ref = self.file['%s/%s_template' % (self.ifo, col)][num]
        return self.file['%s/%s' % (self.ifo, col)][ref]

    def set_template(self, num):
        """ Set the active template to read from

        Parameters
        ----------
        num: int
            The template id to read triggers for

        Returns
        -------
        trigger_id: numpy.ndarray
            The indices of this templates triggers
        """
        self.template_num = num
        times = self.get_data('end_time', num)

        # Determine which of these template's triggers are kept after
        # applying vetoes
        if self.valid:
            self.keep = veto.indices_within_times(times, self.valid[0], self.valid[1])
            logging.info('applying vetoes')
        else:
            self.keep = numpy.arange(0, len(times))

        if self.bank is not None:
            self.param = {}
            if 'parameters' in self.bank.attrs :
                for col in self.bank.attrs['parameters']:
                    self.param[col] = self.bank[col][self.template_num]
            else :
                for col in self.bank:
                    self.param[col] = self.bank[col][self.template_num]

        # Calculate the trigger id by adding the relative offset in self.keep
        # to the absolute beginning index of this templates triggers stored
        # in 'template_boundaries'
        trigger_id = self.keep + self.file['%s/template_boundaries' % self.ifo][num]
        return trigger_id

    def __getitem__(self, col):
        """ Return the column of data for current active template after
        applying vetoes

        Parameters
        ----------
        col: str
            Name of column to read

        Returns
        -------
        data: numpy.ndarray
            The requested column of data
        """
        if self.template_num == None:
            raise ValueError('You must call set_template to first pick the '
                             'template to read data from')
        data = self.get_data(col, self.template_num)
        data = data[self.keep] if self.valid else data
        return data

logging.info('Starting...')

num_templates = len(h5py.File(args.template_bank, "r")['template_hash'])
tmin, tmax = parse_template_range(num_templates, args.template_fraction_range)
logging.info('Analyzing template %s - %s' % (tmin, tmax-1))

# Create dictionary for trigger files
trigs={}
for i in range(len(args.trigger_files)):
    logging.info('Opening trigger file %s: %s' % (i,args.trigger_files[i]))
    trigs["trigs{0}".format(i)]=ReadByTemplate(args.trigger_files[i], args.template_bank, args.segment_name, args.veto_files)


# Two detector case
if len(args.trigger_files) == 2:
    coinc_segs = (trigs['trigs0'].segs & trigs['trigs1'].segs).coalesce()
    if args.strict_coinc_time:
        trigs['trigs0'].segs = coinc_segs
        trigs['trigs1'].segs = coinc_segs
        trigs['trigs0'].valid = veto.segments_to_start_end(trigs['trigs0'].segs)
        trigs['trigs1'].valid = veto.segments_to_start_end(trigs['trigs1'].segs)
    # initialize a Stat class instance to calculate the coinc ranking statistic
    rank_method = stat.get_statistic(args.ranking_statistic)(args.statistic_files)
    if args.use_maxalpha:
        rank_method.use_alphamax()
    det0, det1 = detector.Detector(trigs['trigs0'].ifo), detector.Detector(trigs['trigs1'].ifo)
    time_window = det0.light_travel_time_to_detector(det1) + args.coinc_threshold

    if time_window >= args.timeslide_interval and args.timeslide_interval is not None:
        raise parser.error("The maximum time delay between detectors should be smaller "
                       "than the timeslide interval.")
    # slide = 0 means don't do timeslides
    if args.timeslide_interval is None:
        args.timeslide_interval = 0

    logging.info('The coincidence window is %3.1f ms' % (time_window * 1000))

    data = {'stat':[], 'decimation_factor':[], 'time1':[], 'time2':[],
        'trigger_id1':[], 'trigger_id2':[], 'timeslide_id':[], 'template_id':[]}

    for tnum in range(tmin, tmax):
        tid0 = trigs['trigs0'].set_template(tnum)
        tid1 = trigs['trigs1'].set_template(tnum)

        if (len(tid0) == 0) or (len(tid1) == 0):
            continue

        t0 = trigs['trigs0']['end_time']
        t1 = trigs['trigs1']['end_time']
        logging.info('Trigs for template %s, %s:%s %s:%s' % \
                (tnum, trigs['trigs0'].ifo, len(t0), trigs['trigs1'].ifo, len(t1)))

        i0, i1, slide = coinc.time_coincidence(t0, t1, time_window, args.timeslide_interval)

        logging.info('Coincident Trigs: %s' % (len(i1)))

        logging.info('Calculating Single Detector Statistic')
        s0, s1 = rank_method.single(trigs['trigs0']), rank_method.single(trigs['trigs1'])

        logging.info('Calculating Multi-Detector Combined Statistic')
        c = rank_method.coinc(s0[i0], s1[i1], slide, args.timeslide_interval)

        #index values of the zerolag triggers
        fi = numpy.where(slide == 0)[0]

        #index values of the background triggers
        bi = numpy.where(slide != 0)[0]
        logging.info('%s foreground triggers' % len(fi))
        logging.info('%s background triggers' % len(bi))

        # We split the background triggers into two types which we keep track of
        # in "bh" (triggers which are *not* decimated, stored in full) and
        # "bl" (triggers which may not be stored in full, but we keep track of
        # how many are removed)
        # "bl_int" keeps track of which triggers are *not* in the bh set. Depending
        # on the decimation factor option we may not store any of these or we may
        # keep a fraction of them corresponding to a subset of the timeslides
        if args.loudest_keep:
            sep = len(bi) - args.loudest_keep
            sep = 0 if sep < 0 else sep

            bsort = numpy.argpartition(c[bi], sep)
            bl_int = bi[bsort[0:sep]]
            bh = bi[bsort[sep:]]
            del bsort
            del bi
        elif args.loudest_keep_value:
            bh = bi[c[bi] > args.loudest_keep_value]
            bl_int = bi[c[bi] <= args.loudest_keep_value]
        else:
            bh = bi

        if args.decimation_factor:
            bl = bl_int[slide[bl_int] % args.decimation_factor == 0]
        else:
            bl = []

        ti = numpy.concatenate([bl, bh, fi]).astype(numpy.uint32)
        logging.info('%s after decimation' % len(ti))

        g0 = i0[ti]
        g1 = i1[ti]
        del i0
        del i1

        data['stat'] += [c[ti]]
        dec_fac = numpy.repeat([args.decimation_factor, 1, 1],
                           [len(bl), len(bh), len(fi)]).astype(numpy.uint32)
        data['decimation_factor'] += [dec_fac]
        data['time1'] += [t0[g0]]
        data['time2'] += [t1[g1]]
        data['trigger_id1'] += [tid0[g0]]
        data['trigger_id2'] += [tid1[g1]]
        data['timeslide_id'] += [slide[ti]]
        data['template_id'] += [numpy.zeros(len(ti), dtype=numpy.uint32) + tnum]

    if len(data['stat']) > 0:
        for key in data:
            data[key] = numpy.concatenate(data[key])

    if args.cluster_window and len(data['stat']) > 0:
        cid = coinc.cluster_coincs(data['stat'], data['time1'], data['time2'],
                               data['timeslide_id'], args.timeslide_interval,
                               args.cluster_window)

    logging.info('saving coincident triggers')
    f = h5py.File(args.output_file, 'w')
    if len(data['stat']) > 0:
        for key in data:
            var = data[key][cid] if args.cluster_window else data[key]
            f.create_dataset(key, data=var,
                              compression='gzip',
                              compression_opts=9,
                              shuffle=True)

    f['segments/coinc/start'], f['segments/coinc/end'] = veto.segments_to_start_end(coinc_segs)

    for t in [trigs['trigs0'], trigs['trigs1']]:
        f['segments/%s/start' % t.ifo], f['segments/%s/end' % t.ifo] = t.valid

    f.attrs['timeslide_interval'] = args.timeslide_interval
    f.attrs['detector_1'] = det0.name
    f.attrs['detector_2'] = det1.name
    f.attrs['foreground_time1'] = abs(trigs['trigs0'].segs)
    f.attrs['foreground_time2'] = abs(trigs['trigs1'].segs)
    f.attrs['coinc_time'] = abs(coinc_segs)

    if args.timeslide_interval:
        nslides = int(max(abs(trigs['trigs0'].segs), abs(trigs['trigs1'].segs)) / args.timeslide_interval)
    else:
        nslides = 0

    f.attrs['num_slides'] = nslides

# 3 detector case
elif len(args.trigger_files and detpaircoinc) == 3:
    detpairs={}
    detpairs['detpair0']=[trigs['trigs0'],trigs['trigs1']]
    detpairs['detpair1']=[trigs['trigs1'],trigs['trigs2']]
    detpairs['detpair2']=[trigs['trigs2'],trigs['trigs0']]
    for i, j in detpairs.items():
        coinc_segs = (j[0].segs & j[1].segs).coalesce()
        if args.strict_coinc_time:
            j[0].segs = coinc_segs
            j[1].segs = coinc_segs
            j[0].valid = veto.segments_to_start_end(j[0].segs)
            j[1].valid = veto.segments_to_start_end(j[1].segs)
        # initialize a Stat class instance to calculate the coinc ranking statistic
        rank_method = stat.get_statistic(args.ranking_statistic)(args.statistic_files)
        if args.use_maxalpha:
            rank_method.use_alphamax()
        det0, det1 = detector.Detector(j[0].ifo), detector.Detector(j[1].ifo)
        time_window = det0.light_travel_time_to_detector(det1) + args.coinc_threshold

        if time_window >= args.timeslide_interval and args.timeslide_interval is not None and args.timeslide_interval!=0:
            raise parser.error("The maximum time delay between detectors should be smaller "
                       "than the timeslide interval.")
        # slide = 0 means don't do timeslides
        if args.timeslide_interval is None:
            args.timeslide_interval = 0

        logging.info('The coincidence window is %3.1f ms' % (time_window * 1000))

        data = {'stat':[], 'decimation_factor':[], 'time1':[], 'time2':[],
            'trigger_id1':[], 'trigger_id2':[], 'timeslide_id':[], 'template_id':[]}

        for tnum in range(tmin, tmax):
            tid0 = j[0].set_template(tnum)
            tid1 = j[1].set_template(tnum)

            if (len(tid0) == 0) or (len(tid1) == 0):
                continue

            t0 = j[0]['end_time']
            t1 = j[1]['end_time']
            logging.info('Trigs for template %s, %s:%s %s:%s' % \
                (tnum, j[0].ifo, len(t0), j[1].ifo, len(t1)))

            i0, i1, slide = coinc.time_coincidence(t0, t1, time_window, args.timeslide_interval)

            logging.info('Coincident Trigs: %s' % (len(i1)))

            logging.info('Calculating Single Detector Statistic')
            s0, s1 = rank_method.single(j[0]), rank_method.single(j[1])

            logging.info('Calculating Multi-Detector Combined Statistic')
            c = rank_method.coinc(s0[i0], s1[i1], slide, args.timeslide_interval)

            #index values of the zerolag triggers
            fi = numpy.where(slide == 0)[0]

            #index values of the background triggers
            bi = numpy.where(slide != 0)[0]
            logging.info('%s foreground triggers' % len(fi))
            logging.info('%s background triggers' % len(bi))

            # We split the background triggers into two types which we keep track of
            # in "bh" (triggers which are *not* decimated, stored in full) and
            # "bl" (triggers which may not be stored in full, but we keep track of
            # how many are removed)
            # "bl_int" keeps track of which triggers are *not* in the bh set. Depending
            # on the decimation factor option we may not store any of these or we may
            # keep a fraction of them corresponding to a subset of the timeslides
            if args.loudest_keep:
                sep = len(bi) - args.loudest_keep
                sep = 0 if sep < 0 else sep

                bsort = numpy.argpartition(c[bi], sep)
                bl_int = bi[bsort[0:sep]]
                bh = bi[bsort[sep:]]
                del bsort
                del bi
            elif args.loudest_keep_value:
                bh = bi[c[bi] > args.loudest_keep_value]
                bl_int = bi[c[bi] <= args.loudest_keep_value]
            else:
                bh = bi

            if args.decimation_factor:
                bl = bl_int[slide[bl_int] % args.decimation_factor == 0]
            else:
                bl = []

            ti = numpy.concatenate([bl, bh, fi]).astype(numpy.uint32)
            logging.info('%s after decimation' % len(ti))

            g0 = i0[ti]
            g1 = i1[ti]
            del i0
            del i1

            data['stat'] += [c[ti]]
            dec_fac = numpy.repeat([args.decimation_factor, 1, 1],
                           [len(bl), len(bh), len(fi)]).astype(numpy.uint32)
            data['decimation_factor'] += [dec_fac]
            data['time1'] += [t0[g0]]
            data['time2'] += [t1[g1]]
            data['trigger_id1'] += [tid0[g0]]
            data['trigger_id2'] += [tid1[g1]]
            data['timeslide_id'] += [slide[ti]]
            data['template_id'] += [numpy.zeros(len(ti), dtype=numpy.uint32) + tnum]

        if len(data['stat']) > 0:
            for key in data:
                data[key] = numpy.concatenate(data[key])

        if args.cluster_window and len(data['stat']) > 0:
            cid = coinc.cluster_coincs(data['stat'], data['time1'], data['time2'],
                               data['timeslide_id'], args.timeslide_interval,
                               args.cluster_window)

        logging.info('saving coincident triggers')
        f = h5py.File(i + args.output_file, 'w')
        if len(data['stat']) > 0:
            for key in data:
                var = data[key][cid] if args.cluster_window else data[key]
                f.create_dataset(key, data=var,
                              compression='gzip',
                              compression_opts=9,
                              shuffle=True)

        f['segments/coinc/start'], f['segments/coinc/end'] = veto.segments_to_start_end(coinc_segs)

        for t in [j[0], j[1]]:
            f['segments/%s/start' % t.ifo], f['segments/%s/end' % t.ifo] = t.valid

        f.attrs['timeslide_interval'] = args.timeslide_interval
        f.attrs['detector_1'] = det0.name
        f.attrs['detector_2'] = det1.name
        f.attrs['foreground_time1'] = abs(j[0].segs)
        f.attrs['foreground_time2'] = abs(j[1].segs)
        f.attrs['coinc_time'] = abs(coinc_segs)

        if args.timeslide_interval:
            nslides = int(max(abs(j[0].segs), abs(j[1].segs)) / args.timeslide_interval)
        else:
            nslides = 0

        f.attrs['num_slides'] = nslides


elif len(args.trigger_files) == 3:
    coinc_segs = (trigs['trigs0'].segs & trigs['trigs1'].segs & trigs['trigs2'].segs).coalesce()
    if args.strict_coinc_time:
        for i in trigs:
            trigs[i].segs = coinc_segs
            trigs[i].valid = veto.segments_to_start_end(trigs[i].segs)
    # initialize a Stat class instance to calculate the coinc ranking statistic
    rank_method = stat.get_statistic(args.ranking_statistic)(args.statistic_files)
    if args.use_maxalpha:
        rank_method.use_alphamax()
    det0, det1, det2 = detector.Detector(trigs['trigs0'].ifo), detector.Detector(trigs['trigs1'].ifo), detector.Detector(trigs['trigs2'].ifo)
    time_window01 = det0.light_travel_time_to_detector(det1) + args.coinc_threshold
    time_window02 = det0.light_travel_time_to_detector(det2) + args.coinc_threshold

    if time_window01 >= args.timeslide_interval and time_window02 >= args.timeslide_interval and args.timeslide_interval is not None:
        raise parser.error("The maximum time delay between detectors should be smaller "
                       "than the timeslide interval.")
    # slide = 0 means don't do timeslides
    if args.timeslide_interval is None:
        args.timeslide_interval = 0

    logging.info('The coincidence window between detector 0 and 1 is %3.1f ms' % (time_window01 * 1000))
    logging.info('The coincidence window between detector 0 and 2  is %3.1f ms' % (time_window02 * 1000))

    data = {'stat':[], 'decimation_factor':[], 'time1':[], 'time2':[], 'time3':[],
        'trigger_id1':[], 'trigger_id2':[], 'trigger_id3':[], 'timeslide_id1':[], 'timeslide_id2':[], 'template_id':[]}

    for tnum in range(tmin, tmax):
        tid0 = trigs['trigs0'].set_template(tnum)
        tid1 = trigs['trigs1'].set_template(tnum)
        tid2 = trigs['trigs2'].set_template(tnum)

        if (len(tid0) == 0) or (len(tid1) == 0 or (len(tid2) == 0):
            continue

        t0 = trigs['trigs0']['end_time']
        t1 = trigs['trigs1']['end_time']
        t2 = trigs['trigs2']['end_time']
        logging.info('Trigs for template %s, %s:%s %s:%s %s:%s' % \
                (tnum, trigs['trigs0'].ifo, len(t0), trigs['trigs1'].ifo, len(t1), trigs['trigs2'].ifo, len(t2)))

        i0, i1, i2, slide1, slide2 = coinc.time_multiifo_coincidence(t0, t1, t2, time_window, args.timeslide_interval)

        logging.info('Coincident Trigs: %s' % (len(i2)))

        logging.info('Calculating Single Detector Statistic')
        s0, s1, s2 = rank_method.single(trigs['trigs0']), rank_method.single(trigs['trigs1']), rank_method.single(trigs['trigs2'])

        logging.info('Calculating Multi-Detector Combined Statistic')
        setofsingledetstat = [s0[i0], s1[i1],s2[i2]]
        c = rank_method.coinc_multiifo(slide, args.timeslide_interval, setofsingledetstat)

        #index values of the zerolag triggers
        fi = numpy.where(slide == 0)[0]

        #index values of the background triggers
        bi = numpy.where(slide != 0)[0]
        logging.info('%s foreground triggers' % len(fi))
        logging.info('%s background triggers' % len(bi))

        # We split the background triggers into two types which we keep track of
        # in "bh" (triggers which are *not* decimated, stored in full) and
        # "bl" (triggers which may not be stored in full, but we keep track of
        # how many are removed)
        # "bl_int" keeps track of which triggers are *not* in the bh set. Depending
        # on the decimation factor option we may not store any of these or we may
        # keep a fraction of them corresponding to a subset of the timeslides
        if args.loudest_keep:
            sep = len(bi) - args.loudest_keep
            sep = 0 if sep < 0 else sep

            bsort = numpy.argpartition(c[bi], sep)
            bl_int = bi[bsort[0:sep]]
            bh = bi[bsort[sep:]]
            del bsort
            del bi
        elif args.loudest_keep_value:
            bh = bi[c[bi] > args.loudest_keep_value]
            bl_int = bi[c[bi] <= args.loudest_keep_value]
        else:
            bh = bi

        if args.decimation_factor:
            bl = bl_int[slide[bl_int] % args.decimation_factor == 0]
        else:
            bl = []

        ti = numpy.concatenate([bl, bh, fi]).astype(numpy.uint32)
        logging.info('%s after decimation' % len(ti))

        g0 = i0[ti]
        g1 = i1[ti]
        g2 = i2[ti]
        del i0
        del i1
        del i2

        data['stat'] += [c[ti]]
        dec_fac = numpy.repeat([args.decimation_factor, 1, 1],
                           [len(bl), len(bh), len(fi)]).astype(numpy.uint32)
        data['decimation_factor'] += [dec_fac]
        data['time1'] += [t0[g0]]
        data['time2'] += [t1[g1]]
        data['time3'] += [t2[g2]]
        data['trigger_id1'] += [tid0[g0]]
        data['trigger_id2'] += [tid1[g1]]
        data['trigger_id3'] += [tid2[g2]]
        data['timeslide_id1'] += [slide1[ti]]
        data['timeslide_id2'] += [slide2[ti]]
        data['template_id'] += [numpy.zeros(len(ti), dtype=numpy.uint32) + tnum]

    if len(data['stat']) > 0:
        for key in data:
            data[key] = numpy.concatenate(data[key])

    if args.cluster_window and len(data['stat']) > 0:
        cid = coinc.cluster_coincs(data['stat'], data['time1'], data['time2'],
                               data['timeslide_id'], args.timeslide_interval,
                               args.cluster_window)

    logging.info('saving coincident triggers')
    f = h5py.File(args.output_file, 'w')
    if len(data['stat']) > 0:
        for key in data:
            var = data[key][cid] if args.cluster_window else data[key]
            f.create_dataset(key, data=var,
                              compression='gzip',
                              compression_opts=9,
                              shuffle=True)

    f['segments/coinc/start'], f['segments/coinc/end'] = veto.segments_to_start_end(coinc_segs)

    for t in [trigs['trigs0'], trigs['trigs1']]:
        f['segments/%s/start' % t.ifo], f['segments/%s/end' % t.ifo] = t.valid

    f.attrs['timeslide_interval'] = args.timeslide_interval
    f.attrs['detector_1'] = det0.name
    f.attrs['detector_2'] = det1.name
    f.attrs['detector_3'] = det2.name
    f.attrs['foreground_time1'] = abs(trigs['trigs0'].segs)
    f.attrs['foreground_time2'] = abs(trigs['trigs1'].segs)
    f.attrs['foreground_time3'] = abs(trigs['trigs2'].segs)
    f.attrs['coinc_time'] = abs(coinc_segs)

    if args.timeslide_interval:
        nslides = int(max(abs(trigs['trigs0'].segs), abs(trigs['trigs1'].segs), abs(trigs['trigs2'].segs)) / args.timeslide_interval)
    else:
        nslides = 0

    f.attrs['num_slides'] = nslides


logging.info('Done')
