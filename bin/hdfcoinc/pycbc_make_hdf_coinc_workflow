#!/usr/bin/env python

# Copyright (C) 2013 Ian W. Harry, Alex Nitz
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
Programe for running multi-detector workflow analysis through coincidence and then
generate post-processing and plots with pipedown.
"""
import pycbc
import pycbc.version
__author__  = "Ian Harry <ian.harry@astro.cf.ac.uk>"
__version__ = pycbc.version.git_verbose_msg
__date__    = pycbc.version.date
__program__ = "weekly_workflow"

import os, argparse, ConfigParser, logging
from pycbc import workflow as wf

logging.basicConfig(format='%(asctime)s:%(levelname)s : %(message)s', 
                    level=logging.INFO)

_desc = __doc__[1:]
parser = argparse.ArgumentParser(description=_desc)
parser.add_argument('--version', action='version', version=__version__)
parser.add_argument("-d", "--output-dir", default=None,
                    help="Path to output directory.")
wf.add_workflow_command_line_group(parser)
args = parser.parse_args()

workflow = wf.Workflow(args, 'weekly_ahope')

# Needed later for WIP
if args.output_dir:
    runDir = args.output_dir
else:
    baseDir = os.getcwd()
    runDir = os.path.join(baseDir, '%d-%d' %tuple(workflow.analysis_time))
if not os.path.exists(runDir):
    os.makedirs(runDir)
os.chdir(runDir)

currDir = os.getcwd()

segDir = os.path.join(currDir, "segments")
bankDir = os.path.join(currDir, "bank")
fdDir = os.path.join(currDir, "full_data")
dfDir = os.path.join(currDir, "datafind")
injDir = os.path.join(currDir, "inj_files")
flDir = os.path.join(currDir, "file_lists")

# Get segments and find where the data is
science_segs, science_seg_file = wf.get_analyzable_segments(workflow, segDir)
datafind_files, science_segs = wf.setup_datafind_workflow(workflow, 
                                         science_segs, dfDir, science_seg_file)

cum_veto_files = wf.get_cumulative_veto_group_files(workflow, 'segments-veto-groups', segDir)
final_veto_file = wf.get_cumulative_veto_group_files(workflow, 'segments-final-veto-group', segDir)

# Template bank stuff
bank_files = wf.setup_tmpltbank_workflow(workflow, science_segs, 
                                            datafind_files, dfDir)
                                            
hdfbank = wf.convert_bank_to_hdf(workflow, bank_files, dfDir)
                                       
splitbank_files = wf.setup_splittable_workflow(workflow, bank_files, dfDir) 


# setup the injection files
inj_files, inj_tags = wf.setup_injection_workflow(workflow, 
                                                     output_dir=injDir)
                                                     
bg_file = None                                                                                     
tags = ["full_data"] + inj_tags
output_dirs = [fdDir]
inj_coincs = wf.FileList()
output_dirs.extend([os.path.join(currDir, tag) for tag in inj_tags])
for inj_file, tag, output_dir in zip([None]+inj_files, tags, output_dirs):

    if tag == 'full_data':
        ctags = [tag, 'full']
    else:
        ctags = [tag, 'inj']
        output_dir += '_coinc'

    # setup the matchedfilter jobs                                                     
    insps = wf.setup_matchedfltr_workflow(workflow, science_segs, 
                                       datafind_files, splitbank_files, 
                                       output_dir, injection_file=inj_file,
                                       tags = [tag])

    # convert insps to hdf
    insps_hdf = wf.convert_trig_to_hdf(workflow, hdfbank, insps, output_dir, tags=[tag])
    insps_hdf = wf.merge_single_detector_hdf_files(workflow, insps_hdf, output_dir, tags=[tag])

    # setup coinc for the filtering jobs
    if tag == 'full_data':
        bg_files = wf.setup_interval_coinc(workflow, hdfbank, insps_hdf, cum_veto_files,
                                       output_dir, tags=ctags)  
        final_bg_file =  wf.setup_interval_coinc(workflow, hdfbank, insps_hdf, final_veto_file,
                                       output_dir, tags=ctags)   
        wf.make_foreground_table(workflow, final_bg_file[0], hdfbank[0], tag, 'plots/foreground')
    else:
        inj_coinc = wf.setup_interval_coinc_inj(workflow, hdfbank, 
                                                insps_hdf, final_bg_file,
                                                output_dir, tags = ctags)
        found_inj = wf.find_injections_in_hdf_coinc(workflow, wf.FileList([inj_coinc]),
                                                wf.FileList([inj_file]), final_veto_file[0], 
                                                output_dir, tags=ctags)
        inj_coincs += [inj_coinc]                      
        wf.make_sensitivity_plot(workflow, found_inj, 'plots/sensitivity',
                                 tags=ctags)
        wf.make_foundmissed_plot(workflow, found_inj, tag, 'plots/foundmissed', 
                                 tags=[tag])


found_inj = wf.find_injections_in_hdf_coinc(workflow, inj_coincs,
                                            inj_files, final_veto_file[0], 
                                            output_dir, tags=['ALLINJ'])                                                
wf.make_sensitivity_plot(workflow, found_inj, 'plots/sensitivity', tags=['ALLINJ'])

workflow.save()
logging.info("Written dax.")
