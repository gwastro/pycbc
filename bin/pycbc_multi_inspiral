#!/usr/bin/env /usr/bin/python

# Copyright (C) 2014 Alex Nitz
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

import logging
from collections import defaultdict
import argparse
from pycbc import vetoes, psd, waveform, events, strain, scheme, fft, DYN_RANGE_FAC
from pycbc.filter import MatchedFilterControl
from pycbc.types import TimeSeries, zeros, float32, complex64
from pycbc.types import MultiDetOptionAction
import pycbc.detector
import numpy as np

parser = argparse.ArgumentParser(usage='',
    description="Find multiple detector gravitational-wave triggers.")
parser.add_argument("-V", "--verbose", action="store_true",
                  help="print extra debugging information", default=False )
parser.add_argument("--output", type=str)
parser.add_argument("--instruments", nargs="+", type=str, required=True,
                    help="List of instruments to analyze.")
parser.add_argument("--bank-file", type=str)
parser.add_argument("--snr-threshold",
                  help="SNR threshold for trigger generation", type=float)
parser.add_argument("--newsnr-threshold", type=float, metavar='THRESHOLD',
                    help="Cut triggers with NewSNR less than THRESHOLD")
parser.add_argument("--low-frequency-cutoff", type=float,
                  help="The low frequency cutoff to use for filtering (Hz)")

# add approximant arg
waveform.bank.add_approximant_arg(parser)
parser.add_argument("--order", type=str,
                  help="The integer half-PN order at which to generate"
                       " the approximant.")
taper_choices = ["start","end","startend"]
parser.add_argument("--taper-template", choices=taper_choices,
                  help="For time-domain approximants, taper the start and/or "
                       "end of the waveform before FFTing.")
parser.add_argument("--cluster-method", choices=["template", "window"])
parser.add_argument("--cluster-window", type=float, default = -1,
                  help="Length of clustering window in seconds")

parser.add_argument("--bank-veto-bank-file", type=str)

parser.add_argument("--chisq-bins", default=0)
parser.add_argument("--chisq-threshold", type=float, default=0)
parser.add_argument("--chisq-delta", type=float, default=0)

parser.add_argument("--autochi-number-points", type=int, default=0)
parser.add_argument("--autochi-stride", type=int, default=0)
parser.add_argument("--autochi-onesided", action='store_true', default=False)
parser.add_argument("--downsample-factor", type=int, default=1,
                    help="Factor that determines the interval between the "
                         "initial SNR sampling. If not set (or 1) no sparse "
                         "sample is created, and the standard full SNR is "
                         "calculated.")
parser.add_argument("--upsample-threshold", type=float,
                    help="The fraction of the SNR threshold to check the "
                         "sparse SNR sample.")
parser.add_argument("--upsample-method", choices=["pruned_fft"],
                    default='pruned_fft',
                    help="The method to find the SNR points between the "
                         "sparse SNR sample.")
parser.add_argument("--user-tag", type=str, metavar="TAG", help="""
                    This is used to identify FULL_DATA jobs for 
                    compatibility with pipedown post-processing. 
                    Option will be removed when no longer needed.""")


# Add options groups
strain.insert_strain_option_group_multi_ifo(parser)
strain.StrainSegments.insert_segment_option_group_multi_ifo(parser)
psd.insert_psd_option_group_multi_ifo(parser)
scheme.insert_processing_option_group(parser)
fft.insert_fft_option_group(parser)

"""
For GRB we have sky position
"""
longitude = 6.103652
latitude = 0.9999715 
pol = 1.21771
t_gps = 1169088610.0

ra = longitude 
dec = latitude

opt = parser.parse_args()

strain.verify_strain_options_multi_ifo(opt, parser, opt.instruments)
strain.StrainSegments.verify_segment_options_multi_ifo(opt, parser,
                                                       opt.instruments)
psd.verify_psd_options_multi_ifo(opt, parser, opt.instruments)
scheme.verify_processing_options(opt, parser)
fft.verify_fft_options(opt,parser)

if opt.verbose:
    log_level = logging.DEBUG
else:
    log_level = logging.WARN
logging.basicConfig(format='%(asctime)s : %(message)s', level=log_level)

ctx = scheme.from_cli(opt)

def dominant_polarization(f_sig):
    """
    Given the detector responses, compute the dominant polarization F+, Fx and the
    angle that we have to rotate through to get to D.P.
    :param f_sig: sensitivity of detectors: sigma * (F+, Fx)
    """
    nifo = len(f_sig.keys())
    m = np.zeros((2, 2))
    for ifo in f_sig.keys():
        m += np.outer(f_sig[ifo], f_sig[ifo])
    f_cross, f_plus = np.sort(np.sqrt(np.linalg.eig(m)[0])) #This returns eigenvalues of m 
    chi = 1./4 * np.arctan2(2 * m[0,1], m[0,0] - m[1,1])
    return f_plus, f_cross, chi

strain_dict = strain.from_cli_multi_ifos(opt, opt.instruments, 
                                         dyn_range_fac=DYN_RANGE_FAC)
strain_segments_dict = strain.StrainSegments.from_cli_multi_ifos(opt, 
  				         strain_dict, opt.instruments)
with ctx:
    fft.from_cli(opt)

    # Set some often used variables for easy access
    flow = opt.low_frequency_cutoff
    flow_dict = defaultdict(lambda : flow)
    for count, ifo in enumerate(opt.instruments):
	if count == 0:
	    sample_rate = strain_dict[ifo].sample_rate
	    sample_rate_dict = defaultdict(lambda : sample_rate)
	    flen = strain_segments_dict[ifo].freq_len
	    flen_dict = defaultdict(lambda : flen)
	    tlen = strain_segments_dict[ifo].time_len
	    tlen_dict = defaultdict(lambda : tlen)
	    delta_f = strain_segments_dict[ifo].delta_f
	    delta_f_dict = defaultdict(lambda : delta_f)            
	else:
	    try:
		assert(sample_rate == strain_dict[ifo].sample_rate)
		assert(flen == strain_segments_dict[ifo].freq_len)
		assert(tlen == strain_segments_dict[ifo].time_len)
		assert(delta_f == strain_segments_dict[ifo].delta_f)
	    except:
		err_msg = "Sample rate, frequency length and time length "
		err_msg += "must all be consistent across ifos."
		raise ValueError(err_msg)

    logging.info("Making frequency-domain data segments")
    segments = {}
    for ifo in opt.instruments:
	segments[ifo] = strain_segments_dict[ifo].fourier_segments()
	del strain_segments_dict[ifo]

    logging.info("Computing noise PSD")
    psds = psd.from_cli_multi_ifos(opt, flen_dict, delta_f_dict, flow_dict,
				  opt.instruments, strain_dict=strain_dict, 
				  dyn_range_factor=DYN_RANGE_FAC)
    for ifo in opt.instruments:
	psds[ifo] = psds[ifo].astype(float32)

    psd.associate_psds_to_multi_ifo_segments(opt, segments, strain_dict, flen,
	    delta_f, flow, opt.instruments, dyn_range_factor=DYN_RANGE_FAC,
	    precision='single')

    # Currently we are using the same matched-filter parameters for all ifos.
    # Therefore only one MatchedFilterControl needed. Maybe this can change if
    # needed. Segments is only used to get tlen etc. which is same for all
    # ifos, so just send the first ifo
    template_mem = zeros(tlen, dtype=complex64)
    if opt.cluster_window == 0:
	use_cluster = False
    else:
	use_cluster = True
    
    matched_filter = {}
    time_delay = {}
    time_delay_idx = {}
    for ifo in opt.instruments:
	time_delay[ifo] =  \
	pycbc.detector.Detector(ifo).time_delay_from_earth_center(ra,dec,t_gps)
	time_delay_idx[ifo] = int(round(time_delay[ifo]*sample_rate))
	matched_filter[ifo] = MatchedFilterControl(opt.low_frequency_cutoff, None,
				   opt.snr_threshold, tlen, delta_f, complex64,
				   segments[ifo], template_mem,
				   use_cluster,
				   downsample_factor=opt.downsample_factor,
				   upsample_threshold=opt.upsample_threshold,
				   upsample_method=opt.upsample_method,
				   cluster_function='symmetric')

    logging.info("Initializing signal-based vetoes.")
    # The existing SingleDetPowerChisq can calculate the single detector
    # chisq for multiple ifos, so just use that directly.
    power_chisq = vetoes.SingleDetPowerChisq(opt.chisq_bins)
    # The existing SingleDetBankVeto can calculate the single detector
    # bank veto for multiple ifos, so we just use it directly.
    bank_chisq = vetoes.SingleDetBankVeto(opt.bank_veto_bank_file, flen,
					  delta_f, flow, complex64,
					  phase_order=opt.order,
					  approximant=opt.approximant)
    # Same here
    autochisq = vetoes.SingleDetAutoChisq(opt.autochi_stride,
					 opt.autochi_number_points,
					 onesided=opt.autochi_onesided)

    logging.info("Overwhitening frequency-domain data segments")
    for ifo in opt.instruments:
	for seg in segments[ifo]:
	    seg /= psds[ifo]

    out_types = {
	'time_index'     : int,
	'ifo'            : int, # IFO is stored as an int internally!
	'snr'            : complex64,
	'chisq'          : float32,
	'chisq_dof'      : int,
	'bank_chisq'     : float32,
	'bank_chisq_dof' : int,
	'cont_chisq'     : float32,
		}
    out_vals = {
	'time_index'     : None,
	'ifo'            : None,
	'snr'            : None,
	'chisq'          : None,
	'chisq_dof'      : None,
	'bank_chisq'     : None,
	'bank_chisq_dof' : None,
	'cont_chisq'     : None,
	       }
    names = sorted(out_vals.keys())

    event_mgr = events.EventManagerMultiDet(opt, opt.instruments, names,
					[out_types[n] for n in names], psd=psds)

    logging.info("Read in template bank")
    bank = waveform.FilterBank(opt.bank_file, flen, delta_f, complex64,
			       low_frequency_cutoff=flow, phase_order=opt.order,
			       taper=opt.taper_template,
			       approximant=opt.approximant, out=template_mem)
    roh_coh_dict = {}
    roh_coinc_dict={}
    for t_num, template in enumerate(bank):
	if t_num == 5: #use small sample set for development
	   break
	"""
	Calculating f_p and f_c for each detector for use later in calculating the coherent snr
	"""
	roh_coh_dict[t_num]={}
	roh_coinc_dict[t_num]={}
	Fp = {} #Antenna patterns
	Fc = {}
	sigmasq = {}
	sigma = {}
	f_sig = {}
	for ifo in opt.instruments:
	    Fp[ifo], Fc[ifo] = pycbc.detector.Detector(ifo).antenna_pattern(ra, dec, pol, t_gps)
	    sigmasq[ifo] = template.sigmasq(psds[ifo])
	    sigma[ifo] = np.sqrt(sigmasq[ifo])
	    f_sig[ifo] = sigma[ifo]*np.array([Fp[ifo],Fc[ifo]])
	chi_dp = dominant_polarization(f_sig)[2] #Dominant polarisation angle
	#Change to dominant polarisation frame
	Fp_dp={}
	Fc_dp={}
	for ifo in opt.instruments:
	    Fp_dp[ifo] = Fp[ifo]*np.cos(2*chi_dp) + Fc[ifo]*np.sin(2*chi_dp)       
	    Fc_dp[ifo] = -Fp[ifo]*np.sin(2*chi_dp) + Fc[ifo]*np.cos(2*chi_dp)       
	#define the orthogonal unit vectors
	fp_denominator = 0
	fc_denominator = 0
	for ifo in opt.instruments:
	    fp_denominator += (sigma[ifo]*Fp_dp[ifo])**2 
	    fc_denominator += (sigma[ifo]*Fc_dp[ifo])**2 
	fp_denominator = np.sqrt(fp_denominator) 
	fc_denominator = np.sqrt(fc_denominator) 
	fp = {}
	fc = {}
	for ifo in opt.instruments:
	    fp[ifo] = sigma[ifo]*Fp_dp[ifo]/fp_denominator
	    fc[ifo] = sigma[ifo]*Fc_dp[ifo]/fc_denominator

	event_mgr.new_template(tmplt=template.params, sigmasq=sigmasq)

	for ifo in opt.instruments:
	    if opt.cluster_method == "window":
		cluster_window = int(opt.cluster_window * sample_rate)
	    elif opt.cluster_method == "template":
		cluster_window = int(template.chirp_length * sample_rate)
	    elif opt.cluster_window == 0:
		 cluster_window = int(0)

	s = range(len(segments[opt.instruments[0]])) #find number of segments (same in both ifos)
	for s_num in s: #for each segment loop over the ifos
	    if s_num==5:
		break
	    snr={} 
	    norm_dict={}
	    snrv_dict={}
	    for ifo in opt.instruments:
		stilde = segments[ifo][s_num]
		logging.info("Filtering template %d/%d ifo %s segment %d/%d" % \
			     (t_num + 1, len(bank), ifo, s_num + 1,
			      len(segments[ifo])))       
		snr_ts, norm, corr, idx, snrv = \
			matched_filter[ifo].matched_filter_and_cluster(s_num,
			    template.sigmasq(stilde.psd), window=cluster_window)
                #Find the data we need to analyse
		analyse_data = matched_filter[ifo].segments[s_num].analyze
                #Restrict the snr timeseries to just this section
		snr[ifo] = snr_ts[analyse_data]
		norm_dict[ifo] = norm
                if snr[ifo] != []:
		    print ifo, t_num, s_num
		    print "maxsnr="
		    print (max(snr[ifo]*snr[ifo].conj())*norm_dict[ifo]**2)**0.5
                if idx != []:
		    print "maxsnrv="
		    print (max(snrv*snrv.conj())*norm**2)**0.5
		    if max(snr[ifo]*snr[ifo].conj())*norm_dict[ifo]**2 !=\
		                            max(snrv*snrv.conj())*norm**2:
                        print "The loudest trigger should equal the loudest \
                               part of the snr timeseries"
            for ifo in opt.instruments:
                # Move to next segment if this segment has no data
                if snr[ifo] == []:
                    break
                #Apply time delay
		snr[ifo].roll(-time_delay_idx[ifo])
                #Make dictionary of normalisation constants for each ifo
                """
		if len(idx) == 0:
		    continue
                """
		out_vals['bank_chisq'], out_vals['bank_chisq_dof'] = \
			bank_chisq.values(template, stilde.psd, stilde, snrv,
					  norm, idx+stilde.analyze.start)

		out_vals['chisq'], out_vals['chisq_dof'] =\
			power_chisq.values(corr, snrv, norm, stilde.psd,
					   idx+stilde.analyze.start, template)

		out_vals['cont_chisq'] = autochisq.values(snr,
			idx+stilde.analyze.start, template, stilde.psd, norm,
			stilde=stilde, low_frequency_cutoff=flow)

		idx += stilde.cumulative_index

		out_vals['time_index'] = idx
		out_vals['snr'] = snrv * norm
		snrv_dict[ifo]=snrv*norm
		# IFO is stored as an int
		out_vals['ifo'] = event_mgr.ifo_dict[ifo]

		event_mgr.add_template_events_to_ifo(ifo, names,
			[out_vals[n] for n in names])


	    #Calculate coherent snr roh_coh
	    #Check we have data before we try to compute the coherent snr
            if snr[ifo] != []:
		for ifo in opt.instruments:
		    snr[ifo]*=norm_dict[ifo]
		roh_coh2=0
		roh_coinc2=0
		for ifo1 in opt.instruments:
		    for ifo2 in opt.instruments:
			roh_coh2 += snr[ifo1].conj()*(fp[ifo1]*fp[ifo2]+fc[ifo1]*fc[ifo2])*snr[ifo2]
		    roh_coinc2 += snr[ifo1].conj()*snr[ifo1]
		roh_coh_dict[t_num][s_num]=np.sqrt(roh_coh2)
		roh_coinc_dict[t_num][s_num]=np.sqrt(roh_coinc2)
                print "max coherent snr"
                print max(roh_coh_dict[t_num][s_num])
                print "max coincident snr"
                print max(roh_coinc_dict[t_num][s_num])
	    event_mgr.cluster_template_events_single_ifo("time_index",
						    "snr", cluster_window, ifo)


"""
#FIXME This stuff doesn't work for more than 2 detectors.
	event_mgr.finalize_template_events()
logging.info("Found %s triggers" % str(len(event_mgr.events)))
if opt.chisq_threshold and opt.chisq_bins:
    logging.info("Removing triggers with poor chisq")
    event_mgr.chisq_threshold(opt.chisq_threshold, opt.chisq_bins,
				   opt.chisq_delta)
    logging.info("%d remaining triggers" % len(event_mgr.events))

if opt.newsnr_threshold and opt.chisq_bins:
    logging.info("Removing triggers with NewSNR below threshold")
    event_mgr.newsnr_threshold(opt.newsnr_threshold)
    logging.info("%d remaining triggers" % len(event_mgr.events))
logging.info("Writing out triggers")
event_mgr.write_events(opt.output)

if opt.fftw_output_float_wisdom_file:
    fft.fftw.export_single_wisdom_to_filename(opt.fftw_output_float_wisdom_file)

if opt.fftw_output_double_wisdom_file:
    fft.fftw.export_double_wisdom_to_filename(opt.fftw_output_double_wisdom_file)
"""
logging.info("Finished")


import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
plt.figure()
plt.plot(roh_coinc_dict[0][0],'b.',label='coinc snr')
plt.plot(roh_coh_dict[0][0],'rx',label='coh snr')
plt.legend()
plt.savefig("/home/c1551011/public_html/coh_snr_vs_coinc_snr.png")

plt.figure()
"""
for ifo in opt.instruments:
    print "Time delay for ifo:" + ifo
    print time_delay[ifo]
for ifo in opt.instruments:
    print "Time difference to align peaks for ifo: " + ifo
    print np.argmax((snr[ifo]*snr[ifo].conj())[20400:20500])/float(sample_rate)
for ifo in opt.instruments:
    print "time difference for ifo: " + ifo
    print np.argmax((snr[ifo]*snr[ifo].conj())[20400:20500])/float(sample_rate) - time_delay[ifo]
"""
import lal
for ifo in opt.instruments:
    single_det_snr = snr[ifo]*snr[ifo].conj()**0.5
    plt.plot(single_det_snr[20400:20500],'.',label=ifo)
    plt.plot(np.argmax(single_det_snr[20400:20500]),max(single_det_snr[20400:20500]),'*',label=ifo)
plt.legend()
plt.savefig("/home/c1551011/public_html/single_det_snr.png")
print t_gps
time_delay_between_ifos = {}
time_delay_between_peaks = {}
for ifo1 in opt.instruments:
    time_delay_between_ifos[ifo1] ={} 
    time_delay_between_peaks[ifo1] ={} 
    for ifo2 in opt.instruments:
        print "Time delay between detectors: " + ifo1 + " " + ifo2
        time_delay_between_ifos[ifo1][ifo2] = pycbc.detector.Detector(ifo1).time_delay_from_detector(pycbc.detector.Detector(ifo2),ra,dec,t_gps)
        print time_delay_between_ifos[ifo1][ifo2]
	single_det_snr1 = snr[ifo1]*snr[ifo1].conj()**0.5
	single_det_snr2 = snr[ifo2]*snr[ifo2].conj()**0.5
	print "Time delay between peaks for detectors: " + ifo1 + " " + ifo2
	time_delay_between_peaks[ifo1][ifo2] = float(np.argmax(single_det_snr1[20400:20500]) - np.argmax(single_det_snr2[20400:20500]))/sample_rate
        print time_delay_between_peaks[ifo1][ifo2]
        print np.argmax(single_det_snr1[20400:20500]) - np.argmax(single_det_snr2[20400:20500])
peak_time = {}
for ifo in opt.instruments:
    peak_time[ifo] = float(np.argmax(snr[ifo]*snr[ifo].conj()**0.5))/sample_rate
    print "Time of peak in ifo: " + ifo
    print peak_time[ifo]


