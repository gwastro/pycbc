#!/usr/bin/env /usr/bin/python

# Copyright (C) 2014 Alex Nitz
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

import time

time_init = time.time()

import logging
from collections import defaultdict
import argparse
from pycbc import vetoes, psd, waveform, events, strain, scheme, fft, DYN_RANGE_FAC
from pycbc.filter import MatchedFilterControl
from pycbc.types import TimeSeries, zeros, float32, complex64
from pycbc.types import MultiDetOptionAction
import pycbc.detector
import numpy as np

parser = argparse.ArgumentParser(usage='',
    description="Find multiple detector gravitational-wave triggers.")
parser.add_argument("-V", "--verbose", action="store_true",
                  help="print extra debugging information", default=False )
parser.add_argument("--output", type=str)
parser.add_argument("--instruments", nargs="+", type=str, required=True,
                    help="List of instruments to analyze.")
parser.add_argument("--bank-file", type=str)
parser.add_argument("--snr-threshold",
                  help="SNR threshold for trigger generation", type=float)
parser.add_argument("--newsnr-threshold", type=float, metavar='THRESHOLD',
                    help="Cut triggers with NewSNR less than THRESHOLD")
parser.add_argument("--low-frequency-cutoff", type=float,
                  help="The low frequency cutoff to use for filtering (Hz)")

# add approximant arg
waveform.bank.add_approximant_arg(parser)
parser.add_argument("--order", type=str,
                  help="The integer half-PN order at which to generate"
                       " the approximant.")
taper_choices = ["start","end","startend"]
parser.add_argument("--taper-template", choices=taper_choices,
                  help="For time-domain approximants, taper the start and/or "
                       "end of the waveform before FFTing.")
parser.add_argument("--cluster-method", choices=["template", "window"])
parser.add_argument("--cluster-window", type=float, default = -1,
                  help="Length of clustering window in seconds")

parser.add_argument("--bank-veto-bank-file", type=str)

parser.add_argument("--chisq-bins", default=0)
parser.add_argument("--chisq-threshold", type=float, default=0)
parser.add_argument("--chisq-delta", type=float, default=0)

parser.add_argument("--autochi-number-points", type=int, default=0)
parser.add_argument("--autochi-stride", type=int, default=0)
parser.add_argument("--autochi-onesided", action='store_true', default=False)
parser.add_argument("--downsample-factor", type=int, default=1,
                    help="Factor that determines the interval between the "
                         "initial SNR sampling. If not set (or 1) no sparse "
                         "sample is created, and the standard full SNR is "
                         "calculated.")
parser.add_argument("--upsample-threshold", type=float,
                    help="The fraction of the SNR threshold to check the "
                         "sparse SNR sample.")
parser.add_argument("--upsample-method", choices=["pruned_fft"],
                    default='pruned_fft',
                    help="The method to find the SNR points between the "
                         "sparse SNR sample.")
parser.add_argument("--user-tag", type=str, metavar="TAG", help="""
                    This is used to identify FULL_DATA jobs for 
                    compatibility with pipedown post-processing. 
                    Option will be removed when no longer needed.""")


# Add options groups
strain.insert_strain_option_group_multi_ifo(parser)
strain.StrainSegments.insert_segment_option_group_multi_ifo(parser)
psd.insert_psd_option_group_multi_ifo(parser)
scheme.insert_processing_option_group(parser)
fft.insert_fft_option_group(parser)

"""
These need to be added as arguments 
"""
longitude = 6.103652
latitude = 0.9999715
t_gps = 1169088610.0
pol = 1.21771
coinc_threshold = 11.0
generate_histograms = True
generate_snr_plots = True
"""
pol = 1.21771
longitude = 197.448775
latitude = -23.38383056
t_gps = 1187002884 - 100
"""


ra = longitude 
dec = latitude

opt = parser.parse_args()

strain.verify_strain_options_multi_ifo(opt, parser, opt.instruments)
strain.StrainSegments.verify_segment_options_multi_ifo(opt, parser,
                                                       opt.instruments)
psd.verify_psd_options_multi_ifo(opt, parser, opt.instruments)
scheme.verify_processing_options(opt, parser)
fft.verify_fft_options(opt,parser)

if opt.verbose:
    log_level = logging.DEBUG
else:
    log_level = logging.WARN
logging.basicConfig(format='%(asctime)s : %(message)s', level=log_level)

ctx = scheme.from_cli(opt)


def get_weighted_antenna_patterns(Fp_dict,Fc_dict,sigma_dict):
    """
    Output: wp: 1 x nifo of the weighted antenna response fuctions to plus polarisation\
                 for each ifo
            wc: 1 x nifo of the weighted antenna response fuctions to cross polarisation\
                 for each ifo
    Input:  Fp_dict: Dictionary of the antenna response fuctions to plus polarisation\
                     for each ifo
            Fc_dict: Dictionary of the antenna response fuctions to cross polarisation\
                     for each ifo
           sigma_dict: Sigma dictionary for each ifo (sensitivity of each ifo)
    """
    wp = np.zeros(len(opt.instruments))
    wc = np.zeros(len(opt.instruments))
    for i,ifo in enumerate(opt.instruments):
        wp[i] = sigma_dict[ifo]*Fp_dict[ifo]
        wc[i] = sigma_dict[ifo]*Fc_dict[ifo]
    return wp,wc

def get_projection_matrix(wp,wc):
    """
    Output: projection_matrix: Projects the data onto the signal space
    Input:  wp,wc: The weighted antenna response fuctions to plus and cross\
                   polarisations respectively
    """
    denominator = np.dot(wp,wp) * np.dot(wc,wc) - np.dot(wp,wc)**2
    projection_matrix = (np.dot(wc,wc)*np.outer(wp,wp)+ np.dot(wp,wp)*np.outer(wc,wc)\
                    - np.dot(wp,wc)*(np.outer(wp,wc) + np.outer(wc,wp))) / denominator
    return projection_matrix

def coherent_snr_for_timeseries(snr_dict,projection_matrix):
    """
    Output: Returns an array of the coherent snr for the detector network
    Inputs: snr_dict: is a dictionary of the normalised complex snr time series for 
                      each ifo. The keys are the ifos (e.g. 'L1','H1', and 'V1')
            projection_matrix 
    """
    snr_array = np.array([snr_dict[ifo].numpy() for ifo in opt.instruments])
    x = np.inner(snr_array.conj().transpose(),projection_matrix)
    roh_coh2 = sum(x.transpose()*snr_array)
    roh_coh = np.sqrt(roh_coh2)
    return roh_coh


def coherent_snr(coinc_triggers_dict,projection_matrix):
    """
    Output: Returns an array of the coherent snr for the detector network
    Inputs: coinc_triggers_dict: is a dictionary of the normalised complex snr time series for 
                      each ifo. The keys are the ifos (e.g. 'L1','H1', and 'V1')
            projection_matrix 
    """
    snr_array = np.array([coinc_triggers_dict[ifo] for ifo in opt.instruments])
    x = np.inner(snr_array.conj().transpose(),projection_matrix)
    roh_coh2 = sum(x.transpose()*snr_array)
    roh_coh = np.sqrt(roh_coh2)
    return roh_coh

def coincident_snr(coinc_triggers_dict):
    snr_array = np.array([coinc_triggers_dict[ifo] for ifo in opt.instruments])
    roh_coinc = np.sqrt(np.sum(snr_array*snr_array.conj(),axis=0))
    return roh_coinc

def null_snr(roh_coh,roh_coinc):
    null2 = roh_coinc**2 - roh_coh**2
    #Numerical errors may make this negative and break the sqrt
    null2[null2<0] = 0 
    null_snr = null2**0.5
    return null_snr

def get_coinc_indexes(idx_dict):
    """
    Output: coinc_idx: list of indexes for triggers in geocent time that\
                       appear in multiple detectors 
    Input: idx_dict: Dictionary of indexes of triggers above threshold in each detector
    """
    coinc_list = np.array([],dtype=int)
    for ifo in opt.instruments:
        #Create list of indexes above threshold in single detector in 
        #geocent time. Can then search for triggers that appear in multiple 
        #detectors later.
        if len(idx_dict[ifo]) != 0:
             coinc_list = np.hstack([coinc_list,idx_dict[ifo] - time_delay_idx[ifo]])
    #Search through coinc_idx for repeated indexes. These must have
    #been loud in at least 2 detectors.
    coinc_idx = np.unique(coinc_list,return_counts=True)[0]\
                   [np.unique(coinc_list,return_counts=True)[1]>1]
    return coinc_idx


strain_dict = strain.from_cli_multi_ifos(opt, opt.instruments, 
                                         dyn_range_fac=DYN_RANGE_FAC)
strain_segments_dict = strain.StrainSegments.from_cli_multi_ifos(opt, 
  				         strain_dict, opt.instruments)
with ctx:
    fft.from_cli(opt)

    # Set some often used variables for easy access
    flow = opt.low_frequency_cutoff
    flow_dict = defaultdict(lambda : flow)
    for count, ifo in enumerate(opt.instruments):
	if count == 0:
	    sample_rate = strain_dict[ifo].sample_rate
	    sample_rate_dict = defaultdict(lambda : sample_rate)
	    flen = strain_segments_dict[ifo].freq_len
	    flen_dict = defaultdict(lambda : flen)
	    tlen = strain_segments_dict[ifo].time_len
	    tlen_dict = defaultdict(lambda : tlen)
	    delta_f = strain_segments_dict[ifo].delta_f
	    delta_f_dict = defaultdict(lambda : delta_f)            
	else:
	    try:
		assert(sample_rate == strain_dict[ifo].sample_rate)
		assert(flen == strain_segments_dict[ifo].freq_len)
		assert(tlen == strain_segments_dict[ifo].time_len)
		assert(delta_f == strain_segments_dict[ifo].delta_f)
	    except:
		err_msg = "Sample rate, frequency length and time length "
		err_msg += "must all be consistent across ifos."
		raise ValueError(err_msg)

    logging.info("Making frequency-domain data segments")
    segments = {}
    for ifo in opt.instruments:
	segments[ifo] = strain_segments_dict[ifo].fourier_segments()
	del strain_segments_dict[ifo]

    logging.info("Computing noise PSD")
    psds = psd.from_cli_multi_ifos(opt, flen_dict, delta_f_dict, flow_dict,
				  opt.instruments, strain_dict=strain_dict, 
				  dyn_range_factor=DYN_RANGE_FAC)
    for ifo in opt.instruments:
	psds[ifo] = psds[ifo].astype(float32)

    psd.associate_psds_to_multi_ifo_segments(opt, segments, strain_dict, flen,
	    delta_f, flow, opt.instruments, dyn_range_factor=DYN_RANGE_FAC,
	    precision='single')

    # Currently we are using the same matched-filter parameters for all ifos.
    # Therefore only one MatchedFilterControl needed. Maybe this can change if
    # needed. Segments is only used to get tlen etc. which is same for all
    # ifos, so just send the first ifo
    template_mem = zeros(tlen, dtype=complex64)
    if opt.cluster_window == 0:
	use_cluster = False
    else:
	use_cluster = True
    
    #Calculate time delay to each detector
    time_delay_idx = {}
    for ifo in opt.instruments:
	time_delay_idx[ifo] =  int(round(pycbc.detector.Detector(ifo).\
             time_delay_from_earth_center(ra,dec,t_gps) * sample_rate))
    
    #Matched filter each ifo
    matched_filter = {}
    for ifo in opt.instruments:
	matched_filter[ifo] = MatchedFilterControl(opt.low_frequency_cutoff, None,
				   opt.snr_threshold, tlen, delta_f, complex64,
				   segments[ifo], template_mem,
				   use_cluster,
				   downsample_factor=opt.downsample_factor,
				   upsample_threshold=opt.upsample_threshold,
				   upsample_method=opt.upsample_method,
				   cluster_function='symmetric')

    logging.info("Initializing signal-based vetoes.")
    # The existing SingleDetPowerChisq can calculate the single detector
    # chisq for multiple ifos, so just use that directly.
    power_chisq = vetoes.SingleDetPowerChisq(opt.chisq_bins)
    # The existing SingleDetBankVeto can calculate the single detector
    # bank veto for multiple ifos, so we just use it directly.
    bank_chisq = vetoes.SingleDetBankVeto(opt.bank_veto_bank_file, flen,
					  delta_f, flow, complex64,
					  phase_order=opt.order,
					  approximant=opt.approximant)
    # Same here
    autochisq = vetoes.SingleDetAutoChisq(opt.autochi_stride,
					 opt.autochi_number_points,
					 onesided=opt.autochi_onesided)

    logging.info("Overwhitening frequency-domain data segments")
    for ifo in opt.instruments:
	for seg in segments[ifo]:
	    seg /= psds[ifo]

    out_types = {
	'time_index'     : int,
	'ifo'            : int, # IFO is stored as an int internally!
	'snr'            : complex64,
	'chisq'          : float32,
	'chisq_dof'      : int,
	'bank_chisq'     : float32,
	'bank_chisq_dof' : int,
	'cont_chisq'     : float32,
		}
    out_vals = {
	'time_index'     : None,
	'ifo'            : None,
	'snr'            : None,
	'chisq'          : None,
	'chisq_dof'      : None,
	'bank_chisq'     : None,
	'bank_chisq_dof' : None,
	'cont_chisq'     : None,
	       }
    names = sorted(out_vals.keys())

    event_mgr = events.EventManagerMultiDet(opt, opt.instruments, names,
					[out_types[n] for n in names], psd=psds)

    logging.info("Read in template bank")
    bank = waveform.FilterBank(opt.bank_file, flen, delta_f, complex64,
			       low_frequency_cutoff=flow, phase_order=opt.order,
			       taper=opt.taper_template,
			       approximant=opt.approximant, out=template_mem)
    roh_coh_dict = {}
    roh_coinc_dict={}
    null_dict = {}
    triggers = {}
    coinc_idx = {}
    Fp = {} #Antenna patterns
    Fc = {}
    for ifo in opt.instruments:
        Fp[ifo], Fc[ifo] = pycbc.detector.Detector(ifo).antenna_pattern(\
                                                     ra, dec, pol, t_gps)
    for t_num, template in enumerate(bank):
	if t_num == 1: #use small sample set for development
	   break

	roh_coh_dict[t_num] = {}
	roh_coinc_dict[t_num] = {}
        null_dict[t_num] = {}
        coinc_idx[t_num] = {}
        #Find detector sensitivities (sigma) and make array of normalised 
	sigmasq = {}
	sigma = {}
	for ifo in opt.instruments:
	    sigmasq[ifo] = template.sigmasq(psds[ifo])
	    sigma[ifo] = np.sqrt(sigmasq[ifo])
	event_mgr.new_template(tmplt=template.params, sigmasq=sigmasq)

	for ifo in opt.instruments:
	    if opt.cluster_method == "window":
		cluster_window = int(opt.cluster_window * sample_rate)
	    elif opt.cluster_method == "template":
		cluster_window = int(template.chirp_length * sample_rate)
	    elif opt.cluster_window == 0:
		 cluster_window = int(0)

	for s_num,stilde in enumerate(segments[opt.instruments[0]]): #Loop over segments
            
	    if s_num==1:
		break
            
	    snr={} 
	    idx_dict={}
            coinc_idx[t_num][s_num] = np.array([])
            coinc_triggers = {}
	    for ifo in opt.instruments:
		stilde = segments[ifo][s_num]
		logging.info("Filtering template %d/%d ifo %s segment %d/%d" % \
			     (t_num + 1, len(bank), ifo, s_num + 1,
			      len(segments[ifo])))       
		snr_ts, norm, corr, idx, snrv = \
			matched_filter[ifo].matched_filter_and_cluster(s_num,
			    template.sigmasq(stilde.psd), window=cluster_window)

                #Save the snr triggers and indexes (if we have any)
                #Even if we have none, need to keep an empty dictionary.
                idx_dict[ifo] = idx

                #If we have no data, move onto next ifo and note the
                # number of ifos with data that we have
                nifo = len(opt.instruments)
                if len(snr_ts) == 0:
                    snr[ifo] = []
                    nifo -= 1
                    print "nifo = " + str(nifo)
                    continue

                #Find the data we need to analyse
		analyse_data = matched_filter[ifo].segments[s_num].analyze

                #Restrict the snr timeseries to just this section
		snr[ifo] = snr_ts[analyse_data] * norm
               
            #Find triggers that are coincident (in geocent time) in multiple ifos
            coinc_idx[t_num][s_num] = get_coinc_indexes(idx_dict)
            
            for ifo in opt.instruments:
                # Move to next segment if this segment has no data 
                if snr[ifo] == []:
                    break
                #Apply time delay
		snr[ifo].roll(-time_delay_idx[ifo])
                #Restrict the snr timeseries to just the interesting points
                coinc_triggers[ifo] = snr[ifo][coinc_idx[t_num][s_num]]
                #Make dictionary of normalisation constants for each ifo
                """
		out_vals['bank_chisq'], out_vals['bank_chisq_dof'] = \
			bank_chisq.values(template, stilde.psd, stilde, snrv,
					  norm, idx+stilde.analyze.start)

		out_vals['chisq'], out_vals['chisq_dof'] =\
			power_chisq.values(corr, snrv, norm, stilde.psd,
					   idx+stilde.analyze.start, template)

		out_vals['cont_chisq'] = autochisq.values(snr,
			idx+stilde.analyze.start, template, stilde.psd, norm,
			stilde=stilde, low_frequency_cutoff=flow)

		idx += stilde.cumulative_index

		out_vals['time_index'] = idx
		out_vals['snr'] = snrv * norm
		snrv_dict[ifo]=snrv*norm
		# IFO is stored as an int
		out_vals['ifo'] = event_mgr.ifo_dict[ifo]

		event_mgr.add_template_events_to_ifo(ifo, names,
			[out_vals[n] for n in names])
                """

	    #Calculate coherent snr roh_coh
	    #Check we have data before we try to compute the coherent snr
            if len(coinc_idx[t_num][s_num]) != 0 and nifo > 1:
                roh_coinc_dict[t_num][s_num]=coincident_snr(coinc_triggers)
                print "max coincident snr"
                print max(roh_coinc_dict[t_num][s_num])
                #Get rid of coinc triggers not above coinc threshold
                coinc_idx[t_num][s_num] = coinc_idx[t_num][s_num][\
                    roh_coinc_dict[t_num][s_num] > coinc_threshold]
                for ifo in opt.instruments:
                    coinc_triggers[ifo] = snr[ifo][coinc_idx[t_num][s_num]]
            #If we have triggers above coinc threshold and more than 2 ifos 
            # then calculate the coherent statistics 
            if len(coinc_idx[t_num][s_num]) != 0 and nifo > 2:
                wp,wc=get_weighted_antenna_patterns(Fp,Fc,sigma)
                projection_matrix = get_projection_matrix(wp,wc)
                roh_coh_dict[t_num][s_num] = coherent_snr(\
                                 coinc_triggers,projection_matrix)
                print "max coherent snr"
                print max(roh_coh_dict[t_num][s_num])
                #Find the null snr
                null_dict[t_num][s_num] = null_snr(roh_coh_dict[t_num][s_num],\
                     roh_coinc_dict[t_num][s_num][roh_coinc_dict[t_num][s_num]\
                                                              >coinc_threshold])
                
	    event_mgr.cluster_template_events_single_ifo("time_index",
           					    "snr", cluster_window, ifo)


print "Time to complete analysis: " + str(time.time() - time_init)
"""
#FIXME This stuff doesn't work for more than 2 detectors.
event_mgr.finalize_template_events()
logging.info("Found %s triggers" % str(len(event_mgr.events)))
if opt.chisq_threshold and opt.chisq_bins:
    logging.info("Removing triggers with poor chisq")
    event_mgr.chisq_threshold(opt.chisq_threshold, opt.chisq_bins,opt.chisq_delta)
    logging.info("%d remaining triggers" % len(event_mgr.events))

if opt.newsnr_threshold and opt.chisq_bins:
    logging.info("Removing triggers with NewSNR below threshold")
    event_mgr.newsnr_threshold(opt.newsnr_threshold)
    logging.info("%d remaining triggers" % len(event_mgr.events))
logging.info("Writing out triggers")
event_mgr.write_events(opt.output)

if opt.fftw_output_float_wisdom_file:
    fft.fftw.export_single_wisdom_to_filename(opt.fftw_output_float_wisdom_file)

if opt.fftw_output_double_wisdom_file:
    fft.fftw.export_double_wisdom_to_filename(opt.fftw_output_double_wisdom_file)
"""
np.save('roh_coh.npy',roh_coh_dict)
np.save('roh_coinc.npy',roh_coinc_dict)

logging.info("Finished")


if generate_snr_plots == True or generate_histograms == True:
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt

if generate_snr_plots == True:
    print "Generating network SNR plots..."
    plt.figure()
    plt.plot(coinc_idx[0][0],roh_coinc_dict[0][0][roh_coinc_dict[0][0]\
                               >coinc_threshold],'b.',label='coinc snr')
    plt.plot(coinc_idx[0][0],roh_coh_dict[0][0],'rx',label='coh snr')
    plt.plot(coinc_idx[0][0],null_dict[0][0],'mx',label='null snr')
    #plt.xlim(0,409600)
    plt.legend(loc="best")
    plt.savefig("/home/c1551011/public_html/coh_snr_vs_coinc_snr.png")

    print "Generating network SNR plots..."
    plt.figure()
    coinc_snr = coincident_snr(snr)
    plt.plot(coinc_snr,'b.',label='coinc snr')
    roh_coh = coherent_snr(snr,projection_matrix)
    plt.plot(roh_coh,'rx',label='coh snr')
    plt.plot((coinc_snr**2-roh_coh**2)**0.5,'mx',label='null snr')
    #plt.xlim(0,409600)
    plt.legend(loc="best")
    plt.savefig("/home/c1551011/public_html/coh_snr_vs_coinc_snr_whole_segment.png")
    print "Done"



if generate_histograms == True:
    print "Generating Historgrams..."
    from scipy.stats import chi2
    plt.figure()
    x = np.linspace(0, 20, 100)
    df = 4
    y = chi2.pdf(x, df)
    plt.plot(x,y,'r')
    roh_coh_for_hist = coherent_snr(snr,projection_matrix)
    plt.hist(roh_coh_for_hist.real**2,bins=100,normed=True)
    plt.title("roh_coh2")
    plt.savefig("/home/c1551011/public_html/coh_snr2_histogram.png")
    for ifo in opt.instruments:
        plt.figure()
        x = np.linspace(0, 20, 100)
        df = 2
        y = chi2.pdf(x, df)
        plt.plot(x,y,'r')
        data = (snr[ifo].conj()*snr[ifo]).data
        plt.hist(data.real,bins=100,normed=True)
        plt.title(ifo + " snr2")
        plt.savefig("/home/c1551011/public_html/" + ifo + "_snr2_histogram.png")
    print "Done"
