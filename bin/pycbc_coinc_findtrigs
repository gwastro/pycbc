#!/bin/env  python
"""
This program find the foreground and background coincidence between single
detector gravitational-wave triggers, using  a dynamic  sampling  timeslide
method.
"""
import itertools, copy, numpy, logging, argparse, h5py, os
from pycbc.detector import Detector
import pycbc.events
from glue import segments

def get_statistic_class(option):
    if option == 'newsnr':
        return NewSNRStatistic()
    elif option == 'newsnr_volume':
        return NewSNRVolStatistic()
    elif option == 'newsnr_phasehl':
        return NewSNRPhaseStatistic()

class NewSNRStatistic(object):
    """ ABS Class to handle information about detection statistics
    """
    def single_read(self, f, l, r):
        """ Read in the single detector information and make a single detector
        statistic. Results can either be a single number or a record array.
        """
        x2_dof = 2 * f.attrs['chisq_dof'] - 2
        snr = pycbc.events.newsnr(f['snr'][l:r], f['chisq'][l:r] / x2_dof)
        return numpy.array(snr, ndmin=1)

    def coinc_statistic(self, s1, s2):
        """ Calculate teh coincidend statistic.
        """
        return (s1**2.0 + s2**2.0) ** 0.5
        
class NewSNRVolStatistic(object):
    """ ABS Class to handle information about detection statistics
    """    
    def single_read(self, f, l, r):
        """ Read in the single detector information and make a single detector
        statistic. Results can either be a single number or a record array.
        """
        x2_dof = 2 * f.attrs['chisq_dof'] - 2
        snr = pycbc.events.newsnr(f['snr'][l:r], f['chisq'][l:r] / x2_dof)
        snr = numpy.array(snr, ndmin=1)
        vol = numpy.array(f['sigmasq'][l:r] ** (3.0/2.0), ndmin=1)
        stat1 = numpy.zeros(len(snr), dtype=[('snr', numpy.float32), ('vol', numpy.float32)])
        stat1['vol'] = vol
        stat1['snr'] = snr
        return stat1
    
    def coinc_statistic(self, s1, s2):
        """ Calculate teh coincidend statistic.
        """
        snr1, snr2 = s1['snr'], s2['snr']
        vol1, vol2 = s1['vol'], s2['vol']
        stat = (snr1**2.0 + snr2**2.0) / 2.0 + numpy.log(vol1) + numpy.log(vol2)
        return stat**0.5
        
class NewSNRPhaseStatistic(object):
    """ ABS Class to handle information about detection statistics
    """
    def phase_log_pdf(self, p1, p2):
        pdiff = abs(p2 - p1) / numpy.pi - 1
        sigsq = (1.0 / numpy.pi) ** 2.0       
        
        # This ignores many factors that are simply constants in the loglikelihood
        logsig = -1.0 * (pdiff)**2.0 / 2.0 / sigsq
        lognoise = numpy.log(0.5 * (2 - pdiff))   
        return logsig - lognoise
        
    
    def single_read(self, f, l, r):
        """ Read in the single detector information and make a single detector
        statistic. Results can either be a single number or a record array.
        """
        x2_dof = 2 * f.attrs['chisq_dof'] - 2
        snr = pycbc.events.newsnr(f['snr'][l:r], f['chisq'][l:r] / x2_dof)
        snr = numpy.array(snr, ndmin=1)
        phase = numpy.array(f['coa_phase'][l:r], ndmin=1)
        stat1 = numpy.zeros(len(snr), dtype=[('snr', numpy.float32), ('phase', numpy.float32)])
        stat1['phase'] = phase
        stat1['snr'] = snr
        return stat1
    
    def coinc_statistic(self, s1, s2):
        """ Calculate teh coincidend statistic.
        """
        snr1, snr2 = s1['snr'], s2['snr']
        p1, p2 = s1['phase'], s2['phase']
        stat = (snr1**2.0 + snr2**2.0) / 2.0 + self.phase_log_pdf(p1, p2)
        return stat**0.5

def unique(ar, return_index=False, return_inverse=False):
    """
        KILL ME!!!!
    """
    import numpy as np
    try:
        ar = ar.flatten()
    except AttributeError:
        if not return_inverse and not return_index:
            return np.sort(list(set(ar)))
        else:
            ar = np.asanyarray(ar).flatten()

    if ar.size == 0:
        if return_inverse and return_index:
            return ar, np.empty(0, np.bool), np.empty(0, np.bool)
        elif return_inverse or return_index:
            return ar, np.empty(0, np.bool)
        else:
            return ar

    if return_inverse or return_index:
        if return_index:
            perm = ar.argsort(kind='mergesort')
        else:
            perm = ar.argsort()
        aux = ar[perm]
        flag = np.concatenate(([True], aux[1:] != aux[:-1]))
        if return_inverse:
            iflag = np.cumsum(flag) - 1
            iperm = perm.argsort()
            if return_index:
                return aux[flag], perm[flag], iflag[iperm]
            else:
                return aux[flag], iflag[iperm]
        else:
            return aux[flag], perm[flag]

    else:
        ar.sort()
        flag = np.concatenate(([True], ar[1:] != ar[:-1]))
        return ar[flag]

numpy.unique = unique

def in1d(ar1, ar2, assume_unique=False, invert=False):
    """Stolen from numpy, please kill me and upgrade numpy....
    """
    import numpy as np
    # Ravel both arrays, behavior for the first array could be different
    ar1 = np.asarray(ar1).ravel()
    ar2 = np.asarray(ar2).ravel()

    # This code is significantly faster when the condition is satisfied.
    if len(ar2) < 10 * len(ar1) ** 0.145:
        if invert:
            mask = np.ones(len(ar1), dtype=np.bool)
            for a in ar2:
                mask &= (ar1 != a)
        else:
            mask = np.zeros(len(ar1), dtype=np.bool)
            for a in ar2:
                mask |= (ar1 == a)
        return mask

    # Otherwise use sorting
    if not assume_unique:
        ar1, rev_idx = np.unique(ar1, return_inverse=True)
        ar2 = np.unique(ar2)

    ar = np.concatenate( (ar1, ar2) )
    # We need this to be a stable sort, so always use 'mergesort'
    # here. The values from the first array should always come before
    # the values from the second array.
    order = ar.argsort(kind='mergesort')
    sar = ar[order]
    if invert:
        bool_ar = (sar[1:] != sar[:-1])
    else:
        bool_ar = (sar[1:] == sar[:-1])
    flag = np.concatenate( (bool_ar, [invert]) )
    indx = order.argsort(kind='mergesort')[:len( ar1 )]

    if assume_unique:
        return flag[indx]
    else:
        return flag[indx][rev_idx]

numpy.in1d = in1d

class ArrayList(list):
    def get(self):
        return numpy.concatenate(self)

class BinnedLookup(dict):
    """ Lookup  table for a binned column vectors
    """
    def  __init__(self, columns, column_types, bounds):
        """ Create a lookup table  for a binned column vectors.
        
        Parameters
        ----------
        columns : list  of uint32 numpy  arrays
            The column  vectors to instantiate the lookup table
        column_types : 'neighbors', 'exact'
            Type of binning performed  on the column
        bounds : list of tuples (int, int)
            List of tuples of two ints that represent the cyclic boundaries
            of the bins. None if not cyclic. 
        """
        self.columns = columns
        self.column_types = column_types 
        self.bounds = bounds
       
        if  len(columns) != len(column_types) != len(bounds):
            raise ValueError('Number of columns, names, and types must match')
           
        self.columns, self.indices = self._expand_columns() 
        for j, tup in enumerate(itertools.izip(*self.columns)):   
            if tup not in self:
                self[tup] = [self.indices[j]]
            else:
                self[tup].append(self.indices[j])

    def lookup(self, columns):
        """Return two arrays of index and result for matches between
        the binned input columns and this lookup
        
        Parameters
        ----------
        columns : list of binned column vectors to look up in the dictionary
        
        Returns
        index, index : two arrays containing the matched indices
        """
        if len(columns) != len(self.columns):
            raise ValueError('Number of columns, names, and types must match')
        
        d1_indices = ArrayList()
        d2_indices = ArrayList()

        for index, tup in enumerate(itertools.izip(*columns)): 
            if tup in self:
                val = self[tup]
                indices = numpy.empty(len(val), dtype=numpy.uint32)
                indices.fill(index)
                indices2 = numpy.array(val, dtype=numpy.uint32)
    
                d1_indices.append(indices)
                d2_indices.append(indices2)
        logging.info('lookup done')        

        return d1_indices.get(), d2_indices.get()

    def _cyclic_increment(self, vec, lbound, ubound):
        vec = vec + 1
        if ubound and lbound:
            vec[vec>ubound] = lbound
        return vec
        
    def _cyclic_decrement(self, vec, lbound, ubound):
        vec = vec - 1
        if lbound and lbound:
            vec[vec<lbound] = ubound
        return vec
        
    def _expand_columns(self):
        expanded = copy.deepcopy(self.columns)
        indices = numpy.arange(0, len(expanded[0]), 1).astype(numpy.uint32)
        for i, (c, t) in enumerate(zip(self.columns, self.column_types)):
            if t == 'exact':
                continue
            elif t == 'neighbors':
                for j, (vec, (lb, ub)) in enumerate(zip(expanded, self.bounds)):
                    if i == j:
                        upper = self._cyclic_increment(vec, lb, ub)
                        lower = self._cyclic_decrement(vec, lb, ub)
                        expanded[i] = numpy.concatenate((lower, vec, upper))
                    else:
                        expanded[j] = numpy.resize(vec, len(vec)*3)
                        indices = numpy.resize(indices, len(indices)*3)
        return expanded, indices
        
def bin_vector(vector, window):
    return (vector / window).astype(numpy.uint32)
    
def bin_slide_window(time, slide_step, window):
    bins =  (numpy.remainder(time, slide_step) / window).astype(numpy.uint32)
    return bins
    
def exact_match_binning(time, template, time_step, time_window):
    columns = []
    if time_step is not None:
        columns.append(bin_slide_window(time, time_step, time_window))
    else:
        columns.append(bin_vector(time, time_window)) 
    columns.append(template)
    return columns
    
def test_exact_match_coincidence(index1, index2, time1, time2, slide_step, window):
    coinc_times1 = time1[index1]
    coinc_times2 = time2[index2]
    if slide_step is not None:
        slide_index = numpy.around(((coinc_times2 - coinc_times1) / slide_step)).astype(numpy.int32)
        time_diff = abs(coinc_times2 - coinc_times1 - slide_index * slide_step)
    else:
        slide_index = numpy.zeros(len(index1), dtype=numpy.int32)
        time_diff = abs(coinc_times2 - coinc_times1)
    passes = (time_diff <= window)
    return index1[passes], index2[passes], slide_index[passes]
    
def load_triggers(trigger_files, group_id, stat_class):
    triggers = {}
    group_id = int(group_id)
    trig_segments = {}
    num_trigs = 0
    
    for i, filename in enumerate(trigger_files):
        logging.info('Loading file: %s' % filename)
        f = h5py.File(filename, "r")
        ifo = str(f.attrs['ifo'])

        # Test if file is empty
        try:
            f['snr']
        except KeyError:
            continue

        # Compile a complete segment list for the foreground time
        if ifo not in trig_segments:
            trig_segments[ifo] = segments.segmentlist()  
          
        seg = segments.segment(f.attrs['start_time'], f.attrs['end_time'])
        trig_segments[ifo] += [seg]
        
        l = f['template_group/left'][group_id]
        r = f['template_group/right'][group_id]

        if (r-l) > 0:
            end_time = f['end_time'][l:r]
            th = f['template_id'][l:r]
            snr = stat_class.single_read(f, l, r)
            
            if ifo not in triggers:
                triggers[ifo] = ([], [], [])

            sl, el, tl = triggers[ifo]
            sl += [snr]
            el += [end_time]
            tl += [th]

            num_trigs += len(snr)
            logging.info("%s/%s : Total Trigs=%s:" % (i+1, len(trigger_files), num_trigs))
        f.close()
    for key in triggers.keys():
        sl, el, tl = triggers[key]
        triggers[key] = (numpy.concatenate(sl), 
                         numpy.concatenate(el),
                         numpy.concatenate(tl))
    return triggers, trig_segments
    
def decimate_triggers(stat, slide, num_keep, bins, factor=2):
    """ Remove coinc triggers that are unlikely to be significant in
    determining in affecting the error bars of a FAP calculation
    
    Parameters
    ----------
    stat : numpy array
        The values of the ranking statistic
    slide : numpy array
        The array of timeslide id values that identify the unique timeslide
        this trigger was coincident for. 
    num_keep : int 
        The number of triggers that will be kept at the highest ranking
    statistic value without any decimation.
    bins : int
        The number of network statistic bins to perform separate decimation
    in. The boundaries are divided evenly between the statistic value range.
    factor : int
        The factor to decimate each bin by. This factor of slides will be
    removed.
    
    """
    stat_sort = stat.argsort()
    stat = stat[stat_sort]
    slide = slide[stat_sort]
    
    # keep the number of loud triggers
    keep = numpy.arange(len(stat) - num_keep, len(stat), 1)
    factors = numpy.zeros(len(keep), dtype=numpy.uint32) + 1
    stat = stat[0:len(stat) - num_keep]
    slide = slide[0:len(stat) - num_keep]

    min_slide = slide.min()
    max_slide = slide.max()
 
    if bins > 0:
        edges = [stat[0] + inc*(stat[-1]-stat[0])/bins for inc in range(bins)]
        bin_edges = numpy.append(numpy.searchsorted(stat, edges), len(stat))
        for i in range(bins):
            lbin = bin_edges[-i-2]
            rbin = bin_edges[-i-1]
        
            keep_upper = numpy.arange(0, max_slide + factor, factor)
            keep_lower = numpy.arange(-factor, min_slide - factor, -factor)
            keep_slides = numpy.unique(numpy.append(keep_upper, keep_lower))
            
            bin_int = numpy.in1d(slide[lbin:rbin], keep_slides)
            bin_keep = numpy.where(bin_int)[0] + lbin
            
            factors = numpy.append(factors, numpy.zeros(len(bin_keep))+factor)
            keep = numpy.append(keep, bin_keep)
            factor = factor * 2   

    return stat_sort[keep], factors.astype(numpy.uint32)

def veto_indices(times, ifo, veto_files):
    """ Return the list of indices that should be vetoed by the segments in the
    lsit of veto_files.
    """
    time_sorting = numpy.argsort(times)
    times = times[time_sorting]
    indices = numpy.array([], dtype=numpy.uint32)

    from glue.ligolw import ligolw, table, lsctables, utils as ligolw_utils
    class LIGOLWContentHandler(ligolw.LIGOLWContentHandler):
        pass
    lsctables.use_in(LIGOLWContentHandler)
   
    for veto_file in veto_files:
        indoc = ligolw_utils.load_filename(veto_file, False, contenthandler=LIGOLWContentHandler)
        segment_table  = table.get_table(indoc, lsctables.SegmentTable.tableName)
        
        seg_def_table = table.get_table(indoc, lsctables.SegmentDefTable.tableName)
        def_ifos = seg_def_table.getColumnByName('ifos')
        def_ids = seg_def_table.getColumnByName('segment_def_id')
        ifo_map =  {}
        for def_ifo, def_id in zip(def_ifos, def_ids):
            ifo_map[def_id] = def_ifo
        
        start = numpy.array(segment_table.getColumnByName('start_time')) + numpy.array(segment_table.getColumnByName('start_time_ns')) * 1e-9
        end = numpy.array(segment_table.getColumnByName('end_time')) + numpy.array(segment_table.getColumnByName('end_time_ns')) * 1e-9
        ifos = [ifo_map[v] for v in segment_table.getColumnByName('segment_def_id')]
        
        veto_segs = segments.segmentlist()
        for s, e, ifo_row in zip(start, end, ifos):
            if ifo != ifo_row:
                continue
                
            veto_segs += [segments.segment(s, e)]

        veto_segs.coalesce()        

        left = numpy.searchsorted(times, start, side='left')
        right = numpy.searchsorted(times, end, side='right')
        for li, ri, ifo_row in zip(left, right, ifos):
            if ifo != ifo_row:
                continue
                
            seg_indices = numpy.arange(li, ri, 1).astype(numpy.uint32)
            indices=numpy.union1d(seg_indices, indices)  
            
    return time_sorting[indices], veto_segs

if __name__ == '__main__': 
    parser = argparse.ArgumentParser()
    # General required options
    parser.add_argument('--trigger-files', nargs='+', help='List of hdf format single detector inspiral trigger files.')
    parser.add_argument('--veto-files', nargs='+', help='List of veto files to apply to the single detector triggers.')
    # Could be different ways to communicate this information, think about it
    parser.add_argument('--timeslide-interval', type=float, default=None, help='Interval in seconds between timeslide offsets')
    parser.add_argument('--template-group', nargs='+', help='Template group index to analyze')
    parser.add_argument('--verbose', '-v', action='count')
    parser.add_argument('--output-file')
    parser.add_argument('--decimation-factor', type=int, help='Factor to decimate triggers by for each bin')
    parser.add_argument('--decimation-bins', type=int, help='Number of decimation intervals')
    parser.add_argument('--decimation-keep', type=int, help='Number of the most significant backgroud triggers to keep without decimation')
    parser.add_argument('--coinc-threshold', type=float, default=0, help='Fixed time to add to the time-of-flight coincidence window size in seconds')
    parser.add_argument('--coinc-statistic', default='newsnr', help='The coinc statistic to calculate. The default is newsnr')
    args = parser.parse_args()
    
    stat_class = get_statistic_class(args.coinc_statistic)

    if args.verbose == 1:
        log_level = logging.INFO
    elif args.verbose == 2:
        log_level = logging.DEBUG
    else:
        log_level = logging.WARN
    logging.basicConfig(format='%(asctime)s : %(message)s', level=log_level)       

    stat_all = []
    t1_all = []
    t2_all = []
    dec_all = []
    slide_all = []
    template_all = []
    
    for template_group in args.template_group:
        logging.info('Loading triggers from template group %s' % template_group)
        triggers, foreground_segments = load_triggers(args.trigger_files, template_group, stat_class)

        ifo1, ifo2 = triggers.keys()
        s1, t1, r1 = triggers[ifo1]
        s2, t2, r2 = triggers[ifo2]
        fore_time1 = abs(foreground_segments[ifo1])
        fore_time2 = abs(foreground_segments[ifo2])
        coinc_segs = (foreground_segments[ifo1] & foreground_segments[ifo2]).coalesce()
        coinc_time = abs(coinc_segs)
        
        start = []
        end = []
        for seg in coinc_segs:
            start.append(seg[0])
            end.append(seg[1])
        start = numpy.array(start)
        end = numpy.array(end)    
        
        logging.info('%s time: %s  %s time: %s' %  (ifo1, fore_time1, ifo2, fore_time2))   
        #logging.info('%s loudest: %s  %s loudest: %s' %  (ifo1, s1.max(), ifo2, s2.max()))   
        #logging.info('%s loudest: %s  %s loudest: %s' %  (ifo1, s1.max(), ifo2, s2.max()))
        
        if  args.veto_files:
            logging.info('applying vetoes')
            veto1, vsegs1 = veto_indices(t1, ifo1, args.veto_files)
            s1, t1, r1 = numpy.delete(s1, veto1), numpy.delete(t1, veto1), numpy.delete(r1, veto1)
            fore_time1 -= abs(foreground_segments[ifo1] & vsegs1)

            veto2, vsegs2 = veto_indices(t2, ifo2, args.veto_files)     
            s2, t2, r2 = numpy.delete(s2, veto2), numpy.delete(t2, veto2), numpy.delete(r2, veto2)
            fore_time2 -= abs(foreground_segments[ifo2] & vsegs2)
            logging.info('%s time: %s  %s time: %s' %  (ifo1, fore_time1, ifo2, fore_time2))   
            
        det1 = Detector(ifo1)
        det2 = Detector(ifo2)
        logging.info('%s num: %s   %s num: %s' % (ifo1, len(s1), ifo2, len(s2)))
        time_window = det1.light_travel_time_to_detector(det2) + args.coinc_threshold
        
        #logging.info('%s loudest: %s  %s loudest: %s' %  (ifo1, s1.max(), ifo2, s2.max()))  
        #logging.info('%s loudest: %s  %s loudest: %s' %  (ifo1, s1.max(), ifo2, s2.max()))
        
        slide = args.timeslide_interval  
        logging.info('making bins') 
        c1s = exact_match_binning(t1, r1, slide, time_window)
        c2s = exact_match_binning(t2, r2, slide, time_window)
        
        logging.info('making lookup table')     
        if args.timeslide_interval is not None: 
            logging.info('looking for foreground and background events')
            course = BinnedLookup(c2s, ['neighbors', 'exact'],
                                       [(0, int(slide/time_window)), (None, None)])
            num_slides = int(max(fore_time1, fore_time2) / args.timeslide_interval)
        else:
            logging.info('only looking for foreground events')
            course = BinnedLookup(c2s, ['neighbors', 'exact'], 
                                       [(None, None), (None, None)]) 
            num_slides = 0
            
        logging.info('Number of timeslides: %s' % num_slides)
        logging.info('finding course coincidences')                  
        c1, c2 = course.lookup(c1s)
        
        logging.info('finding coincidences')   
        c1, c2, slide_index = test_exact_match_coincidence(c1, c2, t1, t2, slide, time_window)
        
        logging.info('%s total triggers' % len(c1))
        logging.info('%s foreground triggers' % (slide_index == 0).sum())
        
        logging.info('calculating the coinc statistic')
        stat = stat_class.coinc_statistic(s1[c1], s2[c2])
        
        if args.decimation_keep < len(stat) and args.timeslide_interval is not None: 
            logging.info('decimating triggers')
            d1, decimation_factor = decimate_triggers(stat, slide_index, 
                                   args.decimation_keep, 
                                   args.decimation_bins, 
                                   args.decimation_factor)                                 
            e1 = c1[d1]
            e2 = c2[d1]
            stat, t1, t2, r1 = stat[d1], t1[e1], t2[e2], r1[e1]
            slide_index = slide_index[d1]
            logging.info('%s total triggers' % len(stat))
            logging.info('%s foreground triggers' % (slide_index == 0).sum())
        else:
            t1, t2, r1 = t1[c1], t2[c2], r1[c1]
            decimation_factor = numpy.zeros(len(stat), dtype=numpy.uint32) + 1
        
        # Accumulate the coinc triggers from this groups of templates
        stat_all += [stat]
        t1_all += [t1]
        t2_all += [t2]
        dec_all += [decimation_factor]
        slide_all += [slide_index]
        template_all += [r1]
        
    logging.info('saving coincident triggers')
    f = h5py.File(args.output_file, "w")
    
    if len(stat_all) > 0:
        f.create_dataset('coinc/stat', data=numpy.concatenate(stat_all))
        f.create_dataset('coinc/decimation_factor', data=numpy.concatenate(dec_all))
        f.create_dataset('coinc/time1', data=numpy.concatenate(t1_all))
        f.create_dataset('coinc/time2', data=numpy.concatenate(t2_all))
        f.create_dataset('coinc/timeslide_id', data=numpy.concatenate(slide_all))
        f.create_dataset('coinc/template_id', data=numpy.concatenate(template_all))
        f['coinc/analyzed_start'] = start
        f['coinc/analyzed_end'] = end
        f.attrs['timeslide_interval'] = numpy.float64(args.timeslide_interval)
        f.attrs['detector_1'] = det1.name
        f.attrs['detector_2'] = det2.name
        f.attrs['foreground_time1'] = fore_time1
        f.attrs['foreground_time2'] = fore_time2
        f.attrs['coinc_time'] = coinc_time
        f.attrs['num_slides'] = num_slides
    else:
        logging.info('huh, why are there no triggers?')

    logging.info('Done')
