#!/bin/env  python
"""
The program combines coincident output files generated
by pycbc_coinc_findtrigs to generated a mapping between SNR and FAP, along
with producing the combined foreground and background triggers
"""
import argparse, h5py, numpy, logging, itertools, copy
from scipy.interpolate import interp1d    
    
def load_coincs(coinc_files):
    stat1 = []
    stat2 = []
    time1 = []
    time2 = []
    timeslide_id = []
    template_id = []
    decimation_factor = []    
    for cfile in coinc_files:
        logging.info('reading %s' % cfile)
        f = h5py.File(cfile, "r")
        stat1.append(f['coinc/stat1'])
        stat2.append(f['coinc/stat2'])
        time1.append(f['coinc/time1'])
        time2.append(f['coinc/time2'])
        timeslide_id.append(f['coinc/timeslide_id'])
        template_id.append(f['coinc/template_id'])
        decimation_factor.append(f['coinc/decimation_factor'])
        attr = dict(f.attrs)
    return (numpy.concatenate(stat1),
           numpy.concatenate(stat2),
           numpy.concatenate(time1),
           numpy.concatenate(time2),
           numpy.concatenate(timeslide_id),
           numpy.concatenate(template_id), 
           numpy.concatenate(decimation_factor),
           attr,
           )
  
def changed(arr):
    return arr[:-1] != arr[1:]
  
def cluster_coincs(stat1, stat2, time1, time2, 
                   timeslide_id, timeslide_interval, window):
    """Cluster coincident events for each timeslide separately, across 
    templates, based on the loudest network statistic (stat1**2 + stat2**2). 
    Return the set of indices corresponding to the surviving coincidences.
    """
    
    logging.info('clustering coinc triggers over %ss window' % window)
    
    indices = []
    nstatsq = stat1**2.0 + stat2**2.0
    if numpy.isfinite(timeslide_interval):
        time = (time2 + (time1 + timeslide_id * timeslide_interval)) / 2
    else:
        time = 0.5 * (time2 + time1)
        
    time_sorting = time.argsort()
    nstatsq = nstatsq[time_sorting]
    time = time[time_sorting]
    slide = timeslide_id[time_sorting]
    
    ledge = time - window
    redge = time + window
    lbin = numpy.searchsorted(time, ledge, side='left')
    rbin = numpy.searchsorted(time, redge, side='right')
    
    bin_edges = numpy.where(changed(lbin) + changed(rbin))
    grouped_lbin = lbin[bin_edges]
    grouped_rbin = rbin[bin_edges]  
    
    for l, r in itertools.izip(grouped_lbin, grouped_rbin):  
        unique_slides = numpy.unique(slide[l:r])    
        for slide_id in unique_slides:
            slide_locs = numpy.where(slide[l:r] == slide_id)[0]        
            indices.append(slide_locs[nstatsq[l:r][slide_locs].argmax()] + l)
       
    return numpy.unique(numpy.array(time_sorting[indices], dtype=numpy.uint32))

def calculate_fan_map(combined_stat, dec):
    """ Return a function to map between false alarm number (FAN) and the
    combined ranking statistic.
    """
    stat_sorting = combined_stat.argsort()
    
    combined_stat = combined_stat[stat_sorting]
    fan = dec[stat_sorting][::-1].cumsum()[::-1]
    
    return interp1d(combined_stat, fan, fill_value=0.67, bounds_error=False) 

def cstat(s1, s2):
    return numpy.sqrt(s1**2.0 + s2 ** 2.0)

def sec_to_year(sec):
    return sec / (3.15569e7)

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    # General required options
    parser.add_argument('--coinc-files', nargs='+')
    parser.add_argument('--verbose', action='count')
    parser.add_argument('--cluster-window', type=float, default=0)
    parser.add_argument('--output-file')
    args = parser.parse_args()

    if args.verbose:
        log_level = logging.INFO
        logging.basicConfig(format='%(asctime)s : %(message)s', 
                                level=log_level)
    
    logging.info("Loading coinc triggers")    
    s1, s2, t1, t2, sid, tid, dec, attrs = load_coincs(args.coinc_files)   
    logging.info("We have %s triggers" % len(s1))

    if args.cluster_window != 0:
        logging.info("Clustering coinc triggers")
        cid = cluster_coincs(s1, s2, t1, t2, sid, 
                             attrs['timeslide_interval'], 
                             args.cluster_window)
                         
        s1, s2, t1, t2, sid, tid, dec = s1[cid], s2[cid], t1[cid], t2[cid], sid[cid], tid[cid], dec[cid]
        logging.info("We now have %s triggers" % len(s1))
    
    logging.info("Dumping foreground triggers")
    f = h5py.File(args.output_file, "w")
    
    f.attrs['detector_1'] = attrs['detector_1']
    f.attrs['detector_2'] = attrs['detector_2']
    f.attrs['num_slides'] = attrs['num_slides']
    
    fore_locs = sid == 0
    f['foreground/stat1'] = s1[fore_locs]
    f['foreground/stat2'] = s2[fore_locs]
    f['foreground/time1'] = t1[fore_locs]
    f['foreground/time2'] = t2[fore_locs]
    f['foreground/template_id'] = tid[fore_locs]
    
    back_locs = numpy.where((sid != 0))[0]
    if len(back_locs) > 0:
        logging.info("Dumping background triggers")
        f['background/stat1'] = s1[back_locs]
        f['background/stat2'] = s2[back_locs]
        f['background/time1'] = t1[back_locs]
        f['background/time2'] = t2[back_locs]
        f['background/timeslide_id'] = sid[back_locs]
        f['background/template_id'] = tid[back_locs]

        logging.info("Making mapping from FAN to the combined statistic")
        fanmap = calculate_fan_map(cstat(s1[back_locs], s2[back_locs]), dec[back_locs])
        
        maxtime = max(attrs['foreground_time1'], attrs['foreground_time2'])
        mintime = min(attrs['foreground_time1'], attrs['foreground_time2'])
        
        background_time = int(maxtime / attrs['timeslide_interval']) * mintime
        coinc_time = float(attrs['coinc_time'])

        logging.info("calculating ifar values")
        ifar = background_time / fanmap(cstat(s1[fore_locs], s2[fore_locs]))

        logging.info("calculating fap values")
        fap = numpy.clip(coinc_time/ifar, 0, 1)
        
        f['foreground/ifar'] = sec_to_year(ifar)
        f['foreground/fap'] = fap
    else:
        pass

    logging.info("Done") 
    
