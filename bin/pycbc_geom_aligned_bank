#!/usr/bin/env python

# Copyright (C) 2011 Ian W. Harry
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
Aligned spin bank generator.
"""


from __future__ import division
import matplotlib
matplotlib.use('Agg')
import pylab
import time
startTS = time.time()
elapsed_time = lambda: time.time()-startTS

import os,sys,optparse
import tempfile
import ConfigParser
import numpy
from glue import pipeline
import pycbc.frame
import pycbc.psd
import pycbc.strain
import pycbc.version
import pycbc.tmpltbank
import pycbc.filter

__author__  = "Ian Harry <ian.harry@ligo.org>"
__version__ = pycbc.version.git_verbose_msg
__date__    = pycbc.version.date
__program__ = "pycbc_geom_aligned_bank"

# Read command line options
usage = """usage: %prog [options]"""
_desc = __doc__[1:]
parser = optparse.OptionParser(usage, version=__version__, description=_desc,\
           formatter=pycbc.tmpltbank.IndentedHelpFormatterWithNL())

# Begin with code specific options
parser.add_option("-v", "--verbose", action="store_true", default=False,\
                    help="verbose output, default: %default")
parser.add_option("-O", "--output-file",  help="Output file name. "+\
                                               "REQUIRED ARGUMENT.")
parser.add_option("-L", "--log-path", action="store", type="string",\
                   default=None,\
                   help="Directory to store condor logs. REQUIRED ARGUMENT")
parser.add_option("-m", "--min-match", action="store", type="float",\
                  default=None, help="Minimum match to generate bank with. "+\
                                      "REQUIRED ARGUMENT")
parser.add_option("-s", "--stack-distance", action="store", type="float",\
                  default=0.2, help="Minimum metric spacing before we "+\
                               "stack. OPTIONAL: default= %default")
parser.add_option("-3", "--threed-lattice", action="store_true", default=False,\
                    help="Set this to use a 3D lattice. "+\
                         "OPTIONAL: default= %default")
parser.add_option("-S", "--split-bank-num", action="store", type="int",\
                    default=100,\
                    help="Number of points per job in dag. "+\
                         "OPTIONAL: default= %default")

# Insert the metric calculation options
pycbc.tmpltbank.insert_metric_calculation_options(parser)

# Insert the mass range options
pycbc.tmpltbank.insert_mass_range_option_group(parser)

# Insert the PSD options
pycbc.psd.insert_psd_option_group(parser)

# Insert the data reading options
pycbc.strain.insert_strain_option_group(parser)

(opts,args) = parser.parse_args()
# Sanity check options
if not opts.output_file:
    parser.error("Must supply --output-file")
if not opts.min_match:
    parser.error("Must supply --min-match")
opts.max_mismatch = 1 - opts.min_match
if not opts.log_path:
    parser.error("Must supply --log-path")
pycbc.tmpltbank.verify_metric_calculation_options(opts, parser)
metricParams=pycbc.tmpltbank.metricParameters.from_optparse(opts)
pycbc.tmpltbank.verify_mass_range_options(opts, parser)
massRangeParams=pycbc.tmpltbank.massRangeParameters.from_optparse(opts)
pycbc.psd.verify_psd_options(opts, parser)
if opts.psd_estimation:
    pycbc.strain.verify_strain_options(opts, parser)

# If we are going to use h(t) to estimate a PSD we need h(t)
if opts.psd_estimation:
    # FIXME: It would be nice if this was similar to psd.from_cli()
    if opts.verbose:
        print >>sys.stdout, "Obtaining h(t) for PSD generation at %.3f" \
                            %(elapsed_time())
    strain = pycbc.strain.from_cli(opts)
else:
    strain = None

# Get the PSD using the pycbc interface
if opts.verbose:
    print >>sys.stdout, "Obtaining PSD at %.3f." %(elapsed_time())
# Want the number of samples to be a binary number and Nyquist must be above
# opts.f_upper. All this assumes that 1 / deltaF is a binary number
nyquistFreq = 2**numpy.ceil(numpy.log2(opts.f_upper))
numSamples = int(round(nyquistFreq / opts.delta_f)) + 1
psd = pycbc.psd.from_cli(opts, length=numSamples, delta_f=opts.delta_f, \
                         low_frequency_cutoff=opts.f_low, strain=strain)
metricParams.psd = psd

if opts.verbose:
    print >>sys.stdout, "Calculating metric at %.3f." %(elapsed_time())

# Begin by calculating a metric
metricParams = pycbc.tmpltbank.determine_eigen_directions(metricParams)

if opts.verbose:
    print >>sys.stdout, "Calculating covariance matrix at %.3f."\
                        %(elapsed_time())

vals = pycbc.tmpltbank.estimate_mass_range(1000000, massRangeParams, \
       metricParams, metricParams.fUpper, covary=False) 
cov = numpy.cov(vals)
evalsCV,evecsCV = numpy.linalg.eig(cov)
metricParams.evecsCV = {}
metricParams.evecsCV[metricParams.fUpper] = evecsCV

if opts.verbose:
    print>> sys.stdout, "Covariance matrix calculated at %.3f."\
                        %(elapsed_time())

vals = pycbc.tmpltbank.estimate_mass_range(1000000, massRangeParams, \
       metricParams, metricParams.fUpper, covary=True)

chi1Max = vals[0].max()
chi1Min = vals[0].min()
chi1Diff = chi1Max - chi1Min
chi2Max = vals[1].max()
chi2Min = vals[1].min()
chi2Diff = chi2Max - chi2Min

if opts.verbose:
    print>> sys.stdout, "Calculating lattice at %.3f."\
                        %(elapsed_time())

if not opts.threed_lattice:
    v1s,v2s = pycbc.tmpltbank.generate_hexagonal_lattice(\
              chi1Max+(0.02*chi1Diff), chi1Min-(0.02*chi1Diff),\
              chi2Max+(0.02*chi2Diff), chi2Min-(0.02*chi2Diff),\
              opts.max_mismatch)
else:
    chi3Max = vals[2].max()
    chi3Min = vals[2].min()
    chi3Diff = chi3Max - chi3Min
    v1s, v2s, v3s = pycbc.tmpltbank.generate_anstar_3d_lattice(\
          chi1Max+(0.02*chi1Diff), chi1Min-(0.02*chi1Diff),\
          chi2Max+(0.02*chi2Diff), chi2Min-(0.02*chi2Diff),\
          chi3Max+(0.02*chi3Diff), chi3Min-(0.02*chi3Diff), opts.max_mismatch)
    chi3Max = vals[2].max()
    chi3Min = vals[2].min()
    chi3Diff = chi3Max - chi3Min

if opts.verbose:
    print>> sys.stdout, "Lattice calculated at %.3f." %(elapsed_time())
    print>> sys.stdout, "Lattice contains %d points." %(len(v1s))

# Dump the files needed to define the xi_i parameter space
numpy.savetxt('metric_evecs.dat', metricParams.evecs[opts.f_upper])
numpy.savetxt('metric_evals.dat', metricParams.evals[opts.f_upper])
numpy.savetxt('covariance_evecs.dat', metricParams.evecsCV[opts.f_upper])

# Dump the full bank in \xi_i coordinates
bankFile = open('bank_chis.dat','w')
if opts.threed_lattice:
    for i in xrange(len(v1s)):
        print >> bankFile, "%e %e %e" %(v1s[i],v2s[i],v3s[i])
else:
    for i in xrange(len(v1s)):
        print >> bankFile, "%e %e" %(v1s[i],v2s[i])
bankFile.close()

# Now begin to generate the dag
# First split the bank
if not os.path.isdir('split_banks'):
    os.makedirs('split_banks')
if not os.path.isdir('output_banks'):
    os.makedirs('output_banks')
if not os.path.isdir('logs'):
    os.makedirs('logs')

bankNum = 0
bankFile = open('split_banks/split_bank_%05d.dat'%(bankNum),'w')

if opts.verbose:
    print>> sys.stdout, "Printing split banks at %.3f." %(elapsed_time())

for i in xrange(len(v1s)):
    if opts.threed_lattice:
        print >> bankFile, "%e %e %e" %(v1s[i],v2s[i],v3s[i])
    else:
        print >> bankFile, "%e %e" %(v1s[i],v2s[i])
    if not (i+1) % opts.split_bank_num:
        bankFile.close()
        bankNum = bankNum + 1
        if not i == (len(v1s)-1):
            bankFile = open('split_banks/split_bank_%05d.dat'%(bankNum),'w')
  
if len(v1s) % opts.split_bank_num:
    bankFile.close()

# And begin dag generation
tempfile.tempdir = opts.log_path
tempfile.template='bank_gen.dag.log.'
logfile = tempfile.mktemp()
fh = open( logfile, "w" )
fh.close()
dag = pipeline.CondorDAG(logfile, False)
dag.set_dag_file('bank_generation')
exe_path = pycbc.tmpltbank.which('pycbc_geom_aligned_2dstack')
job = pipeline.CondorDAGJob('vanilla',exe_path)
#pipeline.AnalysisJob.__init__(job,cp,False)
job.set_stdout_file('logs/bank_gen-$(cluster)-$(process).out')
job.set_stderr_file('logs/bank_gen-$(cluster)-$(process).err')
job.set_sub_file('bank_gen.sub')
# Add global job options
cp = ConfigParser.ConfigParser()
cp.add_section('bank')
cp.set('bank','pn-order',opts.pn_order)
cp.set('bank','metric-evals-file','metric_evals.dat')
cp.set('bank','metric-evecs-file','metric_evecs.dat')
cp.set('bank','cov-evecs-file','covariance_evecs.dat')
cp.set('bank','f0',str(opts.f0))
cp.set('bank','max-mass1',str(opts.max_mass1))
cp.set('bank','min-mass1',str(opts.min_mass1))
cp.set('bank','max-mass2',str(opts.max_mass2))
cp.set('bank','min-mass2',str(opts.min_mass2))
if opts.max_total_mass:
    cp.set('bank','max-total-mass',str(opts.max_total_mass))
if opts.min_total_mass:
    cp.set('bank','min-total-mass',str(opts.min_total_mass))
if opts.max_eta:
    cp.set('bank','max-eta',str(opts.max_eta))
if opts.min_eta:
    cp.set('bank','min-eta',str(opts.min_eta))
cp.set('bank','max-ns-spin-mag',str(opts.max_ns_spin_mag))
cp.set('bank','max-bh-spin-mag',str(opts.max_bh_spin_mag))
cp.set('bank','min-match',str(opts.min_match))
cp.set('bank','stack-distance',str(opts.stack_distance))
if opts.nsbh_flag:
    cp.set('bank','nsbh-flag','')
if opts.threed_lattice:
    cp.set('bank','threed-lattice','')
job.add_ini_opts(cp,'bank')
job.add_condor_cmd('Requirements', 'Memory >= 1390')
job.add_condor_cmd('request_memory', '1400')
job.add_condor_cmd('getenv','True')
# Make the output job
cat_path = pycbc.tmpltbank.which('pycbc_aligned_bank_cat')
job_cat = pipeline.CondorDAGJob('vanilla', cat_path)
job_cat.set_stdout_file('logs/bank_cat-$(cluster)-$(process).out')
job_cat.set_stderr_file('logs/bank_cat-$(cluster)-$(process).err')
job_cat.set_sub_file('bank_cat.sub')
job_cat.add_condor_cmd('getenv','True')

# Make the output node
cat_node = pipeline.CondorDAGNode(job_cat)
cat_node.add_var_opt('input-glob', 'output_banks/output_bank_*.dat')
cat_node.add_var_opt('output-file', opts.output_file)

# Make the nodes
numBanks = int((len(v1s) - 0.5)//opts.split_bank_num) + 1
for i in xrange(numBanks):
    node = pipeline.CondorDAGNode(job)
    node.add_var_opt('input-bank-file','split_banks/split_bank_%05d.dat'%(i))
    node.add_var_opt('output-bank-file','output_banks/output_bank_%05d.dat'%(i))
    cat_node.add_parent(node)
    dag.add_node(node)

dag.add_node(cat_node)
dag.write_sub_files()
dag.write_dag()
dag.write_script()

print "Now submit bank_generation.dag to generate your template bank."
print "This may take a while, go make a cup of tea!"
