#!/usr/bin/env python

# Copyright (C) 2011 Ian W. Harry
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

# =============================================================================
# Preamble
# =============================================================================

"""
Determine efficiency and exlucion distances of a PyGRB run.
"""

from __future__ import division

import sys
import os
import logging
import matplotlib.pyplot as plt
from matplotlib import rc
import numpy as np
import scipy
from scipy import stats
import pycbc.version
from pycbc.results import save_fig_with_metadata
from pycbc.results.pygrb_postprocessing_utils import extract_ifos_and_vetoes
from pycbc.results.pygrb_postprocessing_utils import load_xml_table
from pycbc.results.pygrb_postprocessing_utils import load_segment_dict
from pycbc.results.pygrb_postprocessing_utils import find_zero_lag_slide_id
from pycbc.results.pygrb_postprocessing_utils import load_time_slides
from pycbc.results.pygrb_postprocessing_utils import pygrb_plot_opts_parser
from pycbc.results.pygrb_postprocessing_utils import construct_trials
from pycbc.results.pygrb_postprocessing_utils import sort_trigs
from pycbc.results.pygrb_postprocessing_utils import sort_stat
from pycbc.results.pygrb_postprocessing_utils import max_median_stat
from pycbc.results.pygrb_postprocessing_utils import efficiency_with_errs
from glue.ligolw import lsctables

# Deprecated
from pylal import SimInspiralUtils, MultiInspiralUtils
from pylal.coh_PTF_pyutils import readSegFiles, get_bestnr, get_f_resp

plt.switch_backend('Agg')
rc("image")

__author__ = "Francesco Pannarale <francesco.pannarale@ligo.org>"
__version__ = pycbc.version.git_verbose_msg
__date__ = pycbc.version.date
__program__ = "pycbc_pygrb_efficiency"

# =============================================================================
# Main script starts here
# =============================================================================
description = 'Efficiency calculator for the triggered search (PyGRB).'
usage = __program__ + ' [--options]'
opts = pygrb_plot_opts_parser(usage=usage, description=description, version=__version__)

if opts.verbose:
    level = logging.INFO
else:
    level = logging.WARNING
logging.basicConfig(format="%(asctime)s:%(levelname)s : %(message)s",
                    level=level)

# Check options
if opts.offsource_file is None:
    err_msg = "Please specify a path to the offsource trigger file."
    logging.error(err_msg)

if opts.veto_directory and (opts.veto_category is None):
    err_msg = "Must supply veto category if applying vetoes."
    logging.error(err_msg)

if (opts.found_file is None) and (opts.missed_file is None):
    do_injections = False
elif (opts.found_file) and opts.missed_file:
    do_injections = True
else:
    err_msg = "Must provide both found and missed file if running injections."
    logging.error(err_msg)

if opts.background_output_file is None:
    err_msg = "Please specify a path to save the detection efficiency plot."
    logging.error(err_msg)

if opts.onsource_output_file is None:
    err_msg = "Please specify a path to save the exclusion distance plot."
    logging.error(err_msg)

if not opts.newsnr_threshold:
    opts.newsnr_threshold = opts.snr_threshold

# Store options used multiple times in local variables
outdir = os.path.split(os.path.abspath(opts.background_output_file))[0]
trig_file = opts.offsource_file
onsource_file = opts.onsource_file
found_file = opts.found_file
missed_file = opts.missed_file
inj_set_name = os.path.split(os.path.abspath(missed_file))[1].split('INSPINJ')[1].split('_')[1]
chisq_index = opts.chisq_index
chisq_nhigh = opts.chisq_nhigh
wf_err = opts.waveform_error
cal_errs = {}
cal_errs['H1'] = opts.h1_cal_error
cal_errs['K1'] = opts.k1_cal_error
cal_errs['L1'] = opts.l1_cal_error
cal_errs['V1'] = opts.v1_cal_error
cal_dc_errs = {}
cal_dc_errs['H1'] = opts.h1_dc_cal_error
cal_dc_errs['K1'] = opts.k1_dc_cal_error
cal_dc_errs['L1'] = opts.l1_dc_cal_error
cal_dc_errs['V1'] = opts.v1_dc_cal_error
snr_thresh = opts.snr_threshold
sngl_snr_thresh = opts.sngl_snr_threshold
new_snr_thresh = opts.newsnr_threshold
null_grad_thresh = opts.null_grad_thresh
null_grad_val = opts.null_grad_val
null_thresh = map(float, opts.null_snr_threshold.split(','))
upper_dist = opts.upper_inj_dist
lower_dist = opts.lower_inj_dist
num_bins = opts.num_bins
wav_err = opts.waveform_error
cluster_window = opts.cluster_window
glitch_check_fac = opts.glitch_check_factor
num_mc_injs = opts.num_mc_injections
# Initialize random number generator
np.random.seed(opts.seed)
logging.info("Setting random seed to %d.", opts.seed)

# Set output directory
logging.info("Setting output directory.")
if not os.path.isdir(outdir):
    os.makedirs(outdir)

# Extract IFOs and vetoes
logging.info("Extracting IFOs and vetoes.")
ifos, vetoes = extract_ifos_and_vetoes(trig_file, opts.veto_directory, \
                                       opts.veto_category)

# Load triggers, time-slides, and segment dictionary
logging.info("Loading triggers.")
trigs = load_xml_table(trig_file, lsctables.MultiInspiralTable.tableName)
logging.info("%d triggers loaded.", len(trigs))
logging.info("Loading timeslides.")
slide_dict = load_time_slides(trig_file)
logging.info("Loading segments.")
segment_dict = load_segment_dict(trig_file)

# Identify the zero-lag slide and the number of slides
zero_lag_slide_id = find_zero_lag_slide_id(slide_dict)
num_slides = len(slide_dict)

# Get segments
segs = readSegFiles(opts.segment_dir)

# Construct trials
logging.info("Constructing trials.")
trial_dict = construct_trials(num_slides, segs, segment_dict, ifos, slide_dict, vetoes)

# Sort the triggers into each slide
sorted_trigs = sort_trigs(trial_dict, trigs, num_slides, segment_dict)

total_trials = sum([len(trial_dict[slide_id]) for slide_id in range(num_slides)])

msg = "Segments loaded, triggers sorted, and %d trials generated." % (total_trials)
logging.info(msg)

# Extract basic trigger properties and store as dictionaries
trig_time = {}
trig_snr = {}
trig_bestnr = {}
for slide_id in range(num_slides):
    slide_trigs = sorted_trigs[slide_id]
    trig_time[slide_id] = np.asarray(slide_trigs.get_end()).astype(float)
    trig_snr[slide_id] = np.asarray(slide_trigs.get_column('snr'))
    trig_bestnr[slide_id] = [get_bestnr(t, q=chisq_index, n=chisq_nhigh,\
                                        null_thresh=null_thresh,\
                                        snr_threshold=snr_thresh,\
                                        sngl_snr_threshold=sngl_snr_thresh,\
                                        chisq_threshold=new_snr_thresh,\
                                        null_grad_thresh=null_grad_thresh,\
                                        null_grad_val=null_grad_val) \
                             for t in slide_trigs]
    trig_bestnr[slide_id] = np.array(trig_bestnr[slide_id])
logging.info("Time, SNR, and BestNR of triggers extracted.")

# Calculate SNR and BestNR values and maxima
time_veto_max_snr = {}
time_veto_max_bestnr = {}
time_veto_max_snr_uncut = {}

for slide_id in range(num_slides):
    num_slide_segs = len(trial_dict[slide_id])
    time_veto_max_snr[slide_id] = np.zeros(num_slide_segs)
    time_veto_max_bestnr[slide_id] = np.zeros(num_slide_segs)
    time_veto_max_snr_uncut[slide_id] = np.zeros(num_slide_segs)

for slide_id in range(num_slides):
    for j, trial in enumerate(trial_dict[slide_id]):
        trial_cut = (trial[0] <= trig_time[slide_id])\
                          & (trig_time[slide_id] < trial[1])
        if not trial_cut.any():
            continue
        # Max SNR
        time_veto_max_snr[slide_id][j] = \
                        max(trig_snr[slide_id][trial_cut])
        # Max BestNR
        time_veto_max_bestnr[slide_id][j] = \
                        max(trig_bestnr[slide_id][trial_cut])
        # Max SNR for triggers passing SBVs
        sbv_cut = trig_bestnr[slide_id] != 0
        if not (trial_cut&sbv_cut).any():
            continue
        time_veto_max_snr_uncut[slide_id][j] =\
                          max(trig_snr[slide_id][trial_cut & sbv_cut])

logging.info("SNR and bestNR maxima calculated.")

# Output details of loudest offsouce triggers
offsource_trigs = []
for slide_id in range(num_slides):
    offsource_trigs.extend(zip(trig_bestnr[slide_id], sorted_trigs[slide_id]))
offsource_trigs.sort(key=lambda element: element[0])
offsource_trigs.reverse()

# ==========================
# Print loudest SNRs to file
# THIS OUTPUT FILE IS CURRENTLY UNUSED - MAYBE DELETE?
# Note: the only new info from above is the median SNR, bestnr
# and loudest SNR, so could just add this to the above's caption.
# ==========================
full_time_veto_max_snr = sort_stat(time_veto_max_snr)
full_time_veto_max_snr_uncut = sort_stat(time_veto_max_snr_uncut)
max_bestnr, _, full_time_veto_max_bestnr =\
    max_median_stat(num_slides, time_veto_max_bestnr, trig_bestnr, total_trials)


# =======================
# Load on source triggers
# =======================
if onsource_file:

    # Get trigs
    on_trigs = load_xml_table(onsource_file, "multi_inspiral")
    msg = "%d onsource triggers loaded." % (len(on_trigs))

    # Separate off chirp mass column
    on_mchirp = on_trigs.get_column('mchirp')

    # Set loudest event arrays
    loud_on_bestnr_trigs = None
    loud_on_bestnr = 0
    loud_on_fap = 1

    # Record loudest trig by SNR and by BestNR
    bin_trigs = sorted(np.asarray(on_trigs), key=lambda t: t.snr, reverse=True)
    bin_trigs.sort(key=lambda t: get_bestnr(t, q=chisq_index, n=chisq_nhigh,\
                                            null_thresh=null_thresh,\
                                            snr_threshold=snr_thresh,\
                                            sngl_snr_threshold=sngl_snr_thresh,\
                                            chisq_threshold=new_snr_thresh,\
                                            null_grad_thresh=null_grad_thresh,\
                                            null_grad_val=null_grad_val),\
                                            reverse=True)
    loud_on_bestnr_trigs = bin_trigs[0]
    loud_on_bestnr = get_bestnr(bin_trigs[0], q=chisq_index,\
                                null_thresh=null_thresh,\
                                snr_threshold=snr_thresh,\
                                sngl_snr_threshold=sngl_snr_thresh,\
                                chisq_threshold=new_snr_thresh,
                                n=chisq_nhigh,\
                                null_grad_thresh=null_grad_thresh,\
                                null_grad_val=null_grad_val)
    # If the loudest event has bestnr = 0, there is no event at all!
    if loud_on_bestnr == 0:
        loud_on_bestnr_trigs = None
        loud_on_bestnr = 0

    logging.info("Onsource analysed.")

    if loud_on_bestnr_trigs:
        trig = loud_on_bestnr_trigs
        num_trials_louder = 0
        tot_off_snr = np.array([])
        for slide_id in range(num_slides):
            num_trials_louder += sum(time_veto_max_bestnr[slide_id] > \
                                     loud_on_bestnr)
            tot_off_snr = np.concatenate([tot_off_snr,\
                                          time_veto_max_bestnr[slide_id]])
        fap = num_trials_louder/total_trials
        fap_test = sum(tot_off_snr > loud_on_bestnr)/total_trials
        pval = '< %.3g' % (1./total_trials) if fap == 0 else '%.3g' % fap
        loud_on_fap = fap

else:
    tot_off_snr = np.array([])
    for slide_id in range(num_slides):
        tot_off_snr = np.concatenate([tot_off_snr,\
                                      time_veto_max_bestnr[slide_id]])
    med_snr = np.median(tot_off_snr)
    fap = sum(tot_off_snr > med_snr)/total_trials

# =======================
# Post-process injections
# =======================
if do_injections:

    sites = [ifo[0] for ifo in ifos]

    # Triggers and injections recovered in some form
    found_trigs_nveto = MultiInspiralUtils.ReadMultiInspiralFromFiles([found_file])
    found_injs_no_veto = SimInspiralUtils.ReadSimInspiralFromFiles([found_file])

    found_trigs = lsctables.New(lsctables.MultiInspiralTable)
    found_injs = lsctables.New(lsctables.SimInspiralTable)

    for trig, sim in zip(found_trigs_nveto, found_injs_no_veto):
        if sim.get_end() not in vetoes.union(vetoes.keys()):
            found_injs.append(sim)
            found_trigs.append(trig)

    logging.info("Missed/found injections/triggers loaded.")

    # Extract columns of found injections and triggers
    found_inj_time = np.asarray(found_injs.get_column('geocent_end_time')) +\
                     np.asarray(found_injs.get_column('geocent_end_time_ns')*\
                                10**-9)
    found_inj_mchirp = np.asarray(found_injs.get_column('mchirp'))
    found_inj_mtot = np.asarray(found_injs.get_column('mtotal'))
    found_inj_m1 = np.asarray(found_injs.get_column('mass1'))
    found_inj_m2 = np.asarray(found_injs.get_column('mass2'))
    found_inj_eff_site_dist =\
        dict((ifo, found_injs.get_column('eff_dist_%s' % ifo.lower()))\
             for ifo in sites)
    found_inj_eff_dist = np.power(np.power(\
                                  np.asarray(found_inj_eff_site_dist.\
                                  values()), -1).sum(0), -1)
    found_inj_ra = np.asarray(found_injs.get_column('longitude'))
    found_inj_dec = np.asarray(found_injs.get_column('latitude'))
    found_inj_dist = np.asarray(found_injs.get_column('distance'))
    found_inj_inc = np.asarray(found_injs.get_column('inclination'))
    found_inj_spin1x = np.asarray(found_injs.get_column('spin1x'))
    found_inj_spin1y = np.asarray(found_injs.get_column('spin1y'))
    found_inj_spin1z = np.asarray(found_injs.get_column('spin1z'))
    found_inj_spin2x = np.asarray(found_injs.get_column('spin2x'))
    found_inj_spin2y = np.asarray(found_injs.get_column('spin2y'))
    found_inj_spin2z = np.asarray(found_injs.get_column('spin2z'))

    # TODO: a lot of this is identical to all the stuff above
    found_trig_mchirp = np.asarray(found_trigs.get_column('mchirp'))
    found_trig_m1 = np.asarray(found_trigs.get_column('mass1'))
    found_trig_m2 = np.asarray(found_trigs.get_column('mass2'))
    found_trig_ra = np.asarray(found_trigs.get_column('ra'))
    found_trig_dec = np.asarray(found_trigs.get_column('dec'))
    found_sky_angle = np.arccos(np.cos(found_inj_dec - found_trig_dec) -\
                                np.cos(found_inj_dec)* np.cos(found_trig_dec) *\
                                (1 - np.cos(found_inj_ra - found_trig_ra)))
    found_trig_snr = np.asarray(found_trigs.get_column('snr'))
    found_trig_chisq = np.asarray(found_trigs.get_column('chisq'))
    found_trig_bank = np.asarray(found_trigs.get_column('bank_chisq'))
    found_trig_auto = np.asarray(found_trigs.get_column('cont_chisq'))
    found_trig_null_snr = np.asarray(found_trigs.get_null_snr())
    found_trig_sngl_snr = dict((ifo, np.asarray(found_trigs.get_sngl_snr(ifo)))\
                                for ifo in ifos)

    # Grab values of detection statistic
    found_trig_bestnr = [get_bestnr(t, q=chisq_index, n=chisq_nhigh,\
                             null_thresh=null_thresh,\
                             snr_threshold=snr_thresh,\
                             sngl_snr_threshold=sngl_snr_thresh,\
                             chisq_threshold=new_snr_thresh,\
                             null_grad_thresh=null_grad_thresh,\
                             null_grad_val=null_grad_val) for t in found_trigs]
    found_trig_bestnr = np.asarray(found_trig_bestnr)

    # Construct conditions for injection:
    # 1) found louder than background,
    zero_fap = np.zeros(len(found_injs)).astype(np.bool)
    zero_fap_cut = found_trig_bestnr > max_bestnr
    zero_fap = zero_fap | (zero_fap_cut)

    # 2) found (bestnr > 0) but not louder than background (non-zero FAP)
    nonzero_fap = ~zero_fap & (found_trig_bestnr != 0)

    # 3) missed after being recovered (i.e., vetoed)
    missed = (~zero_fap) & (~nonzero_fap)

    # Separate triggers into:
    # 1) zero_fap 'g_found'
    # 2) nonzero_fap 'g_ifar'

    # Zero FAP
    g_found_time = found_inj_time[zero_fap]
    g_found_inc = found_inj_inc[zero_fap]

    # Non-zero FAP
    g_ifar_det_stat = found_trig_bestnr[nonzero_fap]
    g_ifar_stat = np.zeros([len(g_ifar_det_stat)])
    for ix, (mc, bestnr) in \
                enumerate(zip(found_trig_mchirp[nonzero_fap], g_ifar_det_stat)):
        g_ifar_stat[ix] = (full_time_veto_max_bestnr > bestnr).sum()
    g_ifar_stat = g_ifar_stat / total_trials
    # Statistics: missed-found
    MF = np.argsort(g_ifar_stat)
    # Statistics: found-missed
    FM = MF[::-1]

    # Set the sigma values
    inj_sigma = found_trigs.get_sigmasqs()
    # If the sigmasqs are not populated, we can still do calibration errors,
    # but only in the 1-detector case
    for ifo in ifos:
        if sum(inj_sigma[ifo] == 0):
            logging.info("%s: sigmasq not set for at least one trigger.", ifo)
        if sum(inj_sigma[ifo] != 0) == 0:
            logging.info("%s: sigmasq not set for any trigger.", ifo)
            if len(ifos) == 1:
                msg = "This is a single ifo analysis. "
                msg += "Setting sigmasq to unity for all triggers."
                logging.info(msg)
                inj_sigma[ifo][:] = 1.

    f_resp = dict((ifo, np.asarray([get_f_resp(inj)[ifo] \
                   for inj in found_injs]))  for ifo in ifos)

    inj_sigma_mult = (np.asarray(inj_sigma.values()) *\
                      np.asarray(f_resp.values()))

    inj_sigma_tot = inj_sigma_mult[0, :]
    for i in range(1, len(ifos)):
        inj_sigma_tot += inj_sigma_mult[i, :]

    inj_sigma_mean = {}
    for ifo in ifos:
        inj_sigma_mean[ifo] = ((inj_sigma[ifo]*f_resp[ifo])/inj_sigma_tot).mean()

    logging.info("%d found injections analysed.", len(found_injs))

    # Missed injections (ones not recovered at all)
    missed_injs = SimInspiralUtils.ReadSimInspiralFromFiles([missed_file])\
            .veto(vetoes.union(vetoes.keys()))

    # Process missed injections 'missed_inj'
    missed_inj_dist = np.asarray(missed_injs.get_column('distance'))
    missed_inj_time = np.asarray(missed_injs.get_column('geocent_end_time') +\
                                 missed_injs.get_column('geocent_end_time_ns') *\
                                 10**-9)
    missed_inj_inc = np.asarray(missed_injs.get_column('inclination'))

    logging.info("%d missed injections analysed.", len(missed_injs))

    # Create new set of injections for efficiency calculations
    total_injs = len(found_injs) + len(missed_injs)
    long_inj_dist = stats.uniform.rvs(size=total_injs) * (upper_dist-lower_dist) +\
                  upper_dist

    logging.info("%d long distance injections created.", total_injs)

    # Set distance bins and data arrays
    dist_bins = zip(np.arange(lower_dist, upper_dist + (upper_dist-lower_dist),\
                             (upper_dist-lower_dist)/num_bins),\
                   np.arange(lower_dist, upper_dist + (upper_dist-lower_dist),\
                             (upper_dist-lower_dist)/num_bins) +\
                             (upper_dist-lower_dist)/num_bins)

    num_injections = np.zeros([len(dist_bins)+1])
    found_max_bestnr = np.zeros([len(dist_bins)+1])
    found_on_bestnr = np.zeros([len(dist_bins)+1])
    num_injections_no_mc = np.zeros([len(dist_bins)+1])
    found_max_bestnr_no_mc = np.zeros([len(dist_bins)+1])
    found_on_bestnr_no_mc = np.zeros([len(dist_bins)+1])

    # Construct FAP list for all found injections
    inj_fap = np.zeros(len(found_injs))
    inj_fap[nonzero_fap] = g_ifar_stat

    # Calculate the amplitude error
    # Begin by calculating the components from each detector
    cal_error = 0
    for ifo in ifos:
        cal_error += cal_errs[ifo]**2 * inj_sigma_mean[ifo]**2
    cal_error = cal_error**0.5

    max_dc_cal_error = max(cal_dc_errs.values())

    # Calibration phase uncertainties are neglected
    logging.info("Calibration amplitude uncertainty calculated.")

    # Now create the numbers for the efficiency plots; these include calibration
    # and waveform errors. These are incorporated by running over each injection
    # num_mc_injs times, where each time we draw a random value of distance

    # Distribute injections
    found_inj_dist_mc = np.ndarray((num_mc_injs+1, len(found_injs)))
    found_inj_dist_mc[0, :] = found_inj_dist
    missed_inj_dist_mc = np.ndarray((num_mc_injs+1, len(missed_injs)))
    missed_inj_dist_mc[0, :] = missed_inj_dist
    long_inj_dist_mc = np.ndarray((num_mc_injs+1, total_injs))
    long_inj_dist_mc[0, :] = long_inj_dist
    for i in range(num_mc_injs):
        # TODO: these is a copy and paste of 3 lines of code
        cal_dist_red = stats.norm.rvs(size=len(found_injs)) * cal_error
        wav_dist_red = np.abs(stats.norm.rvs(size=len(found_injs)) * wav_err)
        found_inj_dist_mc[i+1, :] = found_inj_dist / (max_dc_cal_error * \
                                     (1 + cal_dist_red) * (1 + wav_dist_red))
        cal_dist_red = stats.norm.rvs(size=len(missed_injs)) * cal_error
        wav_dist_red = np.abs(stats.norm.rvs(size=len(missed_injs)) * wav_err)
        missed_inj_dist_mc[i+1, :] = missed_inj_dist / (max_dc_cal_error *\
                                     (1 + cal_dist_red) * (1 + wav_dist_red))
        cal_dist_red = stats.norm.rvs(size=total_injs) * cal_error
        wav_dist_red = np.abs(stats.norm.rvs(size=total_injs) * wav_err)
        long_inj_dist_mc[i+1, :] = long_inj_dist / (max_dc_cal_error *\
                                     (1 + cal_dist_red) * (1 + wav_dist_red))

    logging.info("MC injection set distributed with %d iterations.",\
                 num_mc_injs)

    # Check injections against on source
    more_sig_than_onsource = np.ndarray(len(found_injs))
    if onsource_file:
        more_sig_than_onsource = (inj_fap <= loud_on_fap)
    else:
        more_sig_than_onsource = (inj_fap <= 0.5)

    distance_count = np.zeros(len(dist_bins))

    found_trig_max_bestnr = np.zeros([len(found_trig_mchirp)])
    for ix, mc in enumerate(found_trig_mchirp):
        found_trig_max_bestnr[ix] = max_bestnr

    max_bestnr_cut = (found_trig_bestnr > found_trig_max_bestnr)

    # Check louder than on source
    if onsource_file:
        # TODO: basically the same code as above
        found_trig_loud_on_bestnr = np.zeros([len(found_trig_mchirp)])
        for ix, mc in enumerate(found_trig_mchirp):
            found_trig_loud_on_bestnr[ix] = loud_on_bestnr
    else:
        found_trig_loud_on_bestnr = np.zeros([len(found_trig_mchirp)])
        for ix, mc in enumerate(found_trig_mchirp):
            found_trig_loud_on_bestnr[ix] = med_snr
    on_bestnr_cut = found_trig_bestnr > found_trig_loud_on_bestnr

    # Check whether injection is found for the purposes of exclusion
    # distance calculation.
    # Found: if louder than all on source
    # Missed: if not louder than loudest on source
    found_excl = on_bestnr_cut & (more_sig_than_onsource) & \
                (found_trig_bestnr != 0)
    # If not missed, double check bestnr against nearby triggers
    near_test = np.zeros((found_excl).sum()).astype(bool)
    for j, (t, bestnr) in enumerate(zip(found_inj_time[found_excl],\
                                        found_trig_bestnr[found_excl])):
        near_bestnr = trig_bestnr[zero_lag_slide_id]\
                      [np.abs(trig_time[zero_lag_slide_id]-t) < cluster_window]
        near_test[j] = ~((near_bestnr * glitch_check_fac > bestnr).any())
    # Apply the local test
    # FIXME: putmask does not seem to work...
    #np.putmask(found_excl, found_excl==True, near_test)
    c = 0
    for z, b in enumerate(found_excl):
        if found_excl[z]:
            found_excl[z] = near_test[c]
            c += 1

    # Loop over each random instance of the injection set
    for k in range(num_mc_injs+1):
        # Loop over the distance bins
        for j, dist_bin in enumerate(dist_bins):
            # Construct distance cut
            found_dist_cut = (dist_bin[0] <= found_inj_dist_mc[k, :]) &\
                             (found_inj_dist_mc[k, :] < dist_bin[1])
            missed_dist_cut = (dist_bin[0] <= missed_inj_dist_mc[k, :]) &\
                              (missed_inj_dist_mc[k, :] < dist_bin[1])
            long_dist_cut = (dist_bin[0] <= long_inj_dist_mc[k, :]) &\
                            (long_inj_dist_mc[k, :] < dist_bin[1])

            # Count all injections in this distance bin
            num_found_pass = (found_dist_cut).sum()
            num_missed_pass = (missed_dist_cut).sum()
            num_long_pass = long_dist_cut.sum() or 0
            # Count only zero FAR injections
            num_zero_far = (found_dist_cut & max_bestnr_cut).sum()
            # Count number found for exclusion
            num_excl = (found_dist_cut & (found_excl)).sum()

            # Record number of injections, number found for exclusion
            # and number of zero FAR
            # TODO: serial coding (and redundant items?)
            if k == 0:
                num_injections_no_mc[j] += num_found_pass + num_missed_pass +\
                                           num_long_pass
                num_injections_no_mc[-1] += num_found_pass + num_missed_pass +\
                                            num_long_pass
                found_max_bestnr_no_mc[j] += num_zero_far
                found_max_bestnr_no_mc[-1] += num_zero_far
                found_on_bestnr_no_mc[j] += num_excl
                found_on_bestnr_no_mc[-1] += num_excl
            else:
                num_injections[j] += num_found_pass + num_missed_pass +\
                                     num_long_pass
                num_injections[-1] += num_found_pass + num_missed_pass +\
                                      num_long_pass
                found_max_bestnr[j] += num_zero_far
                found_max_bestnr[-1] += num_zero_far
                found_on_bestnr[j] += num_excl
                found_on_bestnr[-1] += num_excl

    logging.info("Found/missed injection efficiency calculations completed.")

    # Get start and end times
    start = int(min(np.concatenate((found_inj_time, missed_inj_time))))
    end = int(max(np.concatenate((found_inj_time, missed_inj_time))))
    duration = end - start
    # GRB start time
    grb_time = segs['on'][1] - 1
    # Pad times and reset to centre on zero
    start = start - duration*0.05 - grb_time
    end = end + duration*0.05 - grb_time
    missed_inj_time = missed_inj_time - grb_time
    g_found_time -= grb_time

# Post-processing of injections ends here

# ==========
# Make plots
# ==========

if do_injections:
    # Calculate distances (horizontal axis) as means
    dist_plot_vals = [np.asarray(dist_bin).mean() for dist_bin in dist_bins]

    # Calculate error bars for efficiency/distance plots and datafiles
    # using max BestNR of background
    yerr_low_mc, yerr_high_mc, fraction_mc = efficiency_with_errs(\
         found_max_bestnr, num_injections, num_mc_injs=num_mc_injs)
    yerr_low_no_mc, yerr_high_no_mc, fraction_no_mc = efficiency_with_errs(\
                               found_max_bestnr_no_mc, num_injections_no_mc)

    # Calculate and save to disk the 50% sensitive distance
    eff_low = fraction_no_mc
    eff_idx = np.where(eff_low < 0.5)[0]
    if eff_idx.size == 0:
        sens_dist = -1
        err_msg = "Efficiency does not drop below 50%!"
        logging.error(err_msg)
    elif eff_idx[0] == 0:
        sens_dist = 0
        err_msg = "Efficiency does not drop below 90%!"
        logging.error(err_msg)
    else:
        i = eff_idx[0]
        d = dist_plot_vals[i]
        d_low = dist_plot_vals[i-1]
        e = eff_low[i]
        e_low = eff_low[i-1]
        # TODO: insert this in the output pages
        sens_dist = d + (e - 0.5) * (d - d_low) / (e_low - e)

    # Plot efficiency using loudest background
    fig = plt.figure()
    ax = fig.gca()
    ax.plot(dist_plot_vals, (fraction_no_mc), 'g-',
            label='No marginalisation')
    ax.errorbar(dist_plot_vals, (fraction_no_mc),
                yerr=[yerr_low_no_mc, yerr_high_no_mc], c='g')
    marg_eff = fraction_mc
    if not np.isnan(marg_eff.sum()):
        ax.plot(dist_plot_vals, marg_eff, 'r-', label='Marginalised')
        ax.errorbar(dist_plot_vals, marg_eff, yerr=[yerr_low_mc, yerr_high_mc],
                    c='r')
    ax.legend()
    ax.grid()
    ax.set_ylim([0, 1])
    ax.set_xlim(0, 2.*upper_dist - lower_dist)
    ax.set_ylabel("Fraction of injections found louder than loudest background")
    ax.set_xlabel("Distance (Mpc)")
    plot_title = "Detection efficiency - "+inj_set_name
    plot_caption = "Injection recovery efficiency using "
    plot_caption += "BestNR as detection statistic.  "
    plot_caption += "Injections louder than loudest background trigger."
    fig_path = opts.background_output_file
    save_fig_with_metadata(fig, fig_path, cmd=' '.join(sys.argv),
                           title=plot_title, caption=plot_caption)
    plt.close()

    # Calculate error bars for efficiency/distance plots and datafiles
    # using max BestNR of foreground
    yerr_low_no_mc, yerr_high_no_mc, fraction_no_mc = efficiency_with_errs(\
                                found_on_bestnr_no_mc, num_injections_no_mc)
    yerr_low, yerr_high, fraction_mc = efficiency_with_errs(found_on_bestnr,\
                                     num_injections, num_mc_injs=num_mc_injs)

    # Marginalized efficiency (isf = inverse survival function)
    red_efficiency = (fraction_mc) - (yerr_low) * scipy.stats.norm.isf(0.1)

    # Calculate and save to disk 50% and 90% exclusion distances
    for percentile in [50, 90]:
        eff_idx = np.where(red_efficiency < (percentile / 100.))[0]
        if eff_idx.size == 0:
            green_efficiency = (fraction_no_mc)
            excl_efficiency = green_efficiency
            eff_idx = np.where(green_efficiency < (percentile / 100.))[0]
        else:
            excl_efficiency = red_efficiency
        if eff_idx.size and eff_idx[0] != 0:
            i = eff_idx[0]
            d = dist_plot_vals[i]
            d_low = dist_plot_vals[i-1]
            e = excl_efficiency[i]
            e_low = excl_efficiency[i-1]
            excl_dist = d + (e - (percentile / 100.)) * (d - d_low) /\
                    (e_low - e)
        else:
            excl_dist = 0
            err_msg = "Efficiency below %d%% in first bin!" % (percentile)
            logging.error(err_msg)
        # TODO: include percentile, excl_dist on output pages

    # Plot efficiency using loudest foreground
    fig = plt.figure()
    ax = fig.gca()
    ax.grid()
    ax.plot(dist_plot_vals, (fraction_no_mc), 'g-',
            label='No marginalisation')
    ax.errorbar(dist_plot_vals, (fraction_no_mc),
                yerr=[yerr_low_no_mc, yerr_high_no_mc], c='g')
    marg_eff = fraction_mc
    if not np.isnan(marg_eff.sum()):
        ax.plot(dist_plot_vals, marg_eff, 'r-', label='Marginalised')
        ax.errorbar(dist_plot_vals, marg_eff, yerr=[yerr_low, yerr_high], c='r')
    if not np.isnan(red_efficiency.sum()):
        ax.plot(dist_plot_vals, red_efficiency, 'm-',
                label='Inc. counting errors')
    ax.set_ylim([0, 1])
    ax.grid()
    ax.legend()
    ax.get_legend().get_frame().set_alpha(0.5)
    ax.grid()
    ax.set_ylim([0, 1])
    ax.set_xlim(0, 2.*upper_dist - lower_dist)
    ax.set_ylabel("Fraction of injections found louder than "+\
                  "loudest foreground")
    ax.set_xlabel("Distance (Mpc)")
    ax.plot([excl_dist], [0.9], 'gx')
    ax.set_ylim([0, 1])
    ax.set_xlim(0, 2.*upper_dist - lower_dist)
    plot_title = "Exclusion distance - "+inj_set_name
    plot_caption = "Injection recovery efficiency using "
    plot_caption += "BestNR as detection statistic.  "
    plot_caption += "Injections louder than loudest foreground trigger"
    fig_path = opts.onsource_output_file
    save_fig_with_metadata(fig, fig_path, cmd=' '.join(sys.argv),
                           title=plot_title, caption=plot_caption)
    plt.close()

logging.info("Plots complete.")
