#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Duncan Macleod
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""Combine triggers from a splitbank GRB run
"""

import argparse
import os
from collections import defaultdict

import tqdm

import h5py

from astropy.constants import M_sun

from pycbc import __version__

MSUN_SI = M_sun.si.value

__author__ = "Duncan Macleod <duncan.macleod@ligo.org>"

TQDM_BAR_FORMAT = ("{desc}: |{bar}| "
                   "{n_fmt}/{total_fmt} {unit} ({percentage:3.0f}%) "
                   "[{elapsed} | ETA {remaining}]")
TQDM_KW = {
    "ascii": " -=#",
    "bar_format": TQDM_BAR_FORMAT,
    "smoothing": 0.05,
}


# -- utilties -----------------------------------

def merge_injection_files(inputfiles, outputfile, verbose=False,
                          **compression_kw):
    """Merge several HDF5 injections files into a single file.

    Parameters
    ----------
    inputfiles : `list` of `str`
        the paths of the input HDF5 files to merge

    outputfile : `str`
        the path of the output HDF5 file to write
    """
    attributes = {}
    datasets = {}
    total = defaultdict(int)

    nfiles = len(inputfiles)

    def _scan_file(name, obj):
        """Record parameters for a given dataset
        """
        # if this is an injection group determine how many injections there are
        if isinstance(obj, h5py.Group) and name in ("found", "missed"):
            total[name] += obj["mass1"].shape[0]

        # if this is just an individual dataset, record its parameters
        if isinstance(obj, h5py.Dataset):
            datasets[name] = {attr: getattr(obj, attr) for attr in (
                "compression",
                "compression_opts",
                "dtype",
            )}

    # loop once over all files, recording information
    for filename in tqdm.tqdm(inputfiles, desc="Scanning trigger files",
                              disable=not verbose, total=nfiles, unit="files",
                              **TQDM_KW):
        with h5py.File(filename, 'r') as h5f:
            attributes = dict(h5f.attrs)
            h5f.visititems(_scan_file)

    # print summary of what we found
    if verbose:
        print("{} found injections and {} missed injections".format(
            total["found"], total["missed"],
        ))

    position = defaultdict(int)
    eventcount = 0

    with h5py.File(outputfile, 'w') as h5out:
        h5out.attrs.update(attributes)

        # create datasets
        for dset, params in datasets.items():
            size = total["missed"] if dset.startswith("missed")\
                else total["found"]
            h5out.create_dataset(dset, shape=(size,), **params)

        # copy dataset contents
        for filename in tqdm.tqdm(inputfiles, desc="Merging trigger files",
                                  disable=not verbose, total=nfiles,
                                  unit="files", **TQDM_KW):
            with h5py.File(filename, 'r') as h5in:
                for dset in datasets:
                    data = h5in[dset][:][:]
                    size = data.shape[0]
                    pos = position[dset]
                    if dset == "network/network_event_id":
                        # increment event_id by the _total_ number of events
                        # in the file (not just the ones we are using)
                        data += eventcount
                        eventcount += h5in[dset].shape[0]
                    h5out[dset][pos:pos+size] = data
                    position[dset] += size

    if verbose:
        print("Merged triggers written to {}".format(outputfile))
    return outputfile


# -- parse command line -------------------------

parser = argparse.ArgumentParser(
    description=__doc__,
)

parser.add_argument(
    "-v",
    "--verbose",
    action="store_true",
    default=False,
    help="print verbose output (default: %(default)s)",
)
parser.add_argument(
    "-V",
    "--version",
    action="version",
    version=__version__,
    help="show version number and exit",
)

# input/output
parser.add_argument(
    "-f",
    "--input-files",
    nargs="*",
    required=True,
    metavar="INJECTION_TRIGGER_FILE",
    help="read in listed trigger files",
)
parser.add_argument(
    "-o",
    "--output-file",
    required=True,
    default=os.getcwd(),
    help="output directory (default: %(default)s)",
)

args = parser.parse_args()

# merge injection files
merge_injection_files(
    args.input_files,
    args.output_file,
    verbose=args.verbose,
)
