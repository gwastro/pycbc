#!/usr/bin/env python
# coding=utf-8
"""
Make workflow for the archival, targeted, coherent inspiral pipeline.
***IN DEVELOPMENT***
"""

import pycbc.version

__author__ = "Andrew Williamson <andrew.williamson@ligo.org>"
__version__ = pycbc.version.git_verbose_msg
__date__ = pycbc.version.date
__program__ = "pygrb"

import shutil
import os
import argparse
import logging
import pycbc.workflow as _workflow
from glue.segments import segment
from glue.ligolw import ligolw,lsctables,utils
from glue.ligolw.utils import ligolw_add 

def set_start_end(cp, a, b):
    """
    Function to update analysis boundaries as workflow is generated
    """
    cp.set("workflow", "start-time", str(a))
    cp.set("workflow", "end-time", str(b))
    return cp


def get_coh_PTF_files(cp, ifos, run_dir, bank_veto=False, summary_files=False):
    """
    Retrieve the bank_veto_bank.xml file needed to run coh_PTF_inspiral jobs.
    """
    if os.getenv("LAL_SRC") is None:
        raise ValueError("The environment variable LAL_SRC must be set to a "
                         "location containing the file lalsuite.git")
    else:
        lalDir = os.getenv("LAL_SRC")
        sci_seg = segment(int(cp.get("workflow", "start-time")),
                          int(cp.get("workflow", "end-time")))
        file_list = _workflow.FileList([])

        # Bank veto
        if bank_veto:
            shutil.copy("%s/lalapps/src/ring/coh_PTF_config_files/" \
                        "bank_veto_bank.xml" % lalDir, "%s" % run_dir)
            bank_veto_url = "file://localhost%s/bank_veto_bank.xml" % run_dir
            bank_veto = _workflow.File(ifos, "bank_veto_bank",
                                       sci_seg, file_url=bank_veto_url)
            bank_veto.PFN(bank_veto.cache_entry.path, site="local")
            file_list.extend(_workflow.FileList([bank_veto]))

        if summary_files:
            # summary.js file
            shutil.copy("%s/lalapps/src/ring/coh_PTF_config_files/" \
                        "coh_PTF_html_summary.js" % lalDir, "%s" % run_dir)
            summary_js_url = "file://localhost%s/coh_PTF_html_summary.js" \
                             % run_dir
            summary_js = _workflow.File(ifos, "coh_PTF_html_summary_js",
                                       sci_seg, file_url=summary_js_url)
            summary_js.PFN(summary_js.cache_entry.path, site="local")
            file_list.extend(_workflow.FileList([summary_js]))

            # summary.css file
            shutil.copy("%s/lalapps/src/ring/coh_PTF_config_files/" \
                        "coh_PTF_html_summary.css" % lalDir, "%s" % run_dir)
            summary_css_url = "file://localhost%s/coh_PTF_html_summary.css" \
                              % run_dir
            summary_css = _workflow.File(ifos, "coh_PTF_html_summary_css",
                                         sci_seg, file_url=summary_css_url)
            summary_css.PFN(summary_css.cache_entry.path, site="local")
            file_list.extend(_workflow.FileList([summary_css]))

        return file_list


def make_exttrig_file(cp, ifos, sci_seg, out_dir):
    '''
    Make ExtTrig xml file
    '''
    xmldoc = ligolw.Document()
    xmldoc.appendChild(ligolw.LIGO_LW())
    tbl = lsctables.New(lsctables.ExtTriggersTable)
    xmldoc.childNodes[-1].appendChild(tbl)
    row = lsctables.ExtTriggersTable()
    row.process_id = None 
    row.det_alts = None 
    row.det_band = None 
    row.det_fluence = None 
    row.det_fluence_int = None 
    row.det_name = None 
    row.det_peak = None 
    row.det_peak_int = None 
    row.det_snr = '' 
    row.email_time = 0 
    row.event_dec = 0.0 
    row.event_dec_err = 0.0 
    row.event_epoch = '' 
    row.event_err_type = '' 
    row.event_ra_err = 0.0 
    row.start_time_ns = 0 
    row.event_type = '' 
    row.event_z = 0.0 
    row.event_z_err = 0.0 
    row.notice_comments = '' 
    row.notice_id = '' 
    row.notice_sequence = '' 
    row.notice_time = 0 
    row.notice_type = '' 
    row.notice_url = '' 
    row.obs_fov_dec = 0.0 
    row.obs_fov_dec_width = 0.0 
    row.obs_fov_ra = 0.0 
    row.obs_fov_ra_width = 0.0 
    row.obs_loc_ele = 0.0 
    row.obs_loc_lat = 0.0 
    row.obs_loc_long = 0.0 
    row.ligo_fave_lho = 0.0 
    row.ligo_fave_llo = 0.0 
    row.ligo_delay = 0.0 
    row.event_status = 0 
    row.event_ra = float(cp.get("workflow", "ra"))
    row.event_dec = float(cp.get("workflow", "dec"))
    row.start_time = int(cp.get("workflow", "trigger-time"))
    row.event_number_gcn = 9999
    row.event_number_grb = str(cp.get("workflow", "trigger-name"))
    tbl.extend([row])
    xml_file_name = "triggerGRB%s.xml" % str(cp.get("workflow",
                                                    "trigger-name"))
    xml_file_path = os.path.join(out_dir, xml_file_name)
    utils.write_filename(xmldoc, xml_file_path)
    xml_file_url = "file://localhost%s/%s" % (out_dir, xml_file_name)
    xml_file = _workflow.File(ifos, xml_file_name, sci_seg, file_url=xml_file_url)
    xml_file.PFN(xml_file.cache_entry.path, site="local")
    
    return xml_file


def make_cache(input_files, ifos, name, seg, cacheDir, tags=[]):
    """
    Place input_files into a cache file.
    """
    cache_file = _workflow.File(ifos, name, seg, extension="lcf",
                                directory=cacheDir, tags=tags)
    cache_file.PFN(cache_file.cache_entry.path, site="local")

    # Dump output to file
    fP = open(cache_file.storage_path, "w")
    for entry in input_files:
        start = str(int(seg[0]))
        duration = str(int(abs(seg)))
        print >> fP, "%s %s %s %s file://localhost%s" \
            %(ifos, entry.description.upper(), start, duration,
              entry.storage_path)
    fP.close()
    
    return cache_file


###################################
# Workflow generation starts here #
###################################

workflow_name = __program__
logging.basicConfig(format="%(asctime)s:%(levelname)s : %(message)s",
                    level=logging.INFO)

# Parse command line options and instantiate pycbc workflow object
parser = argparse.ArgumentParser()
parser.add_argument("--version", action="version", version=__version__)
parser.add_argument("-d", "--output-dir", default=None,
                    help="Path to output directory.")
_workflow.add_workflow_command_line_group(parser)
args = parser.parse_args()
wflow = _workflow.Workflow(args, workflow_name)
all_files = _workflow.FileList([])
tags = []

# Setup run directory
if args.output_dir:
    baseDir = args.output_dir
else:
    baseDir = os.getcwd()
runDir = os.path.join(baseDir, "GRB%s" % str(wflow.cp.get("workflow",
                                                          "trigger-name")))
if not os.path.exists(runDir):
    os.makedirs(runDir)
os.chdir(runDir)

# SEGMENTS
# Hack to set start and end times based on maximum allowed duration
start = int(wflow.cp.get("workflow", "trigger-time")) - int(wflow.cp.get(
        "workflow-exttrig_segments", "max-duration"))
end = int(wflow.cp.get("workflow", "trigger-time")) + int(wflow.cp.get(
        "workflow-exttrig_segments", "max-duration"))
wflow.cp = set_start_end(wflow.cp, start, end)

# Retrieve segments ahope-style
currDir = os.getcwd()
segDir = os.path.join(currDir, "segments")
sciSegs, segsFileList = _workflow.setup_segment_generation(wflow, segDir)

# Make coherent network segments
onSrc, sciSegs = _workflow.get_triggered_coherent_segment(wflow, segDir,
                                                          sciSegs)
# FIXME: The following lines are crude hacks.
ifo = sciSegs.keys()[0]
padding = int(wflow.cp.get("inspiral", "pad-data"))
deadtime = int(wflow.cp.get("inspiral", "segment-duration")) / 4
wflow.analysis_time = segment(int(sciSegs[ifo][0][0]) + deadtime + padding,
                              int(sciSegs[ifo][0][1]) - deadtime - padding)
wflow.cp = set_start_end(wflow.cp, int(sciSegs[ifo][0][0]),
                         int(sciSegs[ifo][0][1]))
ext_file = None

# DATAFIND
dfDir = os.path.join(currDir, "datafind")
datafind_files, sciSegs = _workflow.setup_datafind_workflow(wflow, sciSegs,
                                                            dfDir,
                                                            segsFileList)
ifo = sciSegs.keys()[0]
ifos = ''.join(sciSegs.keys())

# If using coh_PTF_inspiral we need bank_veto_bank.xml
if os.path.basename(wflow.cp.get("executables", "inspiral")) \
                    == "lalapps_coh_PTF_inspiral":
    datafind_veto_files = _workflow.FileList([])
    bank_veto_file = get_coh_PTF_files(wflow.cp, ifos, runDir, bank_veto=True)
    datafind_veto_files.extend(datafind_files)
    datafind_veto_files.extend(bank_veto_file)
    
    # Make ExtTrig xml file (needed for lalapps_inspinj and summary pages)
    ext_file = make_exttrig_file(wflow.cp, ifos, sciSegs[ifo][0], baseDir)
    all_files.extend(_workflow.FileList([ext_file]))

all_files.extend(datafind_veto_files)

# TEMPLATE BANK AND SPLIT BANK
# TODO: Move from pregenerated to generated coherent network bank
bank_files = _workflow.setup_tmpltbank_workflow(wflow, sciSegs,
                                                datafind_files, dfDir)
splitbank_files = _workflow.setup_splittable_workflow(wflow, bank_files, dfDir,
                                                      tags=["inspiral"])
all_files.extend(bank_files)
all_files.extend(splitbank_files)

# INJECTIONS
inj_files = None
inj_caches = None
inj_insp_files = None
inj_insp_caches = None
if wflow.cp.has_section("workflow-injections"):
    injDir = os.path.join(currDir, "injections")
    inj_caches = _workflow.FileList([])
    inj_insp_caches = _workflow.FileList([])

    # Generate injection files
    inj_files, inj_tags = _workflow.setup_injection_workflow(wflow, injDir,
            exttrig_file=ext_file)
    all_files.extend(inj_files)
    injs = inj_files

    # Either split template bank for injections jobs or use same split banks
    # as for standard matched filter jobs
    if wflow.cp.has_section("workflow-splittable-injections"):
        inj_splitbank_files = _workflow.setup_splittable_workflow(wflow,
                bank_files, injDir, tags=["injections"])
        for inj_split in inj_splitbank_files:
            inj_split.description += '%s_%d' \
                    % (inj_split.tag_str,
                       int(inj_split.name.split('_')[-2][4:]))
        all_files.extend(inj_splitbank_files)
    else:
        inj_splitbank_files = _workflow.FileList(splitbank_files)

    # Split the injection files
    if wflow.cp.has_section("workflow-splittable-split_inspinj"):
        inj_split_files = _workflow.FileList([])
        for inj_file, inj_tag in zip(inj_files, inj_tags):
            file = _workflow.FileList([inj_file])
            inj_splits = _workflow.setup_splittable_workflow(wflow, file,
                    injDir, tags=["split_inspinj", inj_tag])
            for inj_split in inj_splits:
                inj_split.description += '_%s_%d' \
                        % (inj_tag,
                           int(inj_split.name.split('_')[-1].split('-')[0][5:]))
            inj_split_files.extend(inj_splits)
        all_files.extend(inj_split_files)
        injs = inj_split_files

    # Generate injection matched filter workflow
    inj_insp_files = _workflow.setup_matchedfltr_workflow(wflow, sciSegs,
            datafind_veto_files, inj_splitbank_files, injDir, injs)
    for inj_insp_file in inj_insp_files:
        inj_insp_file.description += '_%d' \
                % int(inj_insp_file.name.split('_')[-3][5:])

    # Make cache files (needed for post-processing)
    for inj_tag in inj_tags:
        files = _workflow.FileList([file for file in injs \
                                    if inj_tag in file.tag_str])
        inj_cache = make_cache(files, ifos, "injections", sciSegs[ifo][0],
                               injDir, tags=[inj_tag])
        inj_caches.extend(_workflow.FileList([inj_cache]))

        files = _workflow.FileList([file for file in inj_insp_files \
                                    if inj_tag in file.tag_str])
        inj_insp_cache = make_cache(files, ifos, "inspiral_injections",
                                    sciSegs[ifo][0], injDir, tags=[inj_tag])
        inj_insp_caches.extend(_workflow.FileList([inj_insp_cache]))

    all_files.extend(inj_caches)
    all_files.extend(inj_insp_files)
    all_files.extend(inj_insp_caches)

# MAIN MATCHED FILTERING
# TODO: Write coherent matched filtering code
inspDir = os.path.join(currDir, "inspiral")
inspiral_files = _workflow.setup_matchedfltr_workflow(wflow, sciSegs,
                                                      datafind_veto_files,
                                                      splitbank_files, inspDir)
inspiral_cache = make_cache(inspiral_files, ifos, "inspiral", sciSegs[ifo][0],
                            inspDir)
inspiral_files.extend(_workflow.FileList([inspiral_cache]))
all_files.extend(inspiral_files)

# POST-PROCESSING
ppDir = os.path.join(currDir, "post_processing")
post_proc_method = wflow.cp.get_opt_tags("workflow-postproc",
                                         "postproc-method", tags)

if post_proc_method == "COH_PTF_WORKFLOW":
    # Add parsed config file so it can be linked from summary page
    cp_file_name = "%s_parsed.ini" % workflow_name
    cp_file_url = "file://localhost%s/%s" % (baseDir, cp_file_name)
    cp_file = _workflow.File(ifos, cp_file_name, sciSegs[ifo][0],
                             file_url=cp_file_url)
    cp_file.PFN(cp_file.cache_entry.path, site="local")


    # Generate post-processing workflow
    pp_files = _workflow.setup_coh_PTF_post_processing(wflow, inspiral_files,
            ppDir, segDir, injection_trigger_files=inj_insp_files,
            injection_files=injs, injection_trigger_caches=inj_insp_caches,
            injection_caches=inj_caches, config_file=cp_file, ifos=ifos,
            inj_tags=inj_tags)

    # Retrieve style files for webpage
    summary_files = get_coh_PTF_files(wflow.cp, ifos, ppDir,
                                      summary_files=True)

    pp_files.extend(_workflow.FileList([cp_file]))
    pp_files.extend(summary_files)

all_files.extend(pp_files)

# COMPILE WORKFLOW AND WRITE DAX
wflow.save()
logging.info("Written dax.")

