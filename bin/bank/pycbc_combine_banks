#!/usr/bin/env python

"""
Simple script to combine banks used in the pycbc search,
this is similar to sbank_hdf5_bankcombiner, but allows for
compressed waveforms to be combined as well
"""

import numpy as np
import argparse
import logging

from pycbc import init_logging, add_common_pycbc_options
from pycbc.io import HFile


def compare_dict_to_cached(new_dict, new_fname, cached_dict, keys_only=False):
    """
    Helper to make sure that the keys of two dictrionaries match
    """
    cached_keys = set(cached_dict.keys())
    new_keys = set(new_dict.keys())
    # This does the XOR check of the two sets of keys.
    # Basically we demand that the two files must have the same items.
    if set(cached_keys).symmetric_difference(new_keys):
        err_msg = "All input files must contain the same data structures. "
        err_msg += "File {} ".format(new_fname)
        err_msg += "contains fields {} ".format(new_keys)
        err_msg += "other files contain {}.".format(cached_keys)
        raise ValueError(err_msg)
    if keys_only:
        return
    # Now we compare the values in the dictionary to the cached one.
    # Keys must match now so it shouldnt matter which key list we use now
    for k, v in cached_dict.items():
        logging.debug("Comparing %s", k)
        if not all([n == o for n, o in zip(new_dict[k], v)]):
            raise ValueError(
                "Attribute values must match between files; attribute "
                f"{k} vas value {v} cached from previous files, but "
                f"{new_dict[k]} in {new_fname}"
            )


parser = argparse.ArgumentParser()
add_common_pycbc_options(parser)
parser.add_argument(
    "--output-file",
    help="Combined bank hdf file to output."
)
parser.add_argument(
    "--input-filenames",
    nargs='*',
    help="List of input hdf bank files"
)
args = parser.parse_args()

init_logging(args.verbose, default_level=1)

attrs_dict = None
dsets_dict = None
logging.info("Checking file structure/attributes and copying raw bank values")
# Check attributes and bank structure before copying over the
# compressed waveforms, this ensures a fast fail if the config is wrong
n_banks = len(args.input_filenames)
for i, file_name in enumerate(args.input_filenames):
    logging.debug("Bank %s; %d / %d", file_name, i, n_banks)
    with HFile(file_name, 'r') as hdf_fp:
        logging.debug("File open")
        attrs_new = {}
        for key, item in hdf_fp.attrs.items():
            attrs_new[key] = item
        logging.debug("Got attributes")
        dsets_new = {}
        for key, item in hdf_fp.items():
            dsets_new[key] = list(item[:])
        logging.debug("Got contents")
    if attrs_dict is None:
        attrs_dict = attrs_new
    else:
        compare_dict_to_cached(attrs_new, file_name, attrs_dict)
        logging.debug("Attributes the same")

    if dsets_dict is None:
        dsets_dict = dsets_new
    else:
        compare_dict_to_cached(dsets_new, file_name, dsets_dict, keys_only=True)
        logging.debug("File structure the same, combining")
        for k, item in dsets_new.items():
            if k == "compressed_waveforms":
                continue
            dsets_dict[k] += list(item)

if 'template_hash' in dsets_dict:
    logging.info("Re-sorting by template hash")
    hash_order = np.argsort(dsets_dict['template_hash'])
    for k, v in dsets_dict.items():
        dsets_dict[k] = np.array(v)[hash_order]

logging.info("Adding datasets to the output bank")
outname = HFile(args.output_file, 'w')
for k, attr_val in attrs_dict.items():
    outname.attrs[k] = attr_val

for item, entry in dsets_dict.items():
    outname.create_dataset(item, data=entry)

print(list(dsets_dict.keys()))
if 'compressed_waveforms' not in dsets_dict:
    outname.close()
    logging.info("Done")
    exit(0)

out_compressed_group = outname.create_group('compressed_waveforms')

logging.info("Getting compressed waveforms")
for i, file_name in enumerate(args.input_filenames):
    logging.info(
        "Getting compressed waveforms from bank file %s; %d / %d",
        file_name,
        i,
        n_banks
    )
    with HFile(file_name, 'r') as hdf_fp:
        for hsh in bank_in['compressed_waveforms'].keys():
            hdf_fp.copy('compressed_waveforms/'+str(hsh), out_compressed_group)

logging.info("Finished reading bank files and copying compressed waveforms")
