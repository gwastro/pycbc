; PLEASE NOTE, SECTION NAMES AND OPTIONS SHOULD BE BLOCK LOWER CASE
; VALUES CAN BE MIXED CASE

[ahope]
; https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/initialization.html
; provides details of how to set up an ahope configuration .ini file
h1-channel-name = H1:LDAS-STRAIN
l1-channel-name = L1:LDAS-STRAIN
;h2-channel-name = H2:LDAS-STRAIN

[ahope-ifos]
; This is the list of ifos to analyse
h1 =
l1 =

[ahope-datafind]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/datafind.html
datafind-method = AT_RUNTIME_SINGLE_FRAMES
datafind-h1-frame-type = H1_LDAS_C02_L2
datafind-l1-frame-type = L1_LDAS_C02_L2
;datafind-h2-frame-type = H2_LDAS_C02_L2
datafind-check-segment-gaps = update_times
datafind-check-frames-exist = raise_error
datafind-check-segment-summary = no_test
; Set this to sepcify the datafind server. If this is not set the code will
; use the value in ${LIGO_DATAFIND_SERVER}
;datafind-ligo-datafind-server = ""

[ahope-segments]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/segments.html
; PIPEDOWN demands we use AT_RUNTIME
segments-method = AT_RUNTIME
segments-H1-science-name = H1:DMT-SCIENCE:4
segments-L1-science-name = L1:DMT-SCIENCE:4
;segments-V1-science-name = V1:ITF_SCIENCEMODE:6
segments-database-url = https://segdb.ligo.caltech.edu
segments-veto-definer-url = https://www.lsc-group.phys.uwm.edu/ligovirgo/cbc/public/segments/S6/H1L1V1-S6_CBC_LOWMASS_B_OFFLINE-937473702-0.xml
segments-maximum-veto-category = 5
segments-minimum-segment-length = 2000
segments-generate-coincident-segments =

[ahope-tmpltbank]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/template_bank.html
tmpltbank-method=WORKFLOW_INDEPENDENT_IFOS
; Remove the option below to disable linking with matchedfilter_utils
tmpltbank-link-to-matchedfltr=
analysis-length = 2048

[ahope-injections]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/injections.html
injections-method=IN_WORKFLOW

[ahope-timeslides]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/time_slides.html
timeslides-method=AT_RUNTIME
timeslides-exe = tisi

[ahope-splittable]
splittable-method=IN_WORKFLOW
splittable-num-banks=20

[ahope-matchedfilter]
; See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/matched_filter.html
matchedfilter-method=WORKFLOW_INDEPENDENT_IFOS
analysis-length = 2048

[ahope-coincidence]
; See See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/coincidence.html
coincidence-method=WORKFLOW_DISCRETE_SLIDES 

[ahope-postprocprep]
; See See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/postprocprep.html
postprocprep-method=PIPEDOWN_AHOPE
; Avoiding namespace collisions with pipedown.ini until that gets removed
postprocprep-combiner1-exe=pycbcsqlite
postprocprep-combiner2-exe=pycbcsqlite
postprocprep-cluster-exe=clustercoincs
postprocprep-injfind-exe=databaseinjfind

[ahope-postproc]
; See See https://ldas-jobs.ligo.caltech.edu/~cbc/docs/pycbc/ahope/postproc.html
postproc-method=PIPEDOWN_AHOPE
postproc-computedurations-exe=computedurs
postproc-cfar-exe=pycbccfar

[executables]
; setup of condor universe and location of executables
tmpltbank         = ${which:pycbc_geom_nonspinbank}
inspiral          = ${which:pycbc_inspiral}
splittable = ${which:pycbc_splitbank}
segment_query = ${which:ligolw_segment_query}
segments_from_cats = ${which:ligolw_segments_from_cats}
llwadd = ${which:ligolw_add}
ligolw_combine_segments = ${which:ligolw_combine_segments}
injections = ${which:lalapps_inspinj}
thinca = ${which:ligolw_sstinca}
tisi = ${which:pycbc_timeslides}
write_ihope_page = ${which:lalapps_write_ihope_page}
pycbcsqlite = ${which:pycbc_sqlite_simplify}
clustercoincs = ${which:ligolw_cbc_cluster_coincs}
databaseinjfind = ${which:ligolw_dbinjfind}
computedurs = ${which:pycbc_compute_durations}
pycbccfar = ${which:pycbc_calculate_far}

[datafind]
urltype=file

[segments_from_cats]

[ligolw_combine_segments]

[tmpltbank]
; template bank generation parameters -- added to all tmpltbank jobs
min-match = 0.97
pn-order = threePointFivePN
f-low = 40
f0 = 40
min-mass1 = 1.0
max-mass1 = 25.0
min-mass2 = 1.0
max-mass2 = 25.0
max-total-mass = 25.0
f-upper = 2048
delta-f = 0.01
psd-estimation = median
psd-segment-length = 256
psd-segment-stride = 128
psd-inverse-length = 16
strain-high-pass = 30
pad-data = 8
sample-rate = 4096
calculate-ethinca-metric =

[tmpltbank-h1]
; h1 specific tmpltbank parameters
channel-name = ${ahope|h1-channel-name}

;[tmpltbank-h2]
; h2 specific tmpltbank parameters
;channel-name = ${ahope|h2-channel-name}

[tmpltbank-l1]
; l1 specific tmpltbank parameters
channel-name = ${ahope|l1-channel-name}

[splittable]
; options for splittable job
random-sort =

[inspiral]
; inspiral analysis parameters -- added to all inspiral jobs
snr-threshold = 5.5
low-frequency-cutoff = 40
approximant = SPAtmplt
order = 7
cluster-method = template
maximization-interval = 30
;bank-veto-bank-file = /home/spxiwh/lscsoft_git/src/pycbc/examples/ahope/er_daily_ahope/bank_veto_bank.xml
chisq-bins = 16
chisq-threshold = 10.0
chisq-delta = 0.2
segment-length = 256
segment-start-pad = 64
segment-end-pad = 64
processing-scheme = cpu
psd-estimation = median
psd-segment-length = 256
psd-segment-stride = 128
psd-inverse-length = 16
strain-high-pass = 30
pad-data = 8
sample-rate = 4096

[inspiral-h1]
; h1 specific inspiral parameters
channel-name = ${ahope|h1-channel-name}

[inspiral-l1]
; l1 specific inspiral parameters
channel-name = ${ahope|l1-channel-name}

;[inspiral-h2]
; h2 specific inspiral parameters
;channel-name =${ahope|h2-channel-name}

[llwadd]

[thinca]
drop-veto-info =
make-expr-tables =
e-thinca-parameter = 0.5
weighted-snr = newsnr
magic-number = 6.0 
depop-sngl-inspiral =

[tisi]

[tisi-zerolag]
tisi-slides = H1=0:0:0 L1=0:0:0

[tisi-slides]
inspiral-num-slides = 100:H1=0,L1=5
remove-zero-lag =


[pycbcsqlite]
tmp-space = ${ahope|pipedown-tmp-space}
vacuum =

[clustercoincs]
cluster-window = 10000
ranking-table = coinc_inspiral
ranking-stat = snr
rank-by = MAX
;   following are optional
param-name = mchirp
param-ranges = [0,3.48);[3.48,7.4);[7.4,20]
group-by-ifos =
tmp-space = ${ahope|pipedown-tmp-space}
time-column = end_time
;exclude-coincs = [ALLinH1,H2];[H1,H2inALL];[H2,L1inH1,H2,L1]
; Commenting vacuum as we'll let the combiner do this.
;vacuum =

[databaseinjfind]
simulation-table = sim_inspiral
recovery-table = sngl_inspiral
match-criteria = endTime:endTime:1.0
map-label = insp_nearby
rough-match = geocent_end_time:end_time:10
search = inspiral
tmp-space = ${ahope|pipedown-tmp-space}

[computedurs]
;   set options for compute_durations jobs
livetime-program = inspiral
tmp-space = ${ahope|pipedown-tmp-space}

[pycbccfar]
uncombined-far-column = false_alarm_rate
combined-far-column = combined_far
ranking-table = coinc_inspiral
ranking-stat = snr
rank-by = MAX
param-name = mchirp
param-ranges = [0,3.48);[3.48,7.4);[7.4,20]
group-by-ifos =
tmp-space = ${ahope|pipedown-tmp-space}
