###############################################################################
# Also tell Pegasus to use the Replica Catalog for file locations
pegasus.dir.storage.mapper=Replica
pegasus.dir.storage.mapper.replica=File
pegasus.dir.storage.mapper.replica.file=output.map

# Add Replica selection options so that it will try URLs first, then 
# XrootD for OSG, then gridftp, then anything else
pegasus.selector.replica=Regex
pegasus.selector.replica.regex.rank.1=file://(?!.*(cvmfs)).*
pegasus.selector.replica.regex.rank.2=file:///cvmfs/.*
pegasus.selector.replica.regex.rank.3=root://.*
pegasus.selector.replica.regex.rank.4=gsiftp://red-gridftp.unl.edu.*
pegasus.selector.replica.regex.rank.5=gridftp://.*
pegasus.selector.replica.regex.rank.6=.\*

# Do not transfer files coming from outside the worklow, simply
# create symbolic links
pegasus.transfer.force=true
pegasus.transfer.links=true

dagman.retry=3
dagman.maxpre=1
pegasus.condor.arguments.quote=true
# Override default value of 1800s
condor.periodic_remove=(JobStatus == 5) && ((CurrentTime - EnteredCurrentStatus) > 43200)
pegasus.catalog.site.file=site-catalog.xml

# Use --cache file as a supplement to the in-dax replica catalog
pegasus.catalog.replica.cache.asrc=true
pegasus.catalog.replica.dax.asrc=true

# placing all the job submit files in the submit directory
# as determined from the planner options. This replicated the
# behavior from Pegasus 4.6.x and is needed until we switch to
# hashed sub-directories in the submit directory
# Same for local-site-scratch
pegasus.dir.submit.mapper=Flat
pegasus.dir.staging.mapper=Flat

pegasus.metrics.app=ligo-pycbc

# turn off the creation of the registration jobs even 
# though the files maybe marked to be registered in the 
# replica catalog
pegasus.register=True

# Help Pegasus developers by sharing performance data (optional)
# Requires python package amqplib to be installed
pegasus.monitord.encoding = json
pegasus.catalog.workflow.amqp.url = amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows
