###############################################################################
#
# Reduce number of retries and patience with failing jobs. Turn to production for production runs.
pegasus.mode = development 
# Test files that have been copied in (or staged back from OSG). Do not bother checking symlinked inputs.
pegasus.integrity.checking = nosymlink

# This enables cleanup jobs in sub-workflows. Pegasus believes it is not
# possible to do this because necessary files might be deleted. Ian thinks
# it is possible to do that (and believe things are set up to do this) without
# risking a parent workflow deleting a file needed by a sub-workflow.
pegasus.file.cleanup.scope = deferred

# Also tell Pegasus to use the Replica Catalog for file locations
pegasus.dir.storage.mapper=Replica
pegasus.dir.storage.mapper.replica=File
pegasus.dir.storage.mapper.replica.file=output.map

# Add Replica selection options so that it will try URLs first, then 
# XrootD for OSG, then gridftp, then anything else
# FIXME: This feels like a *site* property, not a global
pegasus.selector.replica=Regex
pegasus.selector.replica.regex.rank.1=file://(?!.*(cvmfs)).*
pegasus.selector.replica.regex.rank.2=file:///cvmfs/.*
pegasus.selector.replica.regex.rank.3=root://.*
pegasus.selector.replica.regex.rank.4=gsiftp://red-gridftp.unl.edu.*
pegasus.selector.replica.regex.rank.5=gridftp://.*
pegasus.selector.replica.regex.rank.6=.\*

dagman.maxpre=1
# Override default value of 1800s
condor.periodic_remove=(JobStatus == 5) && ((CurrentTime - EnteredCurrentStatus) > 43200)

# Use --cache file as a supplement to the in-dax replica catalog
pegasus.catalog.replica.cache.asrc=true
pegasus.catalog.replica.dax.asrc=true

# placing all the job submit files in the submit directory
# as determined from the planner options. This avoids the use
# of hashed sub-directories where debugging can be hard.
# Same for local-site-scratch
pegasus.dir.submit.mapper=Named
pegasus.dir.staging.mapper=Flat

pegasus.metrics.app=ligo-pycbc

# turn off the creation of the registration jobs even 
# though the files maybe marked to be registered in the 
# replica catalog
pegasus.register=False

# Help Pegasus developers by sharing performance data (optional)
# Requires python package amqplib to be installed
pegasus.monitord.encoding = json
pegasus.catalog.workflow.amqp.url = amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows

# Try this
pegasus.transfer.bypass.input.staging = true
# This is needed for symlinking, it does nothing in the pegasus_sites.py
pegasus.transfer.links = true
